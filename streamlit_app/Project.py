import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os
import json
from datetime import datetime
import joblib
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.metrics import (accuracy_score, classification_report, confusion_matrix,
                           roc_curve, auc)
import optuna
import warnings
import matplotlib.dates as mdates
from scipy import stats
warnings.filterwarnings('ignore', category=UserWarning)
warnings.filterwarnings('ignore', category=FutureWarning)

@st.cache_data(ttl=3600, show_spinner=False)
def calculate_correlations(data, category, feature_cols, num_indicators):
    """ìƒê´€ê´€ê³„ ê³„ì‚° ê²°ê³¼ë¥¼ ìºì‹±í•˜ëŠ” í•¨ìˆ˜"""
    correlations = data[feature_cols].corrwith(data[category]).abs().sort_values(ascending=False)
    return correlations.head(num_indicators)

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="StockFlow AI: Reading Stocks Markets with Sentiment and Econmic Indicators", layout="wide")

# ì‹œê°í™” ì„¤ì •
plt.style.use('seaborn-v0_8-whitegrid')
sns.set_theme(style="whitegrid")
plt.rcParams['font.size'] = 12
plt.rcParams['figure.figsize'] = (10, 8)
plt.rcParams['axes.grid'] = True

# ë©”ì¸ íƒ€ì´í‹€
st.title("StockFlow AI: AktienmÃ¤rkte verstehen mit Stimmungs- und Wirtschaftsindikatoren")
st.markdown("""
# Vorhersage von Aktienkursbewegungen mit Machine Learning ğŸ¤–

---
            
### ğŸ’¡ Was diese App macht:

- ğŸ“ˆ **Sammelt tÃ¤gliche Aktienkursdaten**, kategorisiert nach Sektoren (z.â€¯B. Technologie, Energie, Gold, Verteidigung)
- ğŸ“° **Sammelt Finanznachrichten** mithilfe **kategoriespezifischer SchlÃ¼sselwÃ¶rter**
- ğŸ§  **Wendet das FinBERT-Modell an**, um Wahrscheinlichkeiten fÃ¼r positive, neutrale und negative Stimmungen aus Nachrichtenartikeln zu extrahieren
- ğŸ”¥ **Integriert externe Finanzindikatoren** wie den **Fear & Greed Index**, **10-jÃ¤hrige Staatsanleihenrenditen**, **Dollar-Index** und **makroÃ¶konomische Kennzahlen**
- âš™ï¸ **Normalisiert alle Indikatoren**, um einen direkten Vergleich zwischen unterschiedlichen Datentypen zu ermÃ¶glichen
- ğŸ§ª **Erstellt Machine-Learning-Modelle**, um **AufwÃ¤rts- oder AbwÃ¤rtsbewegungen der Aktienkurse** in jeder Kategorie vorherzusagen
- ğŸš¨ **Analysiert extreme Ereignisse**, wie starke KurseinbrÃ¼che, ungewÃ¶hnliche VolatilitÃ¤t und StimmungseinbrÃ¼che auf Basis der Machine-Learning-Ergebnisse
- ğŸ§  **Generiert Marktanalysen**, indem Nachrichtentrends, wirtschaftliche Bedingungen und Modellprognosen kombiniert werden

---

Entdecke, wie Marktstimmungen und Wirtschaftsindikatoren die Zukunft der AktienmÃ¤rkte beeinflussen!

ğŸ‘‰ **Zuerst**: FÃ¼hre die **Machine-Learning-Vorhersage** aus, um Kursbewegungen auf Basis von Stimmungen und wirtschaftlichen Indikatoren zu analysieren.

ğŸ‘‰ **AnschlieÃŸend**: Erkunde **Extreme Events & Anomalien**, um tiefere Einblicke in MarkterschÃ¼tterungen und AusreiÃŸer zu erhalten.
            
---
### ğŸ“š Datenquellen:

- ğŸ“ˆ **Aktienmarktdaten:** yfinance  
- ğŸ“š **Nachrichtensammlung:** GDELT-Bibliothek  
- ğŸ˜¨ **Fear & Greed Index:** Alternative.me Fear & Greed Index API  
- ğŸ“Š **Wirtschaftsindikatoren:** FRED (Federal Reserve Economic Data)          
""")

st.markdown("---")

# ì»¬ëŸ¼ ë‚˜ëˆ„ê¸°
col1, col2 = st.columns(2)

# ì™¼ìª½: Ticker ì†Œê°œ
with col1:
    st.subheader("ğŸ·ï¸ Ticker Overview by Category")
    
    ticker_data = {
        "Category": ["Tech & AI", "Energy", "Gold & Mining", "Defense & Aerospace"],
        "Tickers": [
            "Apple, Miscrosoft, NVIDIA, Palantir",
            "ExxonMobil, Chevron, BP, Shell",
            "Gold ETF, Gold Miners ETF, Agnico Eagle, Newmont",
            "Lockheed Martin, TX Corporation, Boeing, Northrop Grumman"
        ],
        "Description": [
            "Technology and AI Leaders",
            "Major Energy and Oil Companies",
            "Gold ETFs and Mining Stocks",
            "Defense and Aerospace Giants"
        ]
    }
    ticker_df = pd.DataFrame(ticker_data)
    st.dataframe(ticker_df, use_container_width=True)

    st.markdown("""
        ### ğŸ› ï¸ Feature Engineering Ãœberblick
        - **Zeitreihen-Features:** Lag 1/3/5 Tage, Rolling Mean/Std 5/10/20 Tage, Momentum, VolatilitÃ¤t
        - **Technische Indikatoren:** MA5, MA20, MA50, RSI, MACD
        - **Korrelationen:** 90/5/20/50 Tage zwischen Aktienkursen und Wirtschaftsindikatoren
        """)
    st.success("""
    - **Starke VerÃ¤nderungen makroÃ¶konomischer Indikatoren:** PlÃ¶tzliche Anstiege oder RÃ¼ckgÃ¤nge von BIP, Verbraucherpreisindex (CPI) usw.
    - **Black-Swan-Ereignisse:** Unvorhersehbare Marktverwerfungen (starker Einbruch oder starker Anstieg)
    - **Korrelationseinbruch:** Zusammenbruch traditioneller Korrelationen zwischen Sektoren
    - **Technische Muster:** Erkennung wichtiger Signale wie Golden Cross und Death Cross
    """)

# ì˜¤ë¥¸ìª½: ê²½ì œ ì§€í‘œ & ê°ì„± ì§€í‘œ ì†Œê°œ
with col2:
    st.subheader("ğŸ“Š Economic and Sentiment Indicators")
    
    indicator_data = {
    "Type": ["Macroeconomic"] * 11 + ["Sentiment"] * 4, 
    "Indicator": [
        "GDP_norm", "CPI_norm", "Industrial_Production_norm", "Real_Interest_Rate_norm",
        "Consumer_Sentiment_norm", "WTI_Oil_norm",
        "10Y_Treasury_Yield", "Natural_Gas", "Dollar_Index", "Government_Spending",
        "Fed_Funds_Rate_norm",  # ì¶”ê°€
        "Positive/Neutral/Negative Probabilities (FinBERT)", 
        "Sentiment Mean", 
        "Sentiment Variance",
        "Fear & Greed Index"
    ],
    "Description": [
        "Gross Domestic Product Growth (Normalized)",
        "Consumer Price Index Inflation (Normalized)",
        "Industrial Production Rate (Normalized)",
        "Real Interest Rate Movements (Normalized)",
        "Consumer Sentiment Index (Normalized)",
        "Oil Price Movements (WTI Crude Oil, Normalized)",
        "10-Year Treasury Yield (Normalized)",
        "Natural Gas Prices (Normalized)",
        "US Dollar Strength Index (Normalized)",
        "Government Spending Index (Normalized)",
        "**Federal Funds Rate (Normalized)**",   # ì„¤ëª…ë„ ì¶”ê°€
        "Probabilities from Financial News Sentiment Analysis",
        "Average Sentiment Score across news articles",
        "Variance of Sentiment Scores",
        "Market Greed or Fear Sentiment Index"
    ]
}
    indicator_df = pd.DataFrame(indicator_data)
    st.dataframe(indicator_df, use_container_width=True)
    
    st.success("""
    â” Ãœber die reine Analyse historischer Daten hinaus haben wir eine **fortschrittliche Feature-Engineering-Strategie** entwickelt,  
    die auch Marktsentiment und extreme Ereignisse berÃ¼cksichtigt.
    """)
   

st.markdown("---")

st.markdown("""
âœ… Alle Indikatoren sind **normalisiert**, um die Vergleichbarkeit zu gewÃ¤hrleisten.  
âœ… Die Sentiment-Analyse wird mithilfe von **FinBERT** auf Finanznachrichten durchgefÃ¼hrt.  
âœ… Echte Wirtschaftsindikatoren wie **Staatsanleihenrenditen**, **Erdgaspreise** und **Dollar-Index** sind fÃ¼r bessere Vorhersagen integriert.
""")

df = load_data()

# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
# ê²½ì œ ë° ê°ì„± ì§€í‘œ ì»¬ëŸ¼ ëª©ë¡ (ì „ì—­ ë³€ìˆ˜ë¡œ ì •ì˜)
economic_columns = [
    'Consumer_Sentiment',
    'CPI',
    'Dollar_Index',
    'Fed_Funds_Rate',
    'GDP',
    'Gov_Spending',
    'Industrial_Production',
    'Natural_Gas',
    'Real_Interest_Rate',
    'WTI_Oil',
    '10Y_Treasury_Yield'
]

sentiment_columns = [
    'fear_greed_value',
    'sentiment_score_mean',
    'positive_prob_mean',
    'negative_prob_mean',
    'neutral_prob_mean',
    'sentiment_group_Zscore'
]
#'daily_sentiment_mean_first'

def create_output_directory(dir_name='ML_improved_results'):
    """ê²°ê³¼ ì €ì¥ì„ ìœ„í•œ ë””ë ‰í† ë¦¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    if not os.path.exists(dir_name):
        os.makedirs(dir_name)
    # ì¹´í…Œê³ ë¦¬ë³„ í•˜ìœ„ ë””ë ‰í† ë¦¬ ìƒì„±
    categories_dir = os.path.join(dir_name, 'categories')
    if not os.path.exists(categories_dir):
        os.makedirs(categories_dir)
    return dir_name
def preprocess_data(df):
    """ë°ì´í„° ì „ì²˜ë¦¬ ë° íŠ¹ì„± ê³µí•™ - Z-score ì •ê·œí™” í¬í•¨"""
    # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
    df= df.copy()
    categories =df['category'].unique()
    ticker = df['ticker'].unique()
    required_columns = ['ticker', 'date', 'close']
    for col in required_columns:
        if col not in df.columns:
            st.error(f"í•„ìˆ˜ ì»¬ëŸ¼ '{col}'ì´ ë°ì´í„°ì— ì—†ìŠµë‹ˆë‹¤. ì˜¬ë°”ë¥¸ ë°ì´í„° íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.")
            return df

    category_tickers = {}
    for category in categories:
        category_tickers[category] = df[df['category'] == category]['ticker'].unique()
        print(f"{category} Categories's Ticker: {category_tickers[category]}")
    # ì¹´í…Œê³ ë¦¬ ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ì¶”ê°€
    if 'category' not in df.columns:
        # í‹°ì»¤ì— ë”°ë¼ ì¹´í…Œê³ ë¦¬ ë§¤í•‘
        category_map = {}
        for category, tickers in category_tickers.items():
            for ticker in tickers:
                category_map[ticker] = category
        
        # ì¹´í…Œê³ ë¦¬ ì»¬ëŸ¼ ì¶”ê°€
        df['category'] = df['ticker'].map(category_map)
        df['category'].fillna('Other', inplace=True)
    
    # ì¼ì¼ ìˆ˜ìµë¥  ê³„ì‚°
    df['daily_return'] = df.groupby('ticker')['close'].pct_change() * 100

    df['sentiment_score_std'].fillna(df['sentiment_score_std'].median(), inplace=True)
    df['daily_sentiment_std_first'].fillna(df['daily_sentiment_std_first'].median(), inplace=True)


    # ê²½ì œì§€í‘œì™€ ê°ì„±ì§€í‘œ ì»¬ëŸ¼ ì •ì˜
    economic_columns = ['Consumer_Sentiment', 'CPI', 'Dollar_Index', 'Fed_Funds_Rate',
                    'GDP', 'Gov_Spending', 'Industrial_Production', 'Natural_Gas',
                    'Real_Interest_Rate', 'WTI_Oil', '10Y_Treasury_Yield']

    sentiment_columns = ['fear_greed_value', 'sentiment_score_mean', 'positive_prob_mean',
                        'negative_prob_mean', 'neutral_prob_mean', ]#'daily_sentiment_mean_first'

    # ê²½ì œì§€í‘œ ì •ê·œí™” - Z-score ë°©ì‹
    for col in economic_columns:
        mean_val = df[col].mean()
        std_val = df[col].std()
        df[f'{col}_norm'] = (df[col] - mean_val) / std_val

    # ê²½ì œì§€í‘œ ì •ê·œí™” - Min-Max ë°©ì‹ (í•„ìš”ì‹œ)
    for col in economic_columns:
        min_val = df[col].min()
        max_val = df[col].max()
        df[f'{col}_minmax'] = (df[col] - min_val) / (max_val - min_val)

    # ê°ì„±ì§€í‘œ ì •ê·œí™” - Z-score ë°©ì‹
    for col in sentiment_columns:
        mean_val = df[col].mean()
        std_val = df[col].std()
        df[f'{col}_norm'] = (df[col] - mean_val) / std_val

    # ê°ì„±ì§€í‘œ ì •ê·œí™” - Min-Max ë°©ì‹ (í•„ìš”ì‹œ)
    for col in sentiment_columns:
        min_val = df[col].min()
        max_val = df[col].max()
        df[f'{col}_minmax'] = (df[col] - min_val) / (max_val - min_val)

    # ì¹´í…Œê³ ë¦¬ë³„ ê°ì„± Z-score ì •ê·œí™”
    df['category_means'] = df.groupby('category')['sentiment_score_mean'].transform('mean')
    df['category_stds'] = df.groupby('category')['sentiment_score_mean'].transform('std')
    df['sentiment_group_Zscore'] = (df['sentiment_score_mean'] - df['category_means']) / df['category_stds']

    # ì •ê·œí™”ëœ ì»¬ëŸ¼ëª… ë¦¬ìŠ¤íŠ¸ ìƒì„±
    norm_economic_columns = [f'{col}_norm' for col in economic_columns]
    norm_sentiment_columns = [f'{col}_norm' for col in sentiment_columns] + ['sentiment_group_Zscore']

    # ì¹´í…Œê³ ë¦¬ë³„ ì¢…ê°€ í‰ê·  ê³„ì‚° (ê¸°ì¡´ ì½”ë“œ)
    daily_avg_prices = df.groupby(['date', 'category'])['close'].mean().unstack()

    # ì •ê·œí™”ëœ ì§€í‘œë“¤ë¡œ ì¼ë³„ ì§€í‘œ í…Œì´ë¸” ìƒì„±
    daily_indicators = df.groupby('date')[norm_economic_columns + norm_sentiment_columns].first()   
        # ë‚˜ë¨¸ì§€ ì½”ë“œëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€...
        
    return df, daily_avg_prices, daily_indicators

def create_lag_features(df, column, lags=[1, 3, 5, 10, 20]):
    """ì‹œê³„ì—´ ë°ì´í„°ì— ëŒ€í•œ ì§€ì—° íŠ¹ì„±ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    df = df.copy()
    for lag in lags:
        df[f'{column}_lag_{lag}'] = df[column].shift(lag)
    return df

def create_rolling_features(df, column, windows=[5, 10, 20, 50]):
    """ë¡¤ë§ í†µê³„ íŠ¹ì„±ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    df = df.copy()
    for window in windows:
        # ë¡¤ë§ í‰ê· 
        df[f'{column}_rolling_mean_{window}'] = df[column].rolling(window=window).mean()
        # ë¡¤ë§ í‘œì¤€í¸ì°¨
        df[f'{column}_rolling_std_{window}'] = df[column].rolling(window=window).std()
        # ë¡¤ë§ ë³€í™”ìœ¨ (í˜„ì¬ê°’/í‰ê· ê°’)
        df[f'{column}_rolling_ratio_{window}'] = df[column] / df[f'{column}_rolling_mean_{window}']
    return df

def create_momentum_features(df, column, periods=[5, 10, 20, 50]):
    """ëª¨ë©˜í…€(ë³€í™”ìœ¨) íŠ¹ì„±ì„ ìƒì„±í•©ë‹ˆë‹¤."""
   
    for period in periods:
        # ë‹¨ìˆœ ë³€í™”ìœ¨
        df[f'{column}_change_{period}d'] = df[column].pct_change(periods=period)
        # ê°€ì†ë„(ë³€í™”ìœ¨ì˜ ë³€í™”ìœ¨)
        df[f'{column}_acceleration_{period}d'] = df[f'{column}_change_{period}d'].pct_change(periods=period)
    return df

def create_volatility_features(df, column, windows=[5, 10, 20, 50]):
    """ë³€ë™ì„± ì§€í‘œë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
   
    for window in windows:
        # ë¡¤ë§ ë³€ë™ì„± (í‘œì¤€í¸ì°¨ ê¸°ë°˜)
        df[f'{column}_volatility_{window}d'] = df[column].pct_change().rolling(window=window).std()
        # ìƒëŒ€ ë³€ë™ì„± (í‰ê·  ëŒ€ë¹„)
        rolling_mean = df[column].rolling(window=window).mean()
        rolling_std = df[column].rolling(window=window).std()
        df[f'{column}_rel_volatility_{window}d'] = rolling_std / rolling_mean
    return df

def create_technical_indicators(df, column):
    """ê¸°ìˆ ì  ì§€í‘œë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
   
    # ì´ë™í‰ê· ì„  (Moving Averages)
    df[f'{column}_MA5'] = df[column].rolling(window=5).mean()
    df[f'{column}_MA20'] = df[column].rolling(window=20).mean()
    df[f'{column}_MA50'] = df[column].rolling(window=50).mean()
    # ì´ë™í‰ê·  êµì°¨ ì‹ í˜¸
    df[f'{column}_MA_cross_5_20'] = np.where(df[f'{column}_MA5'] > df[f'{column}_MA20'], 1, -1)
    df[f'{column}_MA_cross_20_50'] = np.where(df[f'{column}_MA20'] > df[f'{column}_MA50'], 1, -1)
    # RSI (Relative Strength Index) ê³„ì‚° - 14ì¼ ê¸°ì¤€
    delta = df[column].diff()
    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
    rs = gain / loss
    df[f'{column}_RSI_14'] = 100 - (100 / (1 + rs))
    # MACD (Moving Average Convergence Divergence)
    ema12 = df[column].ewm(span=12, adjust=False).mean()
    ema26 = df[column].ewm(span=26, adjust=False).mean()
    df[f'{column}_MACD'] = ema12 - ema26
    df[f'{column}_MACD_signal'] = df[f'{column}_MACD'].ewm(span=9, adjust=False).mean()
    df[f'{column}_MACD_hist'] = df[f'{column}_MACD'] - df[f'{column}_MACD_signal']

    return df

def create_interaction_features(df, columns):
    """ë³€ìˆ˜ ê°„ ìƒí˜¸ì‘ìš© íŠ¹ì„±ì„ ìƒì„±í•©ë‹ˆë‹¤."""

    # ì£¼ìš” íŠ¹ì„± ê°„ ìƒí˜¸ì‘ìš©
    for i, col1 in enumerate(columns):
        for col2 in columns[i+1:]:
            # ê³±ì…ˆ ìƒí˜¸ì‘ìš©
            df[f'{col1}_mul_{col2}'] = df[col1] * df[col2]
            # ë¹„ìœ¨ ìƒí˜¸ì‘ìš©
            if not (df[col2] == 0).any():  # 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€
                df[f'{col1}_div_{col2}'] = df[col1] / df[col2]
            # í•© ìƒí˜¸ì‘ìš©
            df[f'{col1}_add_{col2}'] = df[col1] + df[col2]
    return df

def create_correlation_features(df, category_tickers, windows=[90, 5, 20, 50]):
    """
    ë¡¤ë§ ìƒê´€ê´€ê³„ íŠ¹ì„± ìƒì„± (ì¹´í…Œê³ ë¦¬-ì§€í‘œ ê°„ ë° ì¹´í…Œê³ ë¦¬-ì¹´í…Œê³ ë¦¬ ê°„)
    """
    # ë°ì´í„° ê²€ì¦ ë° ë””ë²„ê¹… ì •ë³´
    #st.write("ìƒê´€ê´€ê³„ íŠ¹ì„± ìƒì„±ì„ ìœ„í•œ ë°ì´í„° êµ¬ì¡° í™•ì¸:")
    #st.write(f"DataFrame ì»¬ëŸ¼: {df.columns.tolist()[:10]}... ë“± {len(df.columns)}ê°œ")
    
    # í•„ìš”í•œ ì»¬ëŸ¼ í™•ì¸
    required_cols = ['date', 'category', 'close']
    missing_cols = [col for col in required_cols if col not in df.columns]
    if missing_cols:
        st.error(f"í•„ìš”í•œ ì»¬ëŸ¼ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤: {missing_cols}")
        return df, None, None
    
    # ë‚ ì§œ í˜•ì‹ í™•ì¸
    if not pd.api.types.is_datetime64_any_dtype(df['date']):
        st.warning("'date' ì»¬ëŸ¼ì´ datetime í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤. ë³€í™˜ì„ ì‹œë„í•©ë‹ˆë‹¤.")
        try:
            df['date'] = pd.to_datetime(df['date'])
        except Exception as e:
            st.error(f"ë‚ ì§œ ë³€í™˜ ì˜¤ë¥˜: {str(e)}")
            return df, None, None
    
    # ì¹´í…Œê³ ë¦¬ë³„ ì¼ë³„ í‰ê·  ê°€ê²© ê³„ì‚°
    try:
        daily_avg_prices = df.groupby(['date', 'category'])['close'].mean().unstack()
        #st.write(f"ì¹´í…Œê³ ë¦¬ë³„ ì¼ë³„ í‰ê·  ê°€ê²© ê³„ì‚° ì™„ë£Œ: {daily_avg_prices.shape}")
    except Exception as e:
        st.error(f"ì¼ë³„ í‰ê·  ê°€ê²© ê³„ì‚° ì˜¤ë¥˜: {str(e)}")
        return df, None, None
    
    # ì •ê·œí™”ëœ ê²½ì œ ë° ê°ì„± ì§€í‘œ ì»¬ëŸ¼ ì‹ë³„
    norm_economic_columns = [f'{col}_norm' for col in economic_columns if f'{col}_norm' in df.columns]
    norm_sentiment_columns = [f'{col}_norm' for col in sentiment_columns if f'{col}_norm' in df.columns]
    
    # ê²½ê³ : ì •ê·œí™”ëœ ì§€í‘œê°€ ì—†ëŠ” ê²½ìš°
    if not norm_economic_columns and not norm_sentiment_columns:
        st.warning("ì •ê·œí™”ëœ ì§€í‘œê°€ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. '_norm' í˜•ì‹ì˜ ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        # ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ ì¤‘ ê°€ëŠ¥í•œ ì§€í‘œ ì»¬ëŸ¼ í‘œì‹œ
        possible_indicators = [col for col in df.columns if any(econ in col for econ in economic_columns) or 
                              any(sent in col for sent in sentiment_columns)]
        #if possible_indicators:
        #     st.write(f"ê°€ëŠ¥í•œ ì§€í‘œ ì»¬ëŸ¼: {possible_indicators}")
    
    # ëª¨ë“  ì •ê·œí™”ëœ ì§€í‘œ ì»¬ëŸ¼ (+ sentiment_group_Zscore)
    all_norm_columns = norm_economic_columns + norm_sentiment_columns
    if 'sentiment_group_Zscore' in df.columns:
        all_norm_columns.append('sentiment_group_Zscore')
    
    #st.write(f"ì‚¬ìš©í•  ì •ê·œí™”ëœ ì§€í‘œ ì»¬ëŸ¼: {len(all_norm_columns)}ê°œ")
    
    # ì¼ë³„ ì§€í‘œ ë°ì´í„° (ì²« ë²ˆì§¸ í–‰ ê¸°ì¤€)
    if all_norm_columns:
        try:
            daily_indicators = df.groupby('date')[all_norm_columns].first()
            #st.write(f"ì¼ë³„ ì§€í‘œ ë°ì´í„° ê³„ì‚° ì™„ë£Œ: {daily_indicators.shape}")
        except Exception as e:
            st.error(f"ì¼ë³„ ì§€í‘œ ê³„ì‚° ì˜¤ë¥˜: {str(e)}")
            return df, daily_avg_prices, None
    else:
        st.error("ì •ê·œí™”ëœ ì§€í‘œê°€ ì—†ì–´ ì¼ë³„ ì§€í‘œ ë°ì´í„°ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return df, daily_avg_prices, None
    
    # ìƒê´€ê´€ê³„ ê³„ì‚°ì„ ìœ„í•œ ë¹ˆ ë°ì´í„°í”„ë ˆì„ ìƒì„±
    corr_df = pd.DataFrame(index=daily_avg_prices.index)
    
    # 1. ì¤‘ìš” ì¹´í…Œê³ ë¦¬-ì§€í‘œ ì¡°í•©ì— ëŒ€í•œ 90ì¼ ìƒê´€ê´€ê³„ ê³„ì‚°
    important_combinations = [
        ('Gold', 'GDP_norm'),
        ('Gold', 'Gov_Spending_norm'),
        ('Gold', 'Real_Interest_Rate_norm'),
        ('Defense', 'Dollar_Index_norm'),
        ('Defense', 'Industrial_Production_norm'),
        ('Energy', 'Consumer_Sentiment_norm'),
        ('Energy', 'Dollar_Index_norm'),
        ('Energy', 'Gov_Spending_norm'),
        ('Tech_AI', 'Dollar_Index_norm')
    ]
    
    # ì²˜ë¦¬ ê°€ëŠ¥í•œ ì¤‘ìš” ì¡°í•© í•„í„°ë§
    available_combinations = []
    for category, indicator in important_combinations:
        if category in daily_avg_prices.columns and indicator in daily_indicators.columns:
            available_combinations.append((category, indicator))
    
   # st.write(f"Important combinations that can be processed: {len(available_combinations)} / {len(important_combinations)}")
    
    # ì¤‘ìš” ì¡°í•©ì— ëŒ€í•´ 90ì¼ ë¡¤ë§ ìƒê´€ê³„ìˆ˜ ê³„ì‚°
    for category, indicator in available_combinations:
        price_series = daily_avg_prices[category]
        indicator_series = daily_indicators[indicator]
        
        # ì›ë³¸ê³¼ ë™ì¼í•œ ì´ë¦„ ì‚¬ìš©
        corr_name = f"{category}_{indicator}_corr_mean90"
        corr_df[corr_name] = price_series.rolling(window=90).corr(indicator_series)
    
    # 2. ì¹´í…Œê³ ë¦¬-ì¹´í…Œê³ ë¦¬ ë° ì¶”ê°€ ìœˆë„ìš° ìƒê´€ê´€ê³„ ê³„ì‚°
    categories = list(daily_avg_prices.columns)
    reference_columns = []
    
    # ì°¸ì¡° ì»¬ëŸ¼ ì„¤ì • - ì¹´í…Œê³ ë¦¬ + ìƒìœ„ ê²½ì œì§€í‘œ
    if categories:
        reference_columns.extend(categories)
    
    if norm_economic_columns:
        reference_columns.extend(norm_economic_columns[:min(9, len(norm_economic_columns))])
    
    #st.write(f"Reference Columns: {reference_columns}")
    
    # ê° ì¹´í…Œê³ ë¦¬ì™€ ì°¸ì¡° ì»¬ëŸ¼ì— ëŒ€í•´ ì¶”ê°€ ìœˆë„ìš° ìƒê´€ê´€ê³„ ê³„ì‚°
    correlations_created = 0
    
    for category in categories:
        price_series = daily_avg_prices[category]
        
        for ref_col in reference_columns:
            if ref_col == category:  # ìê¸° ìì‹ ê³¼ì˜ ìƒê´€ê´€ê³„ëŠ” ê³„ì‚°í•˜ì§€ ì•ŠìŒ
                continue
                
            # ì´ë¯¸ ì¤‘ìš” ì¡°í•©ìœ¼ë¡œ 90ì¼ ìƒê´€ê´€ê³„ê°€ ê³„ì‚°ë˜ì—ˆëŠ”ì§€ í™•ì¸
            is_important_90d = False
            for imp_cat, imp_ind in available_combinations:
                if category == imp_cat and ref_col == imp_ind:
                    is_important_90d = True
                    break
            
            # ì°¸ì¡° ì‹œë¦¬ì¦ˆ ê°€ì ¸ì˜¤ê¸°
            ref_series = None
            if ref_col in categories:
                ref_series = daily_avg_prices[ref_col]
            elif ref_col in daily_indicators.columns:
                ref_series = daily_indicators[ref_col]
            
            if ref_series is not None:
                # ì›ë˜ ìœˆë„ìš° ëª©ë¡ì—ì„œ í•„ìš”í•œ ê²ƒë§Œ ê³„ì‚°
                for window in windows:
                    # 90ì¼ ìœˆë„ìš°ì´ê³  ì´ë¯¸ ì¤‘ìš” ì¡°í•©ìœ¼ë¡œ ê³„ì‚°ëœ ê²½ìš° ê±´ë„ˆë›°ê¸°
                    if window == 90 and is_important_90d:
                        continue
                    
                    # ì›ë³¸ê³¼ ë™ì¼í•œ ì´ë¦„ ê·œì¹™ ì‚¬ìš©
                    if window == 90 and (category, ref_col) in available_combinations:
                        # ì¤‘ìš” ì¡°í•©ì€ ì´ë¯¸ ìœ„ì—ì„œ ì²˜ë¦¬ë¨
                        continue
                    else:
                        corr_name = f"corr_{category}_{ref_col}_{window}d"
                        corr_df[corr_name] = price_series.rolling(window=window).corr(ref_series)
                        correlations_created += 1
    
   # st.write(f"Created correlation attribution: {correlations_created}ê°œ")
    
    # ìƒê´€ê´€ê³„ íŠ¹ì„±ì´ ì—†ìœ¼ë©´ ê²½ê³ 
    if corr_df.empty:
        st.warning("ìƒì„±ëœ ìƒê´€ê´€ê³„ íŠ¹ì„±ì´ ì—†ìŠµë‹ˆë‹¤.")
        return df, daily_avg_prices, daily_indicators
    
    # ë‚ ì§œë³„ë¡œ ìƒê´€ê´€ê³„ ê°’ì„ ì›ë³¸ ë°ì´í„°í”„ë ˆì„ì— ì¶”ê°€
    merged_count = 0
    for corr_col in corr_df.columns:
        # ë°ì´í„°í”„ë ˆì„ì„ ì¬ì„¤ì •í•˜ì—¬ ë‚ ì§œë¥¼ ì»¬ëŸ¼ìœ¼ë¡œ ë³€í™˜
        temp_df = corr_df[corr_col].reset_index()
        temp_df.columns = ['date', corr_col]
        
        # ì›ë³¸ ë°ì´í„°í”„ë ˆì„ê³¼ ë³‘í•©
        df = pd.merge(df, temp_df, on='date', how='left')
        merged_count += 1
    
    #st.write(f"Merged correlation attributes: {merged_count}")
    
    # ê²°ì¸¡ì¹˜ ì²˜ë¦¬
    for col in corr_df.columns:
        if col in df.columns:
            df[col] = df[col].fillna(method='ffill')  # ì•ì˜ ê°’ìœ¼ë¡œ ì±„ìš°ê¸°
            df[col] = df[col].fillna(0)  # ë‚¨ì€ ê²°ì¸¡ì¹˜ëŠ” 0ìœ¼ë¡œ
    
    return df, daily_avg_prices, daily_indicators

def get_indicator_columns():
    """ë¶„ì„ì— ì‚¬ìš©í•  ì§€í‘œ ì»¬ëŸ¼ëª… ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    # ì •ê·œí™”ëœ ê²½ì œ ì§€í‘œ ë³€ìˆ˜ ëª©ë¡
    norm_economic_columns = [f'{col}_norm' for col in economic_columns]
    
    # ì •ê·œí™”ëœ ê°ì„± ì§€í‘œ ë³€ìˆ˜ ëª©ë¡
    norm_sentiment_columns = [f'{col}_norm' for col in sentiment_columns]
    
    # ì •ê·œí™”ëœ ì¶”ê°€ íŠ¹ì„± ëª©ë¡
    additional_features = [
        'Gold_GDP_norm_corr_mean90',
        'Gold_Gov_Spending_norm_corr_mean90',
        'Gold_Real_Interest_Rate_norm_corr_mean90',
        'Defense_Dollar_Index_norm_corr_mean90',
        'Defense_Industrial_Production_norm_corr_mean90',
        'Energy_Consumer_Sentiment_norm_corr_mean90',
        'Energy_Dollar_Index_norm_corr_mean90',
        'Energy_Gov_Spending_norm_corr_mean90',
        'Tech_AI_Dollar_Index_norm_corr_mean90'
    ]
    
    # ëª¨ë“  ì •ê·œí™”ëœ ì§€í‘œ ë³€ìˆ˜ ëª©ë¡
    all_norm_indicators = norm_economic_columns + norm_sentiment_columns + additional_features
    
    return norm_economic_columns, norm_sentiment_columns, additional_features, all_norm_indicators
def prepare_enhanced_model_data(daily_avg_prices, daily_indicators, categories, important_indicators):
    """í–¥ìƒëœ ëª¨ë¸ë§ ë°ì´í„°ë¥¼ ì¤€ë¹„í•˜ëŠ” í•¨ìˆ˜ (ë¬´í•œëŒ€, ê·¹ë‹¨ê°’ ì•ˆì „ ì²˜ë¦¬ í†µí•© ë²„ì „)"""
    import numpy as np
    import pandas as pd
    import streamlit as st

    model_data = {}
    if 'visualizations' not in st.session_state:
        st.session_state['visualizations'] = {}

    # ì…ë ¥ ë°ì´í„° ê²€ì¦
    if daily_avg_prices is None or daily_indicators is None:
        st.error("ìœ íš¨í•œ ê°€ê²© ë°ì´í„° ë˜ëŠ” ì§€í‘œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return model_data

    # ì¼ì¹˜í•˜ëŠ” ë‚ ì§œ í™•ì¸
    common_dates = daily_avg_prices.index.intersection(daily_indicators.index)
    if len(common_dates) == 0:
        st.error("ê°€ê²© ë°ì´í„°ì™€ ì§€í‘œ ë°ì´í„° ê°„ì— ì¼ì¹˜í•˜ëŠ” ë‚ ì§œê°€ ì—†ìŠµë‹ˆë‹¤.")
        return model_data

    #st.write(f"ëª¨ë¸ ë°ì´í„° ì¤€ë¹„: ì¼ì¹˜í•˜ëŠ” ë‚ ì§œ {len(common_dates)}ê°œ")

    # ì§„í–‰ ìƒí™© í‘œì‹œ
    #progress_bar = st.progress(0)
    #status_text = st.empty()

    available_categories = [cat for cat in categories if cat in daily_avg_prices.columns]
    if len(available_categories) < len(categories):
        st.warning(f"ì¼ë¶€ ì¹´í…Œê³ ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‚¬ìš© ê°€ëŠ¥í•œ ì¹´í…Œê³ ë¦¬: {available_categories}")

    data_loss_diagnostics = {}

    for i, category in enumerate(available_categories):
        #status_text.text(f"{category} Category's Model Data is on the way...")

        try:
            price_series = daily_avg_prices[category]

            # ì•ˆì „í•œ ë¡œê·¸ ìˆ˜ìµë¥  ê³„ì‚° (0ì€ NaNìœ¼ë¡œ ì²˜ë¦¬)
            price_series = price_series.replace(0, np.nan)
            log_returns = np.log(price_series / price_series.shift(1))

            available_indicators = [col for col in important_indicators if col in daily_indicators.columns]
            if len(available_indicators) == 0:
                st.warning(f"{category}ì— ëŒ€í•œ ì‚¬ìš© ê°€ëŠ¥í•œ ì§€í‘œê°€ ì—†ìŠµë‹ˆë‹¤.")
                continue

            # ê¸°ë³¸ ëª¨ë¸ ë°ì´í„° ìƒì„±
            model_df = pd.concat([
                log_returns.rename(category),
                daily_indicators[available_indicators]
            ], axis=1)

            # --- ë¬´í•œëŒ€ì™€ ê·¹ë‹¨ê°’ ì²˜ë¦¬ ì¶”ê°€ ì‹œì‘ ---
            model_df = model_df.replace([np.inf, -np.inf], np.nan)  # ë¬´í•œëŒ€ ì œê±°
            FLOAT32_MAX = np.finfo(np.float32).max
            model_df = model_df.clip(lower=-FLOAT32_MAX, upper=FLOAT32_MAX)  # ê·¹ë‹¨ê°’ í´ë¦¬í•‘
            # --- ë¬´í•œëŒ€ì™€ ê·¹ë‹¨ê°’ ì²˜ë¦¬ ì¶”ê°€ ë ---

            # NaN ì²˜ë¦¬
            essential_cols = [category]
            if model_df[category].isna().any():
                model_df[category] = model_df[category].fillna(method='ffill').fillna(method='bfill')

            # í•„ìˆ˜ lag ë³€ìˆ˜ë“¤ ì²˜ë¦¬ (í•„ìš” ì‹œ)
            lag_cols = [col for col in model_df.columns if col.startswith(f"{category}_lag") or col.startswith(f"{category}_rolling")]
            for col in lag_cols:
                model_df[col] = model_df[col].interpolate(method='linear').fillna(method='ffill').fillna(method='bfill')

            # ë‚˜ë¨¸ì§€ ìˆ˜ì¹˜í˜• ë³€ìˆ˜ë“¤ ì²˜ë¦¬
            numeric_cols = model_df.select_dtypes(include=['float64', 'int64']).columns
            for col in numeric_cols:
                if model_df[col].isna().any():
                    median_value = model_df[col].median()
                    if pd.isna(median_value):
                        model_df[col] = model_df[col].fillna(0)
                    else:
                        model_df[col] = model_df[col].fillna(median_value)

            # ìµœì¢…ì ìœ¼ë¡œ ë‚¨ì€ NaNì€ 0ìœ¼ë¡œ
            model_df = model_df.fillna(0)

            # ìµœì¢… ë°ì´í„° ê²€ì¦
            if len(model_df) < 30:
                st.warning(f"{category}: íŠ¹ì„± ìƒì„± í›„ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤ ({len(model_df)} rows). ìµœì†Œ 30ê°œ í•„ìš”.")
                continue

            #st.write(f"{category} ê°•í™”ëœ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ: {len(model_df)}í–‰, {len(model_df.columns)}ì—´")
            model_data[category] = model_df

        except Exception as e:
            st.error(f"{category} ëª¨ë¸ ë°ì´í„° ì¤€ë¹„ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            import traceback
            st.error(traceback.format_exc())

        # ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸
        #progress_bar.progress((i + 1) / len(available_categories))

    #status_text.text(f"ëª¨ë¸ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ: {len(model_data)}ê°œ ì¹´í…Œê³ ë¦¬")
    #progress_bar.empty()

    if not model_data:
        st.error("ëª¨ë“  ì¹´í…Œê³ ë¦¬ì— ëŒ€í•œ ëª¨ë¸ ë°ì´í„° ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")

    return model_data



def select_best_features(X, y, feature_names, n_features=30):
    """ëœë¤ í¬ë ˆìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ìµœì ì˜ íŠ¹ì„±ì„ ì„ íƒí•©ë‹ˆë‹¤."""
    try:
        # íŠ¹ì„± ì¤‘ìš”ë„ë¥¼ ê³„ì‚°í•˜ê¸° ìœ„í•œ ëœë¤ í¬ë ˆìŠ¤íŠ¸ í•™ìŠµ
        rf = RandomForestClassifier(n_estimators=100, random_state=42)
        rf.fit(X, y)
        
        # íŠ¹ì„± ì¤‘ìš”ë„ì— ë”°ë¼ íŠ¹ì„± ì •ë ¬
        importances = rf.feature_importances_
        indices = np.argsort(importances)[::-1]
        
        # ìƒìœ„ nê°œ íŠ¹ì„± ì„ íƒ
        selected_features = [feature_names[i] for i in indices[:n_features]]
        
        return selected_features
    except Exception as e:
        st.error(f"íŠ¹ì„± ì„ íƒ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return feature_names[:min(n_features, len(feature_names))]  # ì˜¤ë¥˜ ì‹œ ì²˜ìŒ nê°œ íŠ¹ì„± ë°˜í™˜


def find_optimal_threshold(y_true, y_proba):
    """
    ROC ê³¡ì„ ê³¼ ë‹¤ì–‘í•œ ë©”íŠ¸ë¦­ì„ ê³ ë ¤í•˜ì—¬ ìµœì ì˜ ì„ê³„ê°’ì„ ì°¾ìŠµë‹ˆë‹¤.
    ë‹¤ì–‘í•œ ë°©ë²•ì„ ì‹œë„í•˜ê³  ê°€ì¥ ì•ˆì •ì ì¸ ì„ê³„ê°’ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    try:
        # NaN ë˜ëŠ” ë¬´í•œê°’ ì²´í¬ ë° ì²˜ë¦¬
        if np.isnan(y_proba).any() or np.isinf(y_proba).any():
            y_proba = np.nan_to_num(y_proba, nan=0.5, posinf=1.0, neginf=0.0)
        
        # í´ë˜ìŠ¤ ë¹„ìœ¨ í™•ì¸
        class_ratio = np.mean(y_true)
        
        # ê° ë°©ë²•ìœ¼ë¡œ ê³„ì‚°í•œ ì„ê³„ê°’ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
        thresholds_list = []
        
        # 1. ROC ê³¡ì„  ê¸°ë°˜ ë°©ë²• (ìœ í´ë¦¬ë””ì•ˆ ê±°ë¦¬)
        fpr, tpr, thresholds_roc = roc_curve(y_true, y_proba)
        
        # ì™„ë²½í•œ ë¶„ë¥˜ ì§€ì  (0,1)ì—ì„œ ê°€ì¥ ê°€ê¹Œìš´ ì§€ì  ì°¾ê¸° (ìœ í´ë¦¬ë””ì•ˆ ê±°ë¦¬)
        distances = np.sqrt((1-tpr)**2 + fpr**2)
        optimal_idx_roc = np.argmin(distances)
        
        # ê±°ë¦¬ê°€ 0.8ë³´ë‹¤ í¬ë©´ ROC ê¸°ë°˜ ì„ê³„ê°’ì´ ë¶ˆì•ˆì •í•  ìˆ˜ ìˆìŒ
        if distances[optimal_idx_roc] < 0.8:
            thresholds_list.append(thresholds_roc[optimal_idx_roc])
        
        # 2. Youden's J statistic (ë¯¼ê°ë„ + íŠ¹ì´ë„ - 1)
        j_statistic = tpr - fpr
        optimal_j_idx = np.argmax(j_statistic)
        thresholds_list.append(thresholds_roc[optimal_j_idx])
        
        # 3. F1 ìŠ¤ì½”ì–´ ìµœëŒ€í™” ì„ê³„ê°’
        f1_scores = []
        test_thresholds = np.linspace(0.1, 0.9, 9)  # 0.1ë¶€í„° 0.9ê¹Œì§€ 9ê°œ ì§€ì  í…ŒìŠ¤íŠ¸
        
        for threshold in test_thresholds:
            y_pred = (y_proba >= threshold).astype(int)
            # ëª¨ë“  ì˜ˆì¸¡ì´ í•œ í´ë˜ìŠ¤ì¸ ê²½ìš° ì²˜ë¦¬
            if len(np.unique(y_pred)) == 1:
                f1 = 0  # í•œ í´ë˜ìŠ¤ë§Œ ì˜ˆì¸¡í•˜ë©´ F1 ìŠ¤ì½”ì–´ê°€ ì˜ë¯¸ ì—†ìŒ
            else:
                # class labelsê°€ (0,1)ì¸ì§€ í™•ì¸
                labels = sorted(np.unique(np.concatenate([y_true, y_pred])))
                pos_label = 1 if 1 in labels else labels[-1]
                
                # ìˆ˜ë™ìœ¼ë¡œ f1 ê³„ì‚°í•˜ì—¬ ì œë¡œ ë””ë¹„ì „ ì˜¤ë¥˜ ë°©ì§€
                try:
                    from sklearn.metrics import precision_score, recall_score
                    precision = precision_score(y_true, y_pred, pos_label=pos_label, zero_division=0)
                    recall = recall_score(y_true, y_pred, pos_label=pos_label, zero_division=0)
                    
                    f1 = 0 if (precision + recall) == 0 else 2 * (precision * recall) / (precision + recall)
                except Exception:
                    f1 = 0
            f1_scores.append(f1)
        
        # F1 ìŠ¤ì½”ì–´ê°€ ìµœëŒ€ì¸ ì„ê³„ê°’ (0ì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì£¼ì˜)
        if max(f1_scores) > 0:
            f1_optimal_threshold = test_thresholds[np.argmax(f1_scores)]
            thresholds_list.append(f1_optimal_threshold)
        
        # 4. í´ë˜ìŠ¤ ë¶ˆê· í˜•ì„ ê³ ë ¤í•œ ì„ê³„ê°’
        # ê·¹ë‹¨ì ì¸ ë¶ˆê· í˜•(15% ë¯¸ë§Œ ë˜ëŠ” 85% ì´ˆê³¼)ì´ ìˆìœ¼ë©´ í´ë˜ìŠ¤ ë¹„ìœ¨ì— ê°€ê¹Œìš´ ì„ê³„ê°’ë„ ê³ ë ¤
        if class_ratio < 0.15 or class_ratio > 0.85:
            balanced_threshold = (class_ratio + 0.5) / 2  # í´ë˜ìŠ¤ ë¹„ìœ¨ê³¼ 0.5 ì‚¬ì´ì˜ ì¤‘ê°„ê°’
            thresholds_list.append(balanced_threshold)
        
        # 5. ì •í™•ë„ ìµœëŒ€í™” ì„ê³„ê°’
        accuracies = []
        for threshold in test_thresholds:
            y_pred = (y_proba >= threshold).astype(int)
            accuracy = np.mean(y_pred == y_true)
            accuracies.append(accuracy)
        
        accuracy_optimal_threshold = test_thresholds[np.argmax(accuracies)]
        thresholds_list.append(accuracy_optimal_threshold)
        
        # ê²°ê³¼ ì„ê³„ê°’ë“¤ì˜ ì¤‘ì•™ê°’ ì‚¬ìš© (ì´ìƒì¹˜ì— ê°•ê±´í•¨)
        if thresholds_list:
            final_threshold = np.median(thresholds_list)
            
            # ì„ê³„ê°’ì´ ë„ˆë¬´ ê·¹ë‹¨ì ì´ë©´ ë³´ì • (0.2~0.8 ë²”ìœ„ë¡œ ì œí•œ)
            final_threshold = max(0.2, min(0.8, final_threshold))
            
            # ì„ê³„ê°’ ìœ íš¨ì„± ê²€ì‚¬ (NaN ì²´í¬)
            if np.isnan(final_threshold):
                return 0.5  # NaNì¸ ê²½ìš° ê¸°ë³¸ê°’ ë°˜í™˜
                
            return final_threshold
        else:
            return 0.5  # ìœ íš¨í•œ ì„ê³„ê°’ì„ ì°¾ì§€ ëª»í•œ ê²½ìš°
            
    except Exception as e:
        try:
            import streamlit as st
            st.error(f"ìµœì  ì„ê³„ê°’ ì°¾ê¸° ì˜¤ë¥˜: {str(e)}")
        except ImportError:
            print(f"ìµœì  ì„ê³„ê°’ ì°¾ê¸° ì˜¤ë¥˜: {str(e)}")
            import traceback
            print(traceback.format_exc())
        return 0.5  # ì˜¤ë¥˜ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜

def objective(trial, X_train, X_test, y_train, y_test, model_type, class_weight=None):
    """Optunaì˜ ëª©ì  í•¨ìˆ˜: ëª¨ë¸ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ íŠœë‹í•©ë‹ˆë‹¤."""
    try:
        if model_type == 'RandomForest':
            params = {
                'n_estimators': trial.suggest_int('n_estimators', 50, 500),  # ìƒí•œ ì¦ê°€
                'max_depth': trial.suggest_int('max_depth', 3, 15),  # ìƒí•œ ì¦ê°€
                'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),
                'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),
                'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', None]),  # ì¶”ê°€
                'bootstrap': trial.suggest_categorical('bootstrap', [True, False]),  # ì¶”ê°€
                'random_state': 42,
                'class_weight': class_weight
            }
            model = RandomForestClassifier(**params)

        elif model_type == 'GradientBoosting':
            params = {
                'n_estimators': trial.suggest_int('n_estimators', 50, 500),  # ìƒí•œ ì¦ê°€
                'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.3, log=True),  # í•˜í•œ ê°ì†Œ, ë¡œê·¸ ìŠ¤ì¼€ì¼ ì‚¬ìš©
                'max_depth': trial.suggest_int('max_depth', 3, 15),  # ìƒí•œ ì¦ê°€
                'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),
                'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),
                'subsample': trial.suggest_float('subsample', 0.5, 1.0),
                'max_features': trial.suggest_float('max_features', 0.3, 1.0),  # ì¶”ê°€
                'random_state': 42
            }
            model = GradientBoostingClassifier(**params)
        
        else:
            raise ValueError(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” ëª¨ë¸ ìœ í˜•: {model_type}")
        
        # ëª¨ë¸ í›ˆë ¨
        model.fit(X_train, y_train)
        
        # ì„±ëŠ¥ í‰ê°€
        y_prob = model.predict_proba(X_test)[:, 1]
        fpr, tpr, _ = roc_curve(y_test, y_prob)
        roc_auc = auc(fpr, tpr)
        
        return roc_auc
    except Exception as e:
        st.error(f"í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì˜¤ë¥˜: {str(e)}")
        return 0.0  # ì˜¤ë¥˜ ì‹œ ìµœì € ì ìˆ˜ ë°˜í™˜

def save_complete_model(model, X, selected_features, params, results, category_dir):
    """ëª¨ë¸ê³¼ ê´€ë ¨ ì •ë³´ë¥¼ ì™„ì „í•˜ê²Œ ì €ì¥í•˜ëŠ” í•¨ìˆ˜"""
    try:
        # 1. ëª¨ë¸ ìì²´ ì €ì¥
        model_path = os.path.join(category_dir, 'best_model.pkl')
        joblib.dump(model, model_path)
        #st.write(f"model saved: {model_path}")
        
        # 2. ëª¨ë¸ ì •ë³´ ì €ì¥ (ëª¨ë¸ê³¼ ê´€ë ¨ëœ ëª¨ë“  ì¤‘ìš” ì •ë³´ í¬í•¨)
        model_info = {
            'model_type': type(model).__name__,
            'model_parameters': params,
            'features': X.columns.tolist(),  # ëª¨ë“  ì‚¬ìš©ëœ íŠ¹ì„± ëª©ë¡
            'selected_features': selected_features,  # ì„ íƒëœ íŠ¹ì„± ëª©ë¡
            'performance': results  # ëª¨ë¸ ì„±ëŠ¥ ì •ë³´
        }
        
        # íŠ¹ì„± ì¤‘ìš”ë„ê°€ ìˆëŠ” ê²½ìš° ì¶”ê°€
        if hasattr(model, 'feature_importances_'):
            feature_importance = pd.Series(
                model.feature_importances_,
                index=X.columns
            ).sort_values(ascending=False)
            
            model_info['feature_importance'] = feature_importance.to_dict()
        
        # ëª¨ë¸ ì •ë³´ë¥¼ ë³„ë„ íŒŒì¼ë¡œ ì €ì¥
        info_path = os.path.join(category_dir, 'model_info.pkl')
        joblib.dump(model_info, info_path)
        #st.write(f"Completed Saving Model Infomation: {info_path}")
        
        # 3. ì½ê¸° ì‰¬ìš´ í˜•ì‹ìœ¼ë¡œë„ ì €ì¥ (JSON)
        json_info = model_info.copy()
        # JSON ì§ë ¬í™”ë¥¼ ìœ„í•´ ë³µì¡í•œ ê°ì²´ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
        json_info['model_parameters'] = {k: str(v) for k, v in params.items()}
        
        json_path = os.path.join(category_dir, 'model_info.json')
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(json_info, f, indent=2, ensure_ascii=False)
        #st.write(f"â¬‡ï¸Saved as JSON: {json_path}")
        
        return model_path, info_path
    except Exception as e:
        st.error(f"ëª¨ë¸ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return None, None

def build_improved_prediction_model(model_data, category, output_dir, n_features=30, n_trials=50):
    """ê°œì„ ëœ ì˜ˆì¸¡ ëª¨ë¸ì„ êµ¬ì¶•í•©ë‹ˆë‹¤. ì‹œê°í™”ëŠ” ìƒì„±í•˜ì—¬ ì„¸ì…˜ì— ì €ì¥í•˜ì§€ë§Œ í™”ë©´ì—ëŠ” í‘œì‹œí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."""

   
    # ë°ì´í„° ê²€ì¦
    if category not in model_data:
        st.error(f"{category} ì¹´í…Œê³ ë¦¬ì— ëŒ€í•œ ëª¨ë¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None
    
    df = model_data[category]
    
    # ë°ì´í„° í¬ê¸° í™•ì¸
    if len(df) < 30:
        st.error(f"{category} ì¹´í…Œê³ ë¦¬ì˜ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤ ({len(df)} rows). ëª¨ë¸ë§ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return None
    
    # íƒ€ê²Ÿ ë³€ìˆ˜ ì œì™¸í•œ ëª¨ë“  íŠ¹ì„± ì»¬ëŸ¼
    all_feature_columns = [col for col in df.columns if col != category]
    
    # ì…ë ¥ ê²€ì¦
    if len(all_feature_columns) == 0:
        st.error(f"ì˜¤ë¥˜: ì‚¬ìš© ê°€ëŠ¥í•œ ë³€ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤. df ì»¬ëŸ¼: {df.columns.tolist()}")
        return None

    # íŠ¹ì„± ë° íƒ€ê²Ÿ ë³€ìˆ˜ ë¶„ë¦¬
    X = df[all_feature_columns]
    
    # ë°©í–¥ ì˜ˆì¸¡ (ì´ì§„ ë¶„ë¥˜: 0 ë˜ëŠ” 1)
    direction = np.where(df[category] > 0, 1, 0)
    
    # í´ë˜ìŠ¤ ë¶ˆê· í˜• í™•ì¸
    class_count = np.bincount(direction)
    total_samples = len(direction)
    
    # í´ë˜ìŠ¤ ë¶„í¬ê°€ ë„ˆë¬´ ê·¹ë‹¨ì ì¸ì§€ í™•ì¸
    class_ratio = min(class_count) / total_samples
    if class_ratio < 0.1:  # 10% ë¯¸ë§Œì˜ í´ë˜ìŠ¤ê°€ ìˆìœ¼ë©´ ê²½ê³ 
        st.warning(f"Significant class imbalance detected: the minority class represents just {class_ratio:.2%} of the data.")
    
    # íŠ¹ì„± ì„ íƒ (ë„ˆë¬´ ë§ì€ íŠ¹ì„±ì€ ê³¼ì í•©ì„ ìœ ë°œí•  ìˆ˜ ìˆìŒ)
    try:
        selected_features = select_best_features(X, direction, all_feature_columns, n_features=n_features)
        
        # ì„ íƒëœ íŠ¹ì„±ë§Œ ì‚¬ìš©
        X = X[selected_features]
    except Exception as e:
        st.error(f"íŠ¹ì„± ì„ íƒ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        # ì˜¤ë¥˜ ì‹œ ì›ë˜ íŠ¹ì„± ì‚¬ìš©
        selected_features = all_feature_columns[:min(n_features, len(all_feature_columns))]
        X = X[selected_features]
        st.write(f"ì˜¤ë¥˜ë¡œ ì¸í•´ ì²« {len(selected_features)}ê°œ íŠ¹ì„±ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    
    # ì‹¬ê°í•œ í´ë˜ìŠ¤ ë¶ˆê· í˜•ì´ ìˆëŠ” ê²½ìš° í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì ìš©
    class_weight = None
    if min(class_count) / total_samples < 0.3:  # ê¸°ì¤€ì¹˜ ìƒí–¥ ì¡°ì •
        # ê¸°ì¡´ 'balanced' ëŒ€ì‹  ëª…ì‹œì ìœ¼ë¡œ down í´ë˜ìŠ¤ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜ ë¶€ì—¬
        down_class_idx = np.argmin(class_count)  # ì†Œìˆ˜ í´ë˜ìŠ¤ (ì•„ë§ˆë„ 'down')
        up_class_idx = 1 - down_class_idx        # ë‹¤ìˆ˜ í´ë˜ìŠ¤ (ì•„ë§ˆë„ 'up')
        
        # down í´ë˜ìŠ¤ì— 2ë°° ~ 3ë°° ê°€ì¤‘ì¹˜ (ë¶ˆê· í˜• ì •ë„ì— ë”°ë¼ ì¡°ì •)
        weight_ratio = max(2.0, min(3.0, 1 / (min(class_count) / total_samples)))
        
        class_weight = {down_class_idx: weight_ratio, up_class_idx: 1.0}
        st.info(f"Due to class imbalance, a weight of {weight_ratio:.2f}x is applied to class {down_class_idx}.")
    
    # í…ŒìŠ¤íŠ¸ í¬ê¸° ë¹„ìœ¨ í†µì¼
    test_size_ratio = 0.15   # 20% í…ŒìŠ¤íŠ¸, 70% í›ˆë ¨
    
    # í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë¶„í•  (ì‹œê³„ì—´ íŠ¹ì„± ìœ ì§€)
    split_index = int(len(X) * (1 - test_size_ratio))
    if split_index <= 0 or split_index >= len(X):
        st.error(f"ë°ì´í„° ë¶„í•  ì˜¤ë¥˜: split_index={split_index}, ë°ì´í„° ê¸¸ì´={len(X)}")
        return None
    
    X_train, X_test = X.iloc[:split_index], X.iloc[split_index:]
    y_train, y_test = direction[:split_index], direction[split_index:]
    
    # ì¹´í…Œê³ ë¦¬ë³„ ë””ë ‰í† ë¦¬ ìƒì„±
    category_dir = os.path.join(output_dir, 'categories', category)
    if not os.path.exists(category_dir):
        os.makedirs(category_dir)
    
    model_types = ["RandomForest", "GradientBoosting"]

    model_results = {}
    best_auc = 0
    best_model_name = None
    best_model = None
    
    # ê° ëª¨ë¸ ìœ í˜•ì— ëŒ€í•´ Optuna íŠœë‹ ìˆ˜í–‰
    for model_idx, model_type in enumerate(model_types):
        try:
            # Optuna ì—°êµ¬ ê°ì²´ ìƒì„± (maximizeë¡œ ìˆ˜ì •)
            study = optuna.create_study(direction='maximize')
            
            # ìµœì í™” ì‹¤í–‰
            study.optimize(
                lambda trial: objective(
                    trial, X_train, X_test, y_train, y_test,
                    model_type, class_weight
                ), 
                n_trials=n_trials
            )
        
            # ìµœì  ëª¨ë¸ ì¬êµ¬ì„±
            if model_type == 'RandomForest':
                best_params = study.best_params.copy()
                best_params['class_weight'] = class_weight
                best_params['random_state'] = 42
                tuned_model = RandomForestClassifier(**best_params)
            else:  # GradientBoosting
                best_params = study.best_params.copy()
                best_params['random_state'] = 42
                tuned_model = GradientBoostingClassifier(**best_params)
            
            # ëª¨ë¸ í›ˆë ¨
            tuned_model.fit(X_train, y_train)
            # ì˜ˆì¸¡ ì „ X_testì˜ NaN ê²€ì¦ ë° ì²˜ë¦¬
            has_nan_before = np.isnan(X_test.values).any()
            if has_nan_before:
                st.warning(f"{model_type} ëª¨ë¸ ì˜ˆì¸¡ ì „ X_testì— NaNì´ ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
                # ê° ì—´ì˜ ì¤‘ì•™ê°’ìœ¼ë¡œ NaN ëŒ€ì²´
                X_test = X_test.fillna(X_train.median())
                st.info(f"NaNì„ ê° íŠ¹ì„±ì˜ ì¤‘ì•™ê°’ìœ¼ë¡œ ëŒ€ì²´í–ˆìŠµë‹ˆë‹¤.")

            # ì´ì œ NaN ì—†ëŠ” ìƒíƒœë¡œ ì˜ˆì¸¡
            y_pred = tuned_model.predict(X_test)
            y_prob = tuned_model.predict_proba(X_test)[:, 1]

            if np.isnan(y_pred).all():
                st.error(f"{category} ì˜ˆì¸¡ ê²°ê³¼ê°€ ëª¨ë‘ NaNì…ë‹ˆë‹¤. ëª¨ë¸ì„ ì €ì¥í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                return None
            # âœ… 4. ì˜ˆì¸¡ í¸í–¥ ì²´í¬
            if len(np.unique(y_pred)) <= 1:
                st.warning(f"The predictions for {category} are overly biased toward one class.")
            # ì˜ˆì¸¡ ê²°ê³¼ í™•ì¸ (ë””ë²„ê¹…ìš©)
            if np.isnan(y_pred).any():
                st.error(f"X_testì˜ NaNì„ ì²˜ë¦¬í–ˆìŒì—ë„ y_predì— NaNì´ ìˆìŠµë‹ˆë‹¤. ëª¨ë¸ ìì²´ì— ë¬¸ì œê°€ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                print(f"X_test had NaN before: {has_nan_before}")
                print(f"X_test NaN count after: {np.isnan(X_test.values).sum()}")
                print(f"y_pred NaN count: {np.isnan(y_pred).sum()}")
                print(f"y_pred unique values: {np.unique(y_pred)}")
                
                # NaN ìœ„ì¹˜ ì°¾ê¸°
                nan_indices = np.where(np.isnan(y_pred))[0]
                if len(nan_indices) > 0:
                    print(f"NaN indices in y_pred: {nan_indices[:5]}")
                    # í•´ë‹¹ í–‰ ì‚´í´ë³´ê¸°
                    for idx in nan_indices[:2]:  # ì²˜ìŒ 2ê°œë§Œ ì¶œë ¥
                        print(f"X_test row {idx}:")
                        print(X_test.iloc[idx])

            # ì„±ëŠ¥ í‰ê°€
            accuracy = accuracy_score(y_test, y_pred)
            fpr, tpr, _ = roc_curve(y_test, y_prob)
            roc_auc = auc(fpr, tpr)
            
            # ê²°ê³¼ ì €ì¥
            model_results[model_type] = {
                "model": tuned_model,
                "accuracy": accuracy,
                "roc_auc": roc_auc,
                "params": best_params,
                "y_pred": y_pred,
                "y_prob": y_prob
            }
            
            # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì—…ë°ì´íŠ¸
            if roc_auc > best_auc:
                best_auc = roc_auc
                best_model_name = model_type
                best_model = tuned_model
        
        except Exception as e:
            st.error(f"{model_type} ëª¨ë¸ ìµœì í™” ì¤‘ ì˜¤ë¥˜: {str(e)}")
    
    # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ì´ ì—†ëŠ” ê²½ìš°
    if best_model is None or best_model_name is None:
        st.error("ëª¨ë“  ëª¨ë¸ í•™ìŠµì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        return None
    
    # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ê´€ë ¨ ë³€ìˆ˜ ì„¤ì •
    best_params = model_results[best_model_name]["params"]
    y_pred = model_results[best_model_name]["y_pred"]
    y_prob = model_results[best_model_name]["y_prob"]
    
    # í•˜ì´í¼íŒŒë¼ë¯¸í„° ì €ì¥
    with open(os.path.join(category_dir, 'best_params.txt'), 'w') as f:
        for key, value in best_params.items():
            f.write(f"{key}: {value}\n")
    
    # ì„±ëŠ¥ í‰ê°€
    accuracy = accuracy_score(y_test, y_pred)
    classification_rep = classification_report(y_test, y_pred, output_dict=True)
    
    # íŠ¹ì„± ì¤‘ìš”ë„
    if hasattr(best_model, 'feature_importances_'):
        feature_importance = pd.Series(
            best_model.feature_importances_,
            index=X.columns
        ).sort_values(ascending=False)
    else:
        feature_importance = pd.Series(
            [0] * len(X.columns),
            index=X.columns
        )
    
    # ìµœì  ì„ê³„ê°’ ì°¾ê¸°
    optimal_threshold = find_optimal_threshold(y_test, y_prob)
    if np.isnan(optimal_threshold):
        st.warning(f"ìµœì  ì„ê³„ê°’ì´ NaNì…ë‹ˆë‹¤. ê¸°ë³¸ê°’ 0.5ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        optimal_threshold = 0.5
    if np.allclose(y_prob, y_prob[0]):
        st.warning(f"{category}ì˜ ì˜ˆì¸¡ í™•ë¥ ì´ ëª¨ë‘ ìœ ì‚¬í•©ë‹ˆë‹¤. ëª¨ë¸ ë¶ˆì•ˆì • ê°€ëŠ¥ì„± ìˆìŒ.")
        # í´ë˜ìŠ¤ ë¶„í¬ í™•ì¸
    up_ratio = np.mean(y_test)
    down_ratio = 1 - up_ratio
    # 'up' ì˜ˆì¸¡ì´ ê³¼ë„í•˜ê²Œ ë§ì€ì§€ í™•ì¸
    y_pred_default = (y_prob >= 0.5).astype(int)
    pred_up_ratio = np.mean(y_pred_default)
    if pred_up_ratio > up_ratio + 0.1:  # ì‹¤ì œë³´ë‹¤ 10% ì´ìƒ ë§ìœ¼ë©´
            
        # ì„ê³„ê°’ ìƒí–¥ ì¡°ì • (up ì˜ˆì¸¡ì„ ì¤„ì´ê¸° ìœ„í•´)
        adjusted_threshold = max(optimal_threshold + 0.1, 0.6)
        st.write(f"The model is overpredicting 'up' (Predicted: {pred_up_ratio:.2f}, Actual: {up_ratio:.2f}). Adjusting the threshold to {adjusted_threshold:.2f}.")
        optimal_threshold = min(adjusted_threshold, 0.8)  # ìµœëŒ€ 0.8ë¡œ ì œí•œ


    # ìµœì  ì„ê³„ê°’ ê¸°ì¤€ ì˜ˆì¸¡ 
    y_pred_optimal = (y_prob >= optimal_threshold).astype(int)

    # NaN ì²´í¬ ë° ì²˜ë¦¬
    if np.isnan(y_pred_optimal).any():
        st.warning(f"y_pred_optimalì— NaN ê°’ì´ ìˆìŠµë‹ˆë‹¤. ëŒ€ì²´ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
        # ê°€ì¥ ë§ì€ í´ë˜ìŠ¤ë¡œ ëŒ€ì²´
        most_common_class = np.bincount(y_train).argmax()
        y_pred_optimal = np.nan_to_num(y_pred_optimal, nan=most_common_class)

    # í•­ìƒ ì •í™•ë„ ê³„ì‚° (NaN ê°’ì˜ ìœ ë¬´ì™€ ê´€ê³„ì—†ì´)
    accuracy_optimal = accuracy_score(y_test, y_pred_optimal)
    # ì¡°ì • í›„ í™•ì¸
    adjusted_up_ratio = np.mean(y_pred_optimal)
    st.write(f"'Up' prediction ratio before adjustment: {pred_up_ratio:.2f}, after adjustment: {adjusted_up_ratio:.2f}, actual: {up_ratio:.2f}.")

   
    print("y_pred_optimal values:", np.unique(y_pred_optimal))
    

    # ì„±ëŠ¥ ê²°ê³¼ ëª¨ìŒ
    performance_results = {
        "accuracy": float(accuracy),
        "roc_auc": float(best_auc),
        "optimal_threshold": float(optimal_threshold),
        "accuracy_optimal": float(accuracy_optimal),
        "classification_report": classification_rep
    }

    # í–¥ìƒëœ ëª¨ë¸ ì €ì¥ í•¨ìˆ˜ í˜¸ì¶œ
    model_path, info_path = save_complete_model(
        model=best_model,
        X=X,
        selected_features=selected_features,
        params=best_params,
        results=performance_results,
        category_dir=category_dir
    )
    
    # ì„¸ì…˜ ìƒíƒœì— ì‹œê°í™” ì €ì¥ì†Œ ì´ˆê¸°í™”
    if 'visualizations' not in st.session_state:
        st.session_state['visualizations'] = {}
    
    # ì‹œê°í™” ì„¹ì…˜ ì¶”ê°€ - í™”ë©´ì— í‘œì‹œí•˜ì§€ ì•Šê³  ì„¸ì…˜ì—ë§Œ ì €ì¥
    # íŠ¹ì„± ì¤‘ìš”ë„ ì‹œê°í™”
    try:
        feature_importance_fig, ax = plt.subplots(figsize=(10, 8))
        top_features = feature_importance.head(min(15, len(feature_importance)))
        top_features.plot(kind='barh', color='teal', ax=ax)
        
        total = top_features.sum()
        for i, v in enumerate(top_features):
            ax.text(v + 0.01, i, f'{v:.3f} ({v/total * 100:.1f}%)', color='black', va='center', fontweight='bold')
        
        plt.title(f'{category} - Feature Importance', fontsize=16)
        plt.xlabel('Importance', fontsize=14)
        plt.ylabel('Feature', fontsize=14)
        plt.grid(True, axis='x', alpha=0.3)
        plt.tight_layout()
        
        # ì„¸ì…˜ ìƒíƒœì— ì‹œê°í™” ì €ì¥
        st.session_state['visualizations'][f"{category}_feature_importance"] = feature_importance_fig
        
        # íŒŒì¼ë¡œ ì €ì¥
        plt.savefig(os.path.join(category_dir, 'feature_importance.png'), dpi=300, bbox_inches='tight')
        plt.close()
    except Exception as e:
        st.error(f"íŠ¹ì„± ì¤‘ìš”ë„ ì‹œê°í™” ì˜¤ë¥˜: {str(e)}")
    
    # í˜¼ë™ í–‰ë ¬ (Confusion Matrix)
    try:
        cm = confusion_matrix(y_test, y_pred)
        confusion_matrix_fig, ax = plt.subplots(figsize=(10, 8))
        # ë°±ë¶„ìœ¨ë¡œ ë³€í™˜
        cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100
        # í˜¼ë™ í–‰ë ¬ ì‹œê°í™” (ê°œìˆ˜ì™€ ë°±ë¶„ìœ¨ í•¨ê»˜ í‘œì‹œ)
        sns.heatmap(cm, annot=np.asarray([
            [f'{cm[0,0]} ({cm_percent[0,0]:.1f}%)', f'{cm[0,1]} ({cm_percent[0,1]:.1f}%)'],
            [f'{cm[1,0]} ({cm_percent[1,0]:.1f}%)', f'{cm[1,1]} ({cm_percent[1,1]:.1f}%)']
        ]), fmt='', cmap='Blues', cbar=False, annot_kws={"size": 14}, ax=ax)
        
        plt.title(f'Direction Prediction Confusion Matrix\nAccuracy: {accuracy:.2%}', fontsize=16)
        plt.xlabel('Predicted Value', fontsize=14)
        plt.ylabel('Actual Value', fontsize=14)
        plt.xticks([0.5, 1.5], ['Down', 'Up'], fontsize=12)
        plt.yticks([0.5, 1.5], ['Down', 'Up'], fontsize=12)
        plt.tight_layout()
        
        # ì„¸ì…˜ ìƒíƒœì— ì‹œê°í™” ì €ì¥
        st.session_state['visualizations'][f"{category}_Confusion Matrix"] = confusion_matrix_fig
        
        # íŒŒì¼ë¡œ ì €ì¥
        plt.savefig(os.path.join(category_dir, 'confusion_matrix.png'), dpi=300, bbox_inches='tight')
        plt.close()
    except Exception as e:
        st.error(f"í˜¼ë™ í–‰ë ¬ ì‹œê°í™” ì˜¤ë¥˜: {str(e)}")
    
    # ROC ê³¡ì„ 
    try:
        fpr, tpr, thresholds = roc_curve(y_test, y_prob)
        roc_auc = auc(fpr, tpr)
        
        roc_curve_fig, ax = plt.subplots(figsize=(10, 8))
        plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC Curve (AUC = {roc_auc:.3f})')
        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Predictions')
        plt.xlim([0.0, 1.0])
        plt.ylim([0.0, 1.05])
        plt.xlabel('False Positive Rate', fontsize=14)
        plt.ylabel('True Positive Rate', fontsize=14)
        plt.title(f'ROC Curve', fontsize=16)
        plt.legend(loc="lower right", fontsize=12)
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        
        # ì„¸ì…˜ ìƒíƒœì— ì‹œê°í™” ì €ì¥
        st.session_state['visualizations'][f"{category}roc_curve"] = roc_curve_fig
        
        # íŒŒì¼ë¡œ ì €ì¥
        plt.savefig(os.path.join(category_dir, 'roc_curve.png'), dpi=300, bbox_inches='tight')
        plt.close()
    except Exception as e:
        st.error(f"ROC ê³¡ì„  ì‹œê°í™” ì˜¤ë¥˜: {str(e)}")
    
    # í´ë˜ìŠ¤ë³„ ì˜ˆì¸¡ í™•ë¥  ë¶„í¬
    try:
        probability_distribution_fig, ax = plt.subplots(figsize=(10, 8))
        sns.histplot(y_prob[y_test == 0], bins=20, alpha=0.5, color='red', label='Actual Down (0)', ax=ax)
        sns.histplot(y_prob[y_test == 1], bins=20, alpha=0.5, color='green', label='Actual Up (1)', ax=ax)
        
        plt.axvline(x=0.5, color='gray', linestyle='--', label='Standard threshold (0.5)')
        plt.axvline(x=optimal_threshold, color='black', linestyle='--', label=f'Optimal threshold ({optimal_threshold:.3f})')
        plt.xlabel('Up Class[1] Prediction Probability', fontsize=14)
        plt.ylabel('Frequency', fontsize=14)
        plt.title(f'Prediction Probability Distributions', fontsize=16)
        plt.legend(fontsize=12)
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        
        # ì„¸ì…˜ ìƒíƒœì— ì‹œê°í™” ì €ì¥
        st.session_state['visualizations'][f"{category}probability_distribution"] = probability_distribution_fig
        
        # íŒŒì¼ë¡œ ì €ì¥
        plt.savefig(os.path.join(category_dir, 'probability_distribution.png'), dpi=300, bbox_inches='tight')
        plt.close()
    except Exception as e:
        st.error(f"í™•ë¥  ë¶„í¬ ì‹œê°í™” ì˜¤ë¥˜: {str(e)}")
   
    # ì‹œê°„ì  íŒ¨í„´ ë¶„ì„
    if hasattr(df, 'index') and len(df.index) == len(y_test) + len(y_train):
        try:
            prediction_trends_fig, ax = plt.subplots(figsize=(10, 8))
            
            # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ ì¸ë±ìŠ¤ ê°€ì ¸ì˜¤ê¸°
            test_indices = df.index[-len(y_test):]
            
            # ì˜ˆì¸¡ ê²°ê³¼ì™€ ì‹¤ì œ ê°’ ì‹œê°í™”
            plt.plot(test_indices, y_test, 'bo-', label='Actual direction', alpha=0.6, markersize=8)
            plt.plot(test_indices, y_pred, 'ro-', label='Predicted direction', alpha=0.6, markersize=8)
            
            # ì˜¤ë¶„ë¥˜ í¬ì¸íŠ¸ ê°•ì¡°
            incorrect_mask = y_test != y_pred
            incorrect_idx = test_indices[incorrect_mask]
            incorrect_actual = y_test[incorrect_mask]
            plt.scatter(incorrect_idx, incorrect_actual, color='yellow', s=150, zorder=5, 
                        label='Misclassification', edgecolors='black')
            
            plt.title(f'Predicted vs actual direction over time', fontsize=16)
            plt.xlabel('Time', fontsize=14)
            plt.ylabel('Direction (0=Down, 1=UP)', fontsize=14)
            plt.legend(fontsize=12)
            plt.grid(True, alpha=0.3)
            plt.tight_layout()
            
            # ì„¸ì…˜ ìƒíƒœì— ì‹œê°í™” ì €ì¥
            st.session_state['visualizations'][f"{category}temporal_prediction"] = prediction_trends_fig
            
            # íŒŒì¼ë¡œ ì €ì¥
            plt.savefig(os.path.join(category_dir, 'temporal_prediction.png'), dpi=300, bbox_inches='tight')
            plt.close()
        except Exception as e:
            st.error(f"ì‹œê°„ì  íŒ¨í„´ ì‹œê°í™” ì˜¤ë¥˜: {str(e)}")
    
    # ë¶„ë¥˜ ë³´ê³ ì„œ í‘œì‹œ
    try:
        class_report_df = pd.DataFrame(classification_rep).transpose()
    except Exception as e:
        st.error(f"Error Displaying Classification Report: {str(e)}")
    
    # ê²°ê³¼ ì €ì¥
    result = {
        'model_name': best_model_name,
        'accuracy': float(accuracy),
        'accuracy_optimal': float(accuracy_optimal),
        'optimal_threshold': float(optimal_threshold),
        'classification_report': classification_rep,
        'feature_importance': feature_importance.to_dict() if hasattr(best_model, 'feature_importances_') else {},
        'roc_auc': float(roc_auc),
        'model_path': os.path.join(category_dir, 'best_model.pkl'),
        'selected_features': selected_features
    }
    
    # ê²°ê³¼ë¥¼ JSONìœ¼ë¡œ ì €ì¥
    try:
        with open(os.path.join(category_dir, 'model_results.json'), 'w') as f:
            json.dump(result, f, indent=4)
    except Exception as e:
        st.error(f"ê²°ê³¼ ì €ì¥ ì˜¤ë¥˜: {str(e)}")
    
    # ëª¨ë¸ ë©”íƒ€ë°ì´í„° ìƒì„± ë° ì €ì¥
    try:
        model_metadata = {
            'execution_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'category': category,
            'data_size': {
                'total_samples': int(len(direction)),
                'train_samples': int(len(y_train)),
                'test_samples': int(len(y_test)),
                'features_count': int(len(selected_features))
            },
            'features': {
                'all_features': X.columns.tolist(),
                'top_importance_features': feature_importance.head(min(10, len(feature_importance))).index.tolist()
            },
            'class_distribution': {
                'train': {
                    'up': int(sum(y_train)),
                    'down': int(len(y_train) - sum(y_train)),
                    'up_ratio': float(sum(y_train) / len(y_train))
                },
                'test': {
                    'up': int(sum(y_test)),
                    'down': int(len(y_test) - sum(y_test)),
                    'up_ratio': float(sum(y_test) / len(y_test))
                }
            },
            'performance_summary': {
                'accuracy': float(accuracy),
                'accuracy_optimal': float(accuracy_optimal),
                'optimal_threshold': float(optimal_threshold),
                'roc_auc': float(roc_auc),
                'f1_score': float(classification_rep['1']['f1-score']),  # ìƒìŠ¹ í´ë˜ìŠ¤ì˜ F1 ì ìˆ˜
            },
            'model_info': {
                'type': best_model_name,
                'parameters': str(best_model.get_params())
            },
            'files': {
                'model_path': 'best_model.pkl',
                'performance_plots': [
                    'confusion_matrix.png',
                    'roc_curve.png',
                    'feature_importance.png',
                    'probability_distribution.png'
                ]
            }
        }
        
        # ì‹œê°„ì  íŠ¹ì„± í”Œë¡¯ì´ ìƒì„±ëœ ê²½ìš° ì¶”ê°€ 
        if hasattr(df, 'index') and len(df.index) == len(y_test) + len(y_train):
            model_metadata['files']['performance_plots'].append('temporal_prediction.png')
        
        # ë©”íƒ€ë°ì´í„°ë¥¼ JSONìœ¼ë¡œ ì €ì¥
        with open(os.path.join(category_dir, 'model_metadata.json'), 'w', encoding='utf-8') as f:
            json.dump(model_metadata, f, ensure_ascii=False, indent=2)
    except Exception as e:
        st.error(f"Error saving metadata: {str(e)}")
    
    # ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ëª¨ë¸ ë°ì´í„°ì— ì €ì¥
    try:
        # ì›ë³¸ ë°ì´í„°í”„ë ˆì„ ë³µì‚¬
        df_with_predictions = df.copy()
        if np.isnan(y_pred_optimal).any():
            st.warning("ì˜ˆì¸¡ ê²°ê³¼ì— NaN ê°’ì´ ìˆìŠµë‹ˆë‹¤. ëŒ€ì²´ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
            y_pred_optimal = np.nan_to_num(y_pred_optimal, nan=0)
        # í…ŒìŠ¤íŠ¸ ì¸ë±ìŠ¤ì— í•´ë‹¹í•˜ëŠ” í–‰ì— ì˜ˆì¸¡ ê²°ê³¼ ì¶”ê°€
        df_with_predictions.loc[X_test.index, 'predictions'] = y_pred_optimal
        
        # ì—…ë°ì´íŠ¸ëœ ë°ì´í„°í”„ë ˆì„ì„ ëª¨ë¸ ë°ì´í„°ì— ì €ì¥ (ëª¨ë¸ ë°ì´í„°ë¥¼ ì§ì ‘ ì—…ë°ì´íŠ¸)
        model_data[category] = df_with_predictions
        if 'model_data' not in st.session_state:
                st.session_state['model_data'] = {}
                st.session_state['model_data'][category] = df_with_predictions

        if 'enhanced_model_data' not in st.session_state:
            st.session_state['enhanced_model_data'] = {}

        st.session_state['enhanced_model_data'][category] = df_with_predictions
        
        # ì˜ˆì¸¡ ê²°ê³¼ì— ëŒ€í•œ ì •ë³´ ì¶”ê°€
        result['test_indices'] = X_test.index.tolist()
        result['predictions'] = y_pred_optimal.tolist()
        
        # ê°„ë‹¨í•œ ì„±ê³µ ë©”ì‹œì§€ë§Œ í‘œì‹œ
        st.success(f"âœ… {category} Model building for each category is complete! Please check the results in each tab.")
    except Exception as e:
        st.warning(f"ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {str(e)}")
    
    return result



def analyze_data_quality(daily_avg_prices, daily_indicators):
    """Analyze data quality and display basic information."""
    st.subheader("Data Quality Analysis")

    # Data validation
    if daily_avg_prices is None or daily_indicators is None:
        st.error("Missing required data for quality analysis.")
        return

    # Analyze price data
    st.write("### Price Data Overview:")
    st.write(f"Date Range: {daily_avg_prices.index.min()} ~ {daily_avg_prices.index.max()}")
    #st.write(f"Number of Categories: {daily_avg_prices.shape[1]}")
    #st.write(f"Number of Dates: {daily_avg_prices.shape[0]}")
    st.write(f"Number of Missing Values (NaN): {daily_avg_prices.isna().sum().sum()}")

    # Basic statistics per category
    st.write("### Basic Statistics by Category:")
    stats_df = pd.DataFrame()

    for category in daily_avg_prices.columns:
        data = daily_avg_prices[category]
        stats = {
            "Count": data.count(),
            "Mean": data.mean(),
            "Standard Deviation": data.std(),
            "Min": data.min(),
            "Max": data.max(),
            "Coefficient of Variation": data.std() / data.mean() if data.mean() != 0 else float('nan')
        }
        stats_df[category] = pd.Series(stats)

    st.dataframe(stats_df.T)

    # Analyze indicator data
    st.write("### Indicator Data Overview:")
    st.write(f"Date Range: {daily_indicators.index.min()} ~ {daily_indicators.index.max()}")
    st.write(f"Number of Indicators: {daily_indicators.shape[1]}")
    st.write(f"Number of Dates: {daily_indicators.shape[0]}")
    st.write(f"Number of Missing Values (NaN): {daily_indicators.isna().sum().sum()}")

    # Basic statistics for indicators
    if daily_indicators.shape[1] > 0:
        st.write("### Basic Statistics by Indicator:")
        indicator_stats = daily_indicators.describe().T
        st.dataframe(indicator_stats)

    # Check for date alignment
    common_dates = daily_avg_prices.index.intersection(daily_indicators.index)
    st.write(f"Number of matching dates between price and indicator data: {len(common_dates)}")

    # Check normalized variables
    norm_cols = [col for col in daily_indicators.columns if '_norm' in col]
    st.write(f"Number of normalized indicators: {len(norm_cols)}")

    # Correlation heatmap between top indicators and categories
    st.write("### Correlation between Key Indicators:")
    
    # ìƒìœ„ ê²½ì œ ì§€í‘œì™€ ëª¨ë“  ì¹´í…Œê³ ë¦¬ ì„ íƒ
    try:
        top_indicators = daily_indicators.columns[:min(5, len(daily_indicators.columns))]  # ìƒìœ„ 5ê°œ ì§€í‘œ
        correlation_data = pd.concat([daily_avg_prices, daily_indicators[top_indicators]], axis=1)
        
        # ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ
        fig, ax = plt.subplots(figsize=(10, 7))
        correlation_matrix = correlation_data.corr()
        mask = np.triu(np.ones_like(correlation_matrix, dtype=bool), k=1)
        sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='coolwarm', fmt='.2f', 
                    linewidths=0.5, vmin=-1, vmax=1, center=0, ax=ax)
        plt.title('Correlation between key Indicators', fontsize=16)
        plt.xticks(rotation=45, ha='right')
        plt.tight_layout()
        st.pyplot(fig)
        st.success("""
        ### ğŸ§  Kernaussagen:

        - ğŸ›¡ï¸ **Defense & Gold:** Unsicherheit treibt Gold und RÃ¼stungsaktien nach oben.
        - ğŸ’» **Tech & Dollar:** Starker Dollar schwÃ¤cht Tech-Aktien.
        - ğŸ“‰ **Fed Rate:** HÃ¶here Zinsen bremsen Wirtschaft und Konsum.
        """)
        plt.close()
        

    except Exception as e:
        st.error(f"ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ ìƒì„± ì˜¤ë¥˜: {str(e)}")
    
    # ì‹œê³„ì—´ ë°ì´í„° ì‹œê°í™”
    #st.write("### Price trends by category:")
    # ëˆ„ì  ìˆ˜ìµë¥  ê³„ì‚°
    try:
        daily_returns = daily_avg_prices.pct_change()  # ì¼ê°„ ìˆ˜ìµë¥ 
        cumulative_returns = (1 + daily_returns).cumprod()  # ëˆ„ì  ìˆ˜ìµë¥ 
    except Exception as e:
        st.error(f"ëˆ„ì  ìˆ˜ìµë¥  ê³„ì‚° ì˜¤ë¥˜: {str(e)}")
        cumulative_returns = None

    # ì‹œê³„ì—´ ì‹œê°í™”
    if cumulative_returns is not None:
        #st.write("### Cumulative Returns by Category")
        try:
            fig, ax = plt.subplots(figsize=(10, 8))
            for category in cumulative_returns.columns:
                ax.plot(
                    cumulative_returns.index,
                    cumulative_returns[category],
                    label=category,
                    linewidth=2
                )
            ax.set_title('Cumulative Return by Category', fontsize=16)
            ax.set_xlabel('Date', fontsize=14)
            ax.set_ylabel('Cumulative Return', fontsize=14)
            ax.legend()
            ax.grid(True, alpha=0.3)
            plt.tight_layout()
            #st.pyplot(fig)
            plt.close(fig)
        except Exception as e:
            st.error(f"ëˆ„ì  ìˆ˜ìµë¥  ì‹œê°í™” ì˜¤ë¥˜: {str(e)}")

def detect_extreme_events(df, daily_avg_prices, daily_indicators):
    """
    ê²½ì œ ì§€í‘œ, ê°€ê²© ë° ê°ì„± ì§€í‘œì˜ ê·¹ë‹¨ì  ì´ë²¤íŠ¸ë¥¼ ê°ì§€í•©ë‹ˆë‹¤.
    
    Parameters:
    -----------
    df : DataFrame
        ì›ë³¸ ë°ì´í„°í”„ë ˆì„
    daily_avg_prices : DataFrame
        ì¼ë³„ í‰ê·  ê°€ê²© ë°ì´í„°
    daily_indicators : DataFrame
        ì¼ë³„ ì§€í‘œ ë°ì´í„°
    
    Returns:
    --------
    extreme_events_df : DataFrame
        ê°ì§€ëœ ëª¨ë“  ê·¹ë‹¨ì  ì´ë²¤íŠ¸ë¥¼ í¬í•¨í•˜ëŠ” ë°ì´í„°í”„ë ˆì„
    """
    # ì´ë²¤íŠ¸ ì €ì¥ì„ ìœ„í•œ ë¦¬ìŠ¤íŠ¸
    extreme_events = []
    
    # ë°ì´í„° ìœ íš¨ì„± í™•ì¸ - ê°€ì¥ ë¨¼ì € ì²´í¬
    if df is None or df.empty:
        st.warning("No data for analysis")
        return pd.DataFrame(columns=['date', 'event_type', 'indicator', 'value', 
                                   'threshold', 'percentile', 'description'])
    
    if daily_indicators is None or daily_indicators.empty:
        st.warning("No indicators data")
        return pd.DataFrame(columns=['date', 'event_type', 'indicator', 'value', 
                                   'threshold', 'percentile', 'description'])
    
    if daily_avg_prices is None or daily_avg_prices.empty:
        st.warning("No price data")
        return pd.DataFrame(columns=['date', 'event_type', 'indicator', 'value', 
                                   'threshold', 'percentile', 'description'])
    
    # âœ… ì•ˆì „í•œ datetime ë³€í™˜
    try:
        if 'date' in df.columns and not pd.api.types.is_datetime64_any_dtype(df['date']):
            df['date'] = pd.to_datetime(df['date'], errors='coerce')
        
        if not pd.api.types.is_datetime64_any_dtype(daily_indicators.index):
            daily_indicators.index = pd.to_datetime(daily_indicators.index, errors='coerce')
        
        if not pd.api.types.is_datetime64_any_dtype(daily_avg_prices.index):
            daily_avg_prices.index = pd.to_datetime(daily_avg_prices.index, errors='coerce')
    except Exception as e:
        st.error(f"ë‚ ì§œ ë³€í™˜ ì˜¤ë¥˜: {str(e)}")
        return pd.DataFrame(columns=['date', 'event_type', 'indicator', 'value', 
                                   'threshold', 'percentile', 'description'])
    
    # 1. ê²½ì œ ì§€í‘œ ê·¹ë‹¨ê°’ (ìƒìœ„/í•˜ìœ„ 10%)
    try:
        economic_columns = [col for col in daily_indicators.columns if any(
            econ in col for econ in [
                'GDP', 'CPI', 'Dollar_Index', 'Fed_Funds_Rate', 
                'Gov_Spending', 'Industrial_Production', 'Natural_Gas',
                'Real_Interest_Rate', 'WTI_Oil', '10Y_Treasury_Yield'
            ]
        )]
        
        for col in economic_columns:
            try:
                # ìƒìœ„ 10% ì„ê³„ê°’
                high_threshold = daily_indicators[col].quantile(0.90)
                # í•˜ìœ„ 10% ì„ê³„ê°’
                low_threshold = daily_indicators[col].quantile(0.10)
                
                # ìƒìœ„ 10% ì´ë²¤íŠ¸ ê°ì§€
                high_extreme_dates = daily_indicators[daily_indicators[col] > high_threshold].index
                for date in high_extreme_dates:
                    extreme_events.append({
                        'date': date,
                        'event_type': 'Extreme High',
                        'indicator': col,
                        'value': daily_indicators.loc[date, col],
                        'threshold': high_threshold,
                        'percentile': 90,
                        'description': f'High {col.split("_")[0]} ({daily_indicators.loc[date, col]:.2f})'
                    })
                
                # í•˜ìœ„ 10% ì´ë²¤íŠ¸ ê°ì§€
                low_extreme_dates = daily_indicators[daily_indicators[col] < low_threshold].index
                for date in low_extreme_dates:
                    extreme_events.append({
                        'date': date,
                        'event_type': 'Extreme Low',
                        'indicator': col,
                        'value': daily_indicators.loc[date, col],
                        'threshold': low_threshold,
                        'percentile': 10,
                        'description': f'Low {col.split("_")[0]} ({daily_indicators.loc[date, col]:.2f})'
                    })
            except Exception as e:
                st.warning(f"{col} ì§€í‘œ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                continue
    except Exception as e:
        st.warning(f"ê²½ì œ ì§€í‘œ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}")
    
    # 2. ê°€ê²© ê¸‰ë“±ë½ ê°ì§€ (5% ì´ìƒ ë³€í™”)
    try:
        for category in daily_avg_prices.columns:
            try:
                # ì¼ì¼ ë³€í™”ìœ¨ ê³„ì‚° (í¼ì„¼íŠ¸)
                daily_returns = daily_avg_prices[category].pct_change() * 100
                daily_returns = daily_returns.replace([np.inf, -np.inf], np.nan).dropna()
                
                # ê¸‰ë“± ê°ì§€ (5% ì´ìƒ ìƒìŠ¹)
                surge_dates = daily_returns[daily_returns > 5].index
                for date in surge_dates:
                    extreme_events.append({
                        'date': date,
                        'event_type': 'Price Surge',
                        'indicator': f'{category}_price',
                        'value': daily_returns.loc[date],
                        'threshold': 5,
                        'percentile': None,
                        'description': f'{category} Surge (+{daily_returns.loc[date]:.2f}%)'
                    })
                
                # ê¸‰ë½ ê°ì§€ (5% ì´ìƒ í•˜ë½)
                crash_dates = daily_returns[daily_returns < -5].index
                for date in crash_dates:
                    extreme_events.append({
                        'date': date,
                        'event_type': 'Price Crash',
                        'indicator': f'{category}_price',
                        'value': daily_returns.loc[date],
                        'threshold': -5,
                        'percentile': None,
                        'description': f'{category} Crash ({daily_returns.loc[date]:.2f}%)'
                    })
            except Exception as e:
                st.warning(f"{category} ê°€ê²© ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                continue
    except Exception as e:
        st.warning(f"ê°€ê²© ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}")
    
    # 3. ê·¹ë‹¨ì  ê°ì„± ë³€í™” ê°ì§€
    try:
        sentiment_cols = [col for col in daily_indicators.columns if any(
            sent in col for sent in [
                'sentiment_score', 'fear_greed', 'positive_prob', 
                'negative_prob', 'sentiment_group_Zscore'
            ]
        )]
        
        from scipy import stats
        for col in sentiment_cols:
            try:
                # ë°ì´í„°ê°€ ì¶©ë¶„í•œì§€ í™•ì¸
                if daily_indicators[col].dropna().shape[0] < 2:
                    continue
                
                # Z-score ê³„ì‚°
                z_scores = stats.zscore(daily_indicators[col].dropna())
                z_df = pd.DataFrame({'z_score': z_scores}, index=daily_indicators[col].dropna().index)
                
                # ë§¤ìš° ê¸ì •ì ì¸ ê°ì„± (Z-score > 2)
                very_positive_dates = z_df[z_df['z_score'] > 2].index
                for date in very_positive_dates:
                    extreme_events.append({
                        'date': date,
                        'event_type': 'Extreme Positive Sentiment',
                        'indicator': col,
                        'value': daily_indicators.loc[date, col],
                        'threshold': None,
                        'percentile': None,
                        'description': f'Extreme Positive Sentiment {col} (Z-score > 2)'
                    })
                
                # ë§¤ìš° ë¶€ì •ì ì¸ ê°ì„± (Z-score < -2)
                very_negative_dates = z_df[z_df['z_score'] < -2].index
                for date in very_negative_dates:
                    extreme_events.append({
                        'date': date,
                        'event_type': 'Extreme Negative Sentiment',
                        'indicator': col,
                        'value': daily_indicators.loc[date, col],
                        'threshold': None,
                        'percentile': None,
                        'description': f'Extreme Negative Sentiment{col} (Z-score < -2)'
                    })
            except Exception as e:
                st.warning(f"{col} ê°ì„± ì§€í‘œ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                continue
    except Exception as e:
        st.warning(f"ê°ì„± ì§€í‘œ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}")
    
    # ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜
    if extreme_events:
        try:
            extreme_events_df = pd.DataFrame(extreme_events)
            # ë‚ ì§œ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
            extreme_events_df = extreme_events_df.sort_values('date', ascending=False)
            return extreme_events_df
        except Exception as e:
            st.error(f"ë°ì´í„°í”„ë ˆì„ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return pd.DataFrame(columns=['date', 'event_type', 'indicator', 'value', 
                                       'threshold', 'percentile', 'description'])
    else:
        return pd.DataFrame(columns=['date', 'event_type', 'indicator', 'value', 
                                   'threshold', 'percentile', 'description'])


def detect_technical_patterns(daily_avg_prices, window=20):
    """
    ì£¼ìš” ê¸°ìˆ ì  íŒ¨í„´ì„ ê°ì§€í•©ë‹ˆë‹¤ (ê³¨ë“  í¬ë¡œìŠ¤, ë°ìŠ¤ í¬ë¡œìŠ¤, ì¶”ì„¸ ë°˜ì „ ë“±).
    
    Parameters:
    -----------
    daily_avg_prices : DataFrame
        ì¼ë³„ í‰ê·  ê°€ê²© ë°ì´í„°
    window : int
        ì´ë™í‰ê·  ìœˆë„ìš° í¬ê¸°
    
    Returns:
    --------
    patterns_df : DataFrame
        ê°ì§€ëœ ê¸°ìˆ ì  íŒ¨í„´ì„ í¬í•¨í•˜ëŠ” ë°ì´í„°í”„ë ˆì„
    """
    patterns = []
    
    if daily_avg_prices is None or daily_avg_prices.empty:
        return pd.DataFrame(columns=['date', 'category', 'pattern', 'description'])
    
    # datetime í˜•ì‹ ë³€í™˜
    try:
        if not pd.api.types.is_datetime64_any_dtype(daily_avg_prices.index):
            daily_avg_prices.index = pd.to_datetime(daily_avg_prices.index, errors='coerce')
    except Exception as e:
        st.error(f"ë‚ ì§œ ë³€í™˜ ì˜¤ë¥˜: {str(e)}")
        return pd.DataFrame(columns=['date', 'category', 'pattern', 'description'])
    
    for category in daily_avg_prices.columns:
        try:
            prices = daily_avg_prices[category]
            
            # NaN ê°’ í™•ì¸
            if prices.isnull().all():
                continue
            
            # ì´ë™í‰ê·  ê³„ì‚°
            ma_short = prices.rolling(window=5).mean()  # 5ì¼ ì´ë™í‰ê· 
            ma_medium = prices.rolling(window=20).mean()  # 20ì¼ ì´ë™í‰ê· 
            ma_long = prices.rolling(window=50).mean()  # 50ì¼ ì´ë™í‰ê· 
            
            # ì „ì¼ ì´ë™í‰ê· 
            ma_short_prev = ma_short.shift(1)
            ma_medium_prev = ma_medium.shift(1)
            ma_long_prev = ma_long.shift(1)
            
            try:
                # 1. ê³¨ë“  í¬ë¡œìŠ¤ ê°ì§€
                golden_cross_5_20 = (ma_short > ma_medium) & (ma_short_prev <= ma_medium_prev)
                golden_cross_dates_5_20 = golden_cross_5_20[golden_cross_5_20].index
                
                for date in golden_cross_dates_5_20:
                    patterns.append({
                        'date': date,
                        'category': category,
                        'pattern': 'Golden Cross (5-20)',
                        'description': f'{category}: 5 MA crossed above 20 MA'
                    })
                
                golden_cross_20_50 = (ma_medium > ma_long) & (ma_medium_prev <= ma_long_prev)
                golden_cross_dates_20_50 = golden_cross_20_50[golden_cross_20_50].index
                
                for date in golden_cross_dates_20_50:
                    patterns.append({
                        'date': date,
                        'category': category,
                        'pattern': 'Golden Cross (20-50)',
                        'description': f'{category}: 20 MA crossed above 50 MA'
                    })
            except Exception as e:
                st.warning(f"{category} ê³¨ë“  í¬ë¡œìŠ¤ ê°ì§€ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            
            try:
                # 2. ë°ìŠ¤ í¬ë¡œìŠ¤ ê°ì§€
                death_cross_5_20 = (ma_short < ma_medium) & (ma_short_prev >= ma_medium_prev)
                death_cross_dates_5_20 = death_cross_5_20[death_cross_5_20].index
                
                for date in death_cross_dates_5_20:
                    patterns.append({
                        'date': date,
                        'category': category,
                        'pattern': 'Death Cross (5-20)',
                        'description': f'{category}: 5 MA crossed below 20 MA'
                    })
                
                death_cross_20_50 = (ma_medium < ma_long) & (ma_medium_prev >= ma_long_prev)
                death_cross_dates_20_50 = death_cross_20_50[death_cross_20_50].index
                
                for date in death_cross_dates_20_50:
                    patterns.append({
                        'date': date,
                        'category': category,
                        'pattern': 'Death Cross (20-50)',
                        'description': f'{category}: 20 MA crossed below 50 MA'
                    })
            except Exception as e:
                st.warning(f"{category} ë°ìŠ¤ í¬ë¡œìŠ¤ ê°ì§€ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            
            try:
                # 3. ì¶”ì„¸ ë°˜ì „ ê°ì§€
                uptrend_reversal = (prices < ma_medium) & (prices.shift(1) >= ma_medium_prev)
                uptrend_reversal_dates = uptrend_reversal[uptrend_reversal].index
                
                for date in uptrend_reversal_dates:
                    patterns.append({
                        'date': date,
                        'category': category,
                        'pattern': 'Uptrend Reversal',
                        'description': f'{category}:Price dropped below 20 MA'
                    })
                
                downtrend_reversal = (prices > ma_medium) & (prices.shift(1) <= ma_medium_prev)
                downtrend_reversal_dates = downtrend_reversal[downtrend_reversal].index
                
                for date in downtrend_reversal_dates:
                    patterns.append({
                        'date': date,
                        'category': category,
                        'pattern': 'Downtrend Reversal',
                        'description': f'{category}:Price rosw above 20 MA'
                    })
            except Exception as e:
                st.warning(f"{category} ì¶”ì„¸ ë°˜ì „ ê°ì§€ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            
            try:
                # 4. ì§€ì§€ì„ /ì €í•­ì„  í…ŒìŠ¤íŠ¸
                support_test = (
                    (prices <= ma_long * 1.005) &  
                    (prices > ma_long) &  
                    (prices.shift(1) > ma_long * 1.005)
                )
                support_test_dates = support_test[support_test].index
                
                for date in support_test_dates:
                    patterns.append({
                        'date': date,
                        'category': category,
                        'pattern': 'Support Test',
                        'description': f'{category}: Supported at 50 MA'
                    })
                
                resistance_test = (
                    (prices >= ma_long * 0.995) &  
                    (prices < ma_long) &  
                    (prices.shift(1) < ma_long * 0.995)
                )
                resistance_test_dates = resistance_test[resistance_test].index
                
                for date in resistance_test_dates:
                    patterns.append({
                        'date': date,
                        'category': category,
                        'pattern': 'Resistance Test',
                        'description': f'{category}: Resistance at 50 MA'
                    })
            except Exception as e:
                st.warning(f"{category} ì§€ì§€ì„ /ì €í•­ì„  í…ŒìŠ¤íŠ¸ ê°ì§€ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        except Exception as e:
            st.warning(f"{category} íŒ¨í„´ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            continue
    
    # ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜
    if patterns:
        try:
            patterns_df = pd.DataFrame(patterns)
            # ë‚ ì§œ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
            patterns_df = patterns_df.sort_values('date', ascending=False)
            return patterns_df
        except Exception as e:
            st.error(f"íŒ¨í„´ ë°ì´í„°í”„ë ˆì„ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return pd.DataFrame(columns=['date', 'category', 'pattern', 'description'])
    else:
        return pd.DataFrame(columns=['date', 'category', 'pattern', 'description'])

def detect_black_swan_events(df, daily_avg_prices, daily_indicators, std_threshold=3.0):
    """
    ë¸”ë™ ìŠ¤ì™„ ì´ë²¤íŠ¸ë¥¼ ê°ì§€í•©ë‹ˆë‹¤ (ë§¤ìš° í¬ê·€í•˜ê³  ì˜ˆì¸¡í•˜ê¸° ì–´ë ¤ìš´ ê·¹ë‹¨ì  ì´ë²¤íŠ¸).
    
    Parameters:
    -----------
    df : DataFrame
        ì›ë³¸ ë°ì´í„°í”„ë ˆì„
    daily_avg_prices : DataFrame
        ì¼ë³„ í‰ê·  ê°€ê²© ë°ì´í„°
    daily_indicators : DataFrame
        ì¼ë³„ ì§€í‘œ ë°ì´í„°
    std_threshold : float
        í‘œì¤€ í¸ì°¨ ê¸°ì¤€ ì„ê³„ê°’ (ê¸°ë³¸ê°’: 3.0)
    
    Returns:
    --------
    black_swan_df : DataFrame
        ê°ì§€ëœ ë¸”ë™ ìŠ¤ì™„ ì´ë²¤íŠ¸ë¥¼ í¬í•¨í•˜ëŠ” ë°ì´í„°í”„ë ˆì„
    """
    black_swan_events = []
    
    # ë°ì´í„° ìœ íš¨ì„± ì²´í¬ ë¨¼ì €
    if df is None or df.empty:
        print("ì›ë³¸ ë°ì´í„°í”„ë ˆì„ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
        return pd.DataFrame(columns=['date', 'category', 'event_type', 'z_score', 'return_pct', 'description'])
    
    if daily_avg_prices is None or daily_avg_prices.empty:
        print("ì¼ë³„ í‰ê·  ê°€ê²© ë°ì´í„°ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
        return pd.DataFrame(columns=['date', 'category', 'event_type', 'z_score', 'return_pct', 'description'])
    
    # âœ… ë‚ ì§œ í˜•ì‹ ê°•ì œ ë³€í™˜
    try:
        if 'date' in df.columns:
            df['date'] = pd.to_datetime(df['date'], errors='coerce')
        if daily_avg_prices is not None and not pd.api.types.is_datetime64_any_dtype(daily_avg_prices.index):
            daily_avg_prices.index = pd.to_datetime(daily_avg_prices.index, errors='coerce')
        if daily_indicators is not None and not pd.api.types.is_datetime64_any_dtype(daily_indicators.index):
            daily_indicators.index = pd.to_datetime(daily_indicators.index, errors='coerce')
    except Exception as e:
        print(f"ë‚ ì§œ ë³€í™˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return pd.DataFrame(columns=['date', 'category', 'event_type', 'z_score', 'return_pct', 'description'])
    
    # 1. ì¹´í…Œê³ ë¦¬ë³„ ìˆ˜ìµë¥  ë¸”ë™ ìŠ¤ì™„ ê°ì§€
    for category in daily_avg_prices.columns:
        try:
            print(f"{category} Category Analysing...")
            
            # ë¡œê·¸ ìˆ˜ìµë¥  ê³„ì‚° (ë¡œê·¸ ê³„ì‚°ì„ ìœ„í•´ 0ê³¼ ìŒìˆ˜ ì²˜ë¦¬)
            price_data = daily_avg_prices[category].replace(0, np.nan) #0ê°’ì²˜ë¦¬ 
            price_data = price_data.where(price_data > 0, np.nan) # ìŒìˆ˜ì²˜ë¦¬
            
            # ìœ íš¨í•œ ë°ì´í„° ì²´í¬
            if price_data.isnull().all():
                print(f"{category} ì¹´í…Œê³ ë¦¬ëŠ” ìœ íš¨í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                continue
                
            if len(price_data.dropna()) < 60:
                print(f"{category} ì¹´í…Œê³ ë¦¬ëŠ” ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤: {len(price_data.dropna())} < 60")
                continue
            
            # ë²¡í„°í™”ëœ ë°©ì‹ìœ¼ë¡œ ë¡œê·¸ ìˆ˜ìµë¥  ê³„ì‚°
            shifted_price = price_data.shift(1)
            valid_pairs = (price_data > 0) & (shifted_price > 0)
            
            # ë¡œê·¸ ìˆ˜ìµë¥  ê³„ì‚° - ë²¡í„°í™”ëœ ë°©ì‹
            log_returns = pd.Series(np.nan, index=price_data.index, dtype='float64')
            log_returns[valid_pairs] = np.log(price_data[valid_pairs] / shifted_price[valid_pairs])
            
            # ë¬´í•œê°’ ì œê±°
            log_returns = log_returns.replace([np.inf, -np.inf], np.nan)
            
            # ì¶©ë¶„í•œ ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸
            if len(log_returns.dropna()) < 60:
                print(f"{category} ì¹´í…Œê³ ë¦¬ëŠ” ìœ íš¨í•œ ë¡œê·¸ ìˆ˜ìµë¥  ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤: {len(log_returns.dropna())} < 60")
                continue
            
            #print(f"{category} ì¹´í…Œê³ ë¦¬ì˜ ìœ íš¨í•œ ë¡œê·¸ ìˆ˜ìµë¥  ë°ì´í„° ìˆ˜: {len(log_returns.dropna())}")
           
            # ë¡¤ë§ í‰ê·  ë° í‘œì¤€í¸ì°¨ ê³„ì‚° (60ì¼)
            rolling_mean = log_returns.rolling(window=60, min_periods=30).mean()
            rolling_std = log_returns.rolling(window=60, min_periods=30).std()
            
            # í‘œì¤€í¸ì°¨ê°€ 0ì¸ ê²½ìš° ê±´ë„ˆë›°ê¸°
            rolling_std = rolling_std.replace(0, np.nan)
            
            # Z-score ê³„ì‚° (60ì¼ ê¸°ì¤€)
            z_scores = pd.Series(np.nan, index=log_returns.index)
            valid_indices = ~rolling_mean.isna() & ~rolling_std.isna() & ~log_returns.isna() & (rolling_std > 0)
            z_scores[valid_indices] = (log_returns[valid_indices] - rolling_mean[valid_indices]) / rolling_std[valid_indices]
            
            # NaNì´ë‚˜ ë¬´í•œëŒ€ ê°’ ì²˜ë¦¬
            z_scores = z_scores.replace([np.inf, -np.inf], np.nan).dropna()
            
            #print(f"{category} ì¹´í…Œê³ ë¦¬ì˜ ìœ íš¨í•œ Z-score ë°ì´í„° ìˆ˜: {len(z_scores)}")

            # ê·¹ë‹¨ì  ì´ë²¤íŠ¸ ê°ì§€ (Z-scoreì˜ ì ˆëŒ€ê°’ì´ ì„ê³„ê°’ ì´ìƒ)
            extreme_events = z_scores[z_scores.abs() > std_threshold]
            #print(f"{category} ì¹´í…Œê³ ë¦¬ì—ì„œ ê°ì§€ëœ ê·¹ë‹¨ì  ì´ë²¤íŠ¸ ìˆ˜: {len(extreme_events)}")
            
            for date, z_score in extreme_events.items():
                try:
                    event_type = "Extreme Surge" if z_score > 0 else "Extreme Drop"
                    
                    # ì•ˆì „í•˜ê²Œ return_value ê°€ì ¸ì˜¤ê¸° - ì¸ë±ìŠ¤ ìœ íš¨ì„± ì²´í¬ ê°•í™”
                    if date in log_returns.index:
                        return_value = log_returns.loc[date]
                        # ìœ íš¨í•œ ê°’ì¸ì§€ í™•ì¸
                        if pd.notna(return_value) and np.isfinite(return_value):
                            return_pct = (np.exp(return_value) - 1) * 100  # ë°±ë¶„ìœ¨ë¡œ ë³€í™˜
                        else:
                            return_pct = np.nan
                    else:
                        return_value = np.nan
                        return_pct = np.nan
                        
                    # ì„¤ëª… ìƒì„± - NaN ê°’ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
                    if pd.notna(return_pct) and np.isfinite(return_pct):
                        description = f'{category} {event_type} (Daily changes: {return_pct:.2f}%, Z-score: {z_score:.2f})'
                    else:
                        description = f'{category} {event_type} (Z-score: {z_score:.2f})'

                    # ì´ë²¤íŠ¸ ì¶”ê°€ - ëª…ì‹œì ìœ¼ë¡œ float ë³€í™˜ ë° None ëŒ€ì‹  np.nan ì‚¬ìš©
                    event_dict = {
                        'date': date,
                        'category': category,
                        'event_type': event_type,
                        'z_score': float(z_score),
                        'return_pct': float(return_pct) if pd.notna(return_pct) and np.isfinite(return_pct) else np.nan,
                        'description': description
                    }
                    black_swan_events.append(event_dict)
                    print(f"Detected Black swan event: {description}")
                    
                except Exception as e:
                    print(f"{category} ì´ë²¤íŠ¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                    continue
                    
        except Exception as e:
            print(f"{category} ì¹´í…Œê³ ë¦¬ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            continue
    
    # 2. ê²½ì œ ì§€í‘œ ë¸”ë™ ìŠ¤ì™„ ì´ë²¤íŠ¸ ê°ì§€
    if daily_indicators is not None and not daily_indicators.empty:
        important_indicators = [col for col in daily_indicators.columns if any(
            ind in col for ind in [
                'GDP', 'CPI', 'Fed_Funds_Rate', 'Dollar_Index',
                'fear_greed_value', 'sentiment_score_mean'
            ]
        )]
        
        print(f"the number of important indicators: {len(important_indicators)}")
        
        for indicator in important_indicators:
            try:
                print(f"{indicator} analyse...")
                
                # ì§€í‘œì˜ ì¼ë³„ ë³€í™”ëŸ‰
                indicator_data = daily_indicators[indicator].dropna()
                
                # ë°ì´í„° ì¶©ë¶„ì„± í™•ì¸
                if len(indicator_data) < 60:
                    print(f"{indicator} ì§€í‘œëŠ” ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤: {len(indicator_data)} < 60")
                    continue
                
                # ì¼ë³„ ë³€í™”ëŸ‰ ê³„ì‚°
                indicator_change = indicator_data.diff()
                indicator_change = indicator_change.replace([np.inf, -np.inf], np.nan).dropna()
                
                if len(indicator_change) < 60:
                    print(f"{indicator} ì§€í‘œì˜ ë³€í™”ëŸ‰ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤: {len(indicator_change)} < 60")
                    continue
                
                #print(f"{indicator} ì§€í‘œì˜ ìœ íš¨í•œ ë³€í™”ëŸ‰ ë°ì´í„° ìˆ˜: {len(indicator_change)}")
                
                # ë¡¤ë§ í‰ê·  ë° í‘œì¤€í¸ì°¨ ê³„ì‚° (60ì¼)
                rolling_mean = indicator_change.rolling(window=60, min_periods=30).mean()
                rolling_std = indicator_change.rolling(window=60, min_periods=30).std()
                
                # í‘œì¤€í¸ì°¨ê°€ 0ì¸ ê²½ìš° ê±´ë„ˆë›°ê¸°
                rolling_std = rolling_std.replace(0, np.nan)
                
                # Z-score ê³„ì‚° (ì•ˆì „í•œ ë°©ì‹)
                z_scores = pd.Series(np.nan, index=indicator_change.index)
                valid_indices = ~rolling_mean.isna() & ~rolling_std.isna() & ~indicator_change.isna() & (rolling_std > 0)
                z_scores[valid_indices] = (indicator_change[valid_indices] - rolling_mean[valid_indices]) / rolling_std[valid_indices]
                
                # ë¬´í•œê°’ ë° NaN ì²˜ë¦¬
                z_scores = z_scores.replace([np.inf, -np.inf], np.nan).dropna()
                
                #print(f"{indicator} ì§€í‘œì˜ ìœ íš¨í•œ Z-score ë°ì´í„° ìˆ˜: {len(z_scores)}")
                
                # ê·¹ë‹¨ì  ì´ë²¤íŠ¸ ê°ì§€
                extreme_events = z_scores[z_scores.abs() > std_threshold]
                #print(f"{indicator} ì§€í‘œì—ì„œ ê°ì§€ëœ ê·¹ë‹¨ì  ì´ë²¤íŠ¸ ìˆ˜: {len(extreme_events)}")
                
                for date, z_score in extreme_events.items():
                    try:
                        event_type = "Extreme Surge" if z_score > 0 else "Extreme Drop"
                        
                        # ì§€í‘œëª… ì¶”ì¶œ
                        if '_norm' in indicator:
                            indicator_name = indicator.replace('_norm', '')
                        else:
                            indicator_name = indicator
                        
                        # ì´ë²¤íŠ¸ ì¶”ê°€ - ëª…ì‹œì ìœ¼ë¡œ float ë³€í™˜
                        event_dict = {
                            'date': date,
                            'category': 'Economic Indicator',
                            'event_type': f'{indicator_name} {event_type}',
                            'z_score': float(return_pct) if pd.notna(return_pct) and np.isfinite(return_pct) else np.nan,
                            'return_pct': float(return_pct) if pd.notna(return_pct) and np.isfinite(return_pct) else np.nan,  # ì§€í‘œëŠ” ìˆ˜ìµë¥ ì´ ì—†ìœ¼ë¯€ë¡œ np.nan ì‚¬ìš©
                            'description': f'{indicator_name}: {event_type} (Z-score: {z_score:.2f})'
                        }
                        black_swan_events.append(event_dict)
                        #print(f"ë¸”ë™ ìŠ¤ì™„ ì´ë²¤íŠ¸ ê°ì§€ (ì§€í‘œ): {event_dict['description']}")
                        
                    except Exception as e:
                        print(f"{indicator} ì´ë²¤íŠ¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                        continue
                        
            except Exception as e:
                print(f"{indicator} ì§€í‘œ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                continue
    
    # ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜
    if black_swan_events:
        try:
            black_swan_df = pd.DataFrame(black_swan_events)
            print(f"ì´ {len(black_swan_df)}ê°œì˜ ë¸”ë™ ìŠ¤ì™„ ì´ë²¤íŠ¸ê°€ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
            # ê²°ì¸¡ì¹˜ ì§ì ‘ í™•ì¸
            for col in black_swan_df.columns:
                print(f"{col} ì»¬ëŸ¼ì˜ ê²°ì¸¡ì¹˜ ìˆ˜: {black_swan_df[col].isna().sum()}")
            
            # NaN ê°’ ì²˜ë¦¬ - íŠ¹íˆ numeric ì»¬ëŸ¼ì— ëŒ€í•´
            black_swan_df['z_score'] = black_swan_df['z_score'].apply(lambda x: float(x) if pd.notna(x) and np.isfinite(x) else np.nan)
            black_swan_df['return_pct'] = black_swan_df['return_pct'].apply(lambda x: float(x) if pd.notna(x) and np.isfinite(x) else np.nan)
            
            # ë‚ ì§œ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
            black_swan_df = black_swan_df.sort_values('date', ascending=False)
            return black_swan_df
        except Exception as e:
            print(f"ë¸”ë™ ìŠ¤ì™„ ì´ë²¤íŠ¸ ë°ì´í„°í”„ë ˆì„ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return pd.DataFrame(columns=['date', 'category', 'event_type', 'z_score', 'return_pct', 'description'])
    else:
        print("ê°ì§€ëœ ë¸”ë™ ìŠ¤ì™„ ì´ë²¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return pd.DataFrame(columns=['date', 'category', 'event_type', 'z_score', 'return_pct', 'description'])

def detect_correlations_breakdown(daily_avg_prices, daily_indicators, window=30, threshold=0.25):
    """
    ìƒê´€ê´€ê³„ ë¶•ê´´ ì´ë²¤íŠ¸ë¥¼ ê°ì§€í•©ë‹ˆë‹¤ (ì¼ë°˜ì ìœ¼ë¡œ ê°•í•œ ìƒê´€ê´€ê³„ê°€ ê°‘ìê¸° ì•½í•´ì§€ê±°ë‚˜ ë°˜ì „ë˜ëŠ” ê²½ìš°).
    
    Parameters:
    -----------
    daily_avg_prices : DataFrame
        ì¼ë³„ í‰ê·  ê°€ê²© ë°ì´í„°
    daily_indicators : DataFrame
        ì¼ë³„ ì§€í‘œ ë°ì´í„°
    window : int
        ìƒê´€ê´€ê³„ ê³„ì‚°ì„ ìœ„í•œ ë¡¤ë§ ìœˆë„ìš° í¬ê¸° (ê¸°ë³¸ê°’: 30 - ì´ì „ 60ì—ì„œ ì¶•ì†Œ)
    threshold : float
        ìƒê´€ê´€ê³„ ë³€í™” ì„ê³„ê°’ (ê¸°ë³¸ê°’: 0.25 - ì´ì „ 0.4ì—ì„œ ë‚®ì¶¤)
    
    Returns:
    --------
    correlation_events_df : DataFrame
        ê°ì§€ëœ ìƒê´€ê´€ê³„ ë¶•ê´´ ì´ë²¤íŠ¸ë¥¼ í¬í•¨í•˜ëŠ” ë°ì´í„°í”„ë ˆì„
    """
    correlation_events = []
    
    # ë°ì´í„° ìœ íš¨ì„± ë¨¼ì € ê²€ì‚¬ 
    if daily_avg_prices is None or daily_avg_prices.empty:
        print("ì¼ë³„ í‰ê·  ê°€ê²© ë°ì´í„°ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
        return pd.DataFrame(columns=['date', 'pair', 'event_type', 'old_correlation', 
                                    'new_correlation', 'change', 'description'])
    
    # ì§€í‘œ ë°ì´í„°ê°€ ì—†ì–´ë„ ì¹´í…Œê³ ë¦¬ ê°„ ìƒê´€ê´€ê³„ëŠ” ê³„ì‚° ê°€ëŠ¥
    indicators_available = False
    if daily_indicators is not None and not daily_indicators.empty:
        indicators_available = True
    else:
        print("ì¼ë³„ ì§€í‘œ ë°ì´í„°ê°€ ì—†ê±°ë‚˜ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. ì¹´í…Œê³ ë¦¬ ê°„ ìƒê´€ê´€ê³„ë§Œ ë¶„ì„í•©ë‹ˆë‹¤.")
    
    # âœ… datetime í˜•ì‹ìœ¼ë¡œ ê°•ì œ ë³€í™˜
    try:
        if not pd.api.types.is_datetime64_any_dtype(daily_avg_prices.index):
            daily_avg_prices.index = pd.to_datetime(daily_avg_prices.index, errors='coerce')
        if indicators_available and not pd.api.types.is_datetime64_any_dtype(daily_indicators.index):
            daily_indicators.index = pd.to_datetime(daily_indicators.index, errors='coerce')
    except Exception as e:
        print(f"ë‚ ì§œ ë³€í™˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return pd.DataFrame(columns=['date', 'pair', 'event_type', 'old_correlation', 
                                   'new_correlation', 'change', 'description'])
    
    # ë°ì´í„° ìœ íš¨ ë²”ìœ„ í™•ì¸
    if len(daily_avg_prices) < window:
        print(f"ë°ì´í„°ê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {len(daily_avg_prices)} < {window}")
        return pd.DataFrame(columns=['date', 'pair', 'event_type', 'old_correlation', 
                                   'new_correlation', 'change', 'description'])
    
    print(f"ìƒê´€ê´€ê³„ ë¶•ê´´ ë¶„ì„ ì‹œì‘: ìœˆë„ìš°={window}, ì„ê³„ê°’={threshold}")
    print(f"ì¼ë³„ í‰ê·  ê°€ê²© ë°ì´í„° í˜•íƒœ: {daily_avg_prices.shape}, ê¸°ê°„: {daily_avg_prices.index.min()} ~ {daily_avg_prices.index.max()}")
    
    # ë°ì´í„° ì „ì²˜ë¦¬: ëª¨ë“  ì¹´í…Œê³ ë¦¬ì— ëŒ€í•œ ê¸°ë³¸ ì •ë³´ ì¶œë ¥
    for col in daily_avg_prices.columns:
        nan_count = daily_avg_prices[col].isna().sum()
        zero_count = (daily_avg_prices[col] == 0).sum()
        #print(f"ì¹´í…Œê³ ë¦¬ '{col}': ì´ {len(daily_avg_prices[col])}ê°œ ë°ì´í„°, NaN {nan_count}ê°œ, 0ê°’ {zero_count}ê°œ")
    
    # 1. ì¹´í…Œê³ ë¦¬-ì¹´í…Œê³ ë¦¬ ìƒê´€ê´€ê³„ ë¶•ê´´ ê°ì§€
    categories = daily_avg_prices.columns.tolist()
    if len(categories) < 2:
        print("ì¹´í…Œê³ ë¦¬ ìˆ˜ê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ìµœì†Œ 2ê°œ ì´ìƒ í•„ìš”í•©ë‹ˆë‹¤.")
        # ë¹ˆ ë°ì´í„°í”„ë ˆì„ ë°˜í™˜
        return pd.DataFrame(columns=['date', 'pair', 'event_type', 'old_correlation', 
                                   'new_correlation', 'change', 'description'])
    
    #print(f"ë¶„ì„í•  ì¹´í…Œê³ ë¦¬ ìˆ˜: {len(categories)}")
    
    for i in range(len(categories)):
        for j in range(i+1, len(categories)):
            cat1 = categories[i]
            cat2 = categories[j]
            
            try:
                #print(f"\n{cat1}-{cat2} ì¹´í…Œê³ ë¦¬ ìŒ ë¶„ì„ ì¤‘...")
                
                # ì¹´í…Œê³ ë¦¬ì˜ ë°ì´í„° ì¶”ì¶œ (NaNë§Œ ì œê±°, 0ì€ ìœ ì§€)
                prices1 = daily_avg_prices[cat1].dropna()
                prices2 = daily_avg_prices[cat2].dropna()
                
                # ë°ì´í„°ê°€ ì¶©ë¶„í•œì§€ í™•ì¸
                if len(prices1) < window or len(prices2) < window:
                    print(f"{cat1}-{cat2} ì¹´í…Œê³ ë¦¬ ìŒì€ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤: {cat1}={len(prices1)}, {cat2}={len(prices2)}, í•„ìš”={window}")
                    continue
                
                # ê³µí†µ ì¸ë±ìŠ¤ ì°¾ê¸°
                common_dates = prices1.index.intersection(prices2.index)
                
                # ê³µí†µ ì¸ë±ìŠ¤ê°€ ì¶©ë¶„í•œì§€ í™•ì¸
                if len(common_dates) < window:
                    print(f"{cat1}-{cat2} ì¹´í…Œê³ ë¦¬ ìŒì€ ê³µí†µ ì¸ë±ìŠ¤ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤: {len(common_dates)} < {window}")
                    continue
                
                #print(f"{cat1}-{cat2} ì¹´í…Œê³ ë¦¬ ìŒì˜ ê³µí†µ ì¸ë±ìŠ¤ ìˆ˜: {len(common_dates)}")
                
                # ê³µí†µ ë‚ ì§œì— ëŒ€í•œ ë°ì´í„° ì¤€ë¹„
                prices1_common = prices1[common_dates]
                prices2_common = prices2[common_dates]
                
                # ì •ê·œí™”ëœ ê°€ê²©ì„ ì‚¬ìš©í•˜ì—¬ ìƒê´€ê´€ê³„ ê³„ì‚° (ë‹¨ìˆœ ë³€í™”ìœ¨ ëŒ€ì‹ )
                # ì´ë ‡ê²Œ í•˜ë©´ ìŠ¤ì¼€ì¼ ì°¨ì´ë¡œ ì¸í•œ ìƒê´€ê´€ê³„ ì™œê³¡ì„ ì¤„ì¼ ìˆ˜ ìˆìŒ
                prices1_norm = (prices1_common - prices1_common.mean()) / prices1_common.std()
                prices2_norm = (prices2_common - prices2_common.mean()) / prices2_common.std()
                
                # ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ê²°í•©
                price_data = pd.DataFrame({
                    cat1: prices1_norm,
                    cat2: prices2_norm
                })
                
                # NaN í–‰ ì œê±°
                price_data = price_data.dropna()
                
                # ë°ì´í„°ê°€ ì¶©ë¶„í•œì§€ ìµœì¢… í™•ì¸
                if len(price_data) < window:
                    print(f"{cat1}-{cat2} ì¹´í…Œê³ ë¦¬ ìŒì€ ìµœì¢… ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤: {len(price_data)} < {window}")
                    continue
                
                # ê° ì‹œë¦¬ì¦ˆì˜ í‘œì¤€í¸ì°¨ê°€ 0ì¸ì§€ í™•ì¸ - ì •ê·œí™”í–ˆìœ¼ë¯€ë¡œ ì´ ë¶€ë¶„ì€ ìƒëµ ê°€ëŠ¥í•˜ì§€ë§Œ ì•ˆì „ì„ ìœ„í•´ ìœ ì§€
                if price_data[cat1].std() < 1e-8 or price_data[cat2].std() < 1e-8:
                    print(f"{cat1}-{cat2} ì¹´í…Œê³ ë¦¬ ìŒ ì¤‘ í•˜ë‚˜ê°€ ìƒìˆ˜ê°’ì…ë‹ˆë‹¤.")
                    continue
                
                #print(f"{cat1}-{cat2} ì¹´í…Œê³ ë¦¬ ìŒì˜ ìœ íš¨í•œ ë°ì´í„° ìˆ˜: {len(price_data)}")
                
                # ë¡¤ë§ ìƒê´€ê³„ìˆ˜ ê³„ì‚°
                # min_periodsë¥¼ ë” ì‘ê²Œ ì„¤ì •í•˜ì—¬ ì´ˆê¸° ë°ì´í„°ë„ í™œìš©
                min_periods = max(5, window // 3)  # ìµœì†Œ 5ê°œ ë˜ëŠ” ìœˆë„ìš°ì˜ 1/3
                rolling_corr = price_data[cat1].rolling(window=window, min_periods=min_periods).corr(price_data[cat2])
                
                # NaN ë° ë¬´í•œê°’ ì œê±°
                rolling_corr = rolling_corr.replace([np.inf, -np.inf], np.nan)
                
                # í•µì‹¬ ë””ë²„ê¹… ì •ë³´: ë¡¤ë§ ìƒê´€ê³„ìˆ˜ í†µê³„
                valid_corr = rolling_corr.dropna()
                if len(valid_corr) > 0:
                    print(f"ìœ íš¨í•œ ë¡¤ë§ ìƒê´€ê³„ìˆ˜: {len(valid_corr)}ê°œ, í‰ê· : {valid_corr.mean():.4f}, ìµœì†Œ: {valid_corr.min():.4f}, ìµœëŒ€: {valid_corr.max():.4f}")
                else:
                    print("ìœ íš¨í•œ ë¡¤ë§ ìƒê´€ê³„ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    continue
                
                # ë¡¤ë§ ìƒê´€ê³„ìˆ˜ì˜ ë³€í™”ëŸ‰ ê³„ì‚°
                corr_change = rolling_corr.diff().abs()  # ì ˆëŒ€ê°’ìœ¼ë¡œ ë³€í™”ëŸ‰ ê³„ì‚°
                
                # NaN ë° ë¬´í•œê°’ ì œê±°
                corr_change = corr_change.replace([np.inf, -np.inf], np.nan).dropna()
                
                # ìƒê´€ê´€ê³„ ë³€í™”ëŸ‰ì´ ì„ê³„ê°’ì„ ì´ˆê³¼í•˜ëŠ” ë‚ ì§œ ì°¾ê¸°
                breakdown_dates = corr_change[corr_change > threshold].index
                
                #print(f"{cat1}-{cat2} ì¹´í…Œê³ ë¦¬ ìŒì—ì„œ ê°ì§€ëœ ìƒê´€ê´€ê³„ ë³€í™” ì´ë²¤íŠ¸ ìˆ˜: {len(breakdown_dates)}")
                
                # ë³€í™”ëŸ‰ í†µê³„: ì„ê³„ê°’ ì¡°ì •ì— ë„ì›€
                if len(corr_change) > 0:
                    print(f"ìƒê´€ê´€ê³„ ë³€í™”ëŸ‰ í†µê³„: í‰ê· : {corr_change.mean():.4f}, ìµœëŒ€: {corr_change.max():.4f}, 95% ë°±ë¶„ìœ„: {corr_change.quantile(0.95):.4f}")
                
                # ì´ë²¤íŠ¸ ìƒì„±
                for date in breakdown_dates:
                    try:
                        # í˜„ì¬ ë‚ ì§œì™€ ì´ì „ ë‚ ì§œê°€ rolling_corrì— ìˆëŠ”ì§€ í™•ì¸
                        if date not in rolling_corr.index:
                            continue
                            
                        # ì´ì „ ë‚ ì§œ ê°€ì ¸ì˜¤ê¸°
                        date_idx = rolling_corr.index.get_loc(date)
                        if date_idx == 0:  # ì²« ë²ˆì§¸ í–‰ì´ë©´ ì´ì „ ë‚ ì§œê°€ ì—†ìŒ
                            continue
                            
                        prev_date = rolling_corr.index[date_idx - 1]
                        
                        # ì´ì „ ë° í˜„ì¬ ìƒê´€ê³„ìˆ˜ ê°’ ê°€ì ¸ì˜¤ê¸°
                        if prev_date not in rolling_corr.index or pd.isna(rolling_corr.loc[prev_date]):
                            continue
                            
                        old_corr = rolling_corr.loc[prev_date]
                        new_corr = rolling_corr.loc[date]
                        change = abs(new_corr - old_corr)  # ì ˆëŒ€ê°’ ë³€í™”ëŸ‰
                        
                        # ê°’ì´ ìœ íš¨í•œì§€ í™•ì¸
                        if not np.isfinite(old_corr) or not np.isfinite(new_corr) or not np.isfinite(change):
                            continue
                        
                        # ë³€í™” ë°©í–¥ ê²°ì •
                        if new_corr < old_corr:
                            direction = "ê°ì†Œ"
                            event_type = "Correlation Breakdown"
                        else:
                            direction = "ì¦ê°€"
                            event_type = "Correlation Spike"
                        
                        # ì´ë²¤íŠ¸ ì •ë³´ ì €ì¥
                        correlation_events.append({
                            'date': date,
                            'pair': f'{cat1}-{cat2}',
                            'event_type': event_type,
                            'old_correlation': float(old_corr),
                            'new_correlation': float(new_corr),
                            'change': float(change),
                            'description': f'{cat1}ì™€ {cat2} ê°„ì˜ ìƒê´€ê´€ê³„ {direction} ({old_corr:.2f} â†’ {new_corr:.2f}, ë³€í™”ëŸ‰: {change:.2f})'
                        })
                        
                        print(f"ìƒê´€ê´€ê³„ ë³€í™” ì´ë²¤íŠ¸ ê°ì§€: {cat1}ì™€ {cat2} ê°„ì˜ ìƒê´€ê´€ê³„ {direction} ({old_corr:.2f} â†’ {new_corr:.2f}, ë³€í™”ëŸ‰: {change:.2f})")
                    except Exception as e:
                        print(f"{cat1}-{cat2} ìƒê´€ê´€ê³„ ì´ë²¤íŠ¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                        continue
            except Exception as e:
                print(f"{cat1}-{cat2} ìƒê´€ê´€ê³„ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                continue
    
    # 2. ì¹´í…Œê³ ë¦¬-ì§€í‘œ ìƒê´€ê´€ê³„ ë¶•ê´´ ê°ì§€ (ì§€í‘œ ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ)
    if indicators_available:
        # ì¤‘ìš” ì§€í‘œ í•„í„°ë§
        important_indicators = [col for col in daily_indicators.columns if any(
            ind in col for ind in [
                'GDP', 'CPI', 'Fed_Funds_Rate', 'Dollar_Index',
                'fear_greed_value', 'sentiment_score_mean'
            ]
        )]
        
        #print(f"\në¶„ì„í•  ì¤‘ìš” ì§€í‘œ ìˆ˜: {len(important_indicators)}")
        
        for category in categories:
            for indicator in important_indicators:
                try:
                    print(f"\n{category}-{indicator} ì¹´í…Œê³ ë¦¬-ì§€í‘œ ìŒ ë¶„ì„ ì¤‘...")
                    
                    # ì¹´í…Œê³ ë¦¬ì™€ ì§€í‘œ ë°ì´í„° ì¶”ì¶œ (NaNë§Œ ì œê±°)
                    prices = daily_avg_prices[category].dropna()
                    indicator_values = daily_indicators[indicator].dropna()
                    
                    # ë°ì´í„°ê°€ ì¶©ë¶„í•œì§€ í™•ì¸
                    if len(prices) < window or len(indicator_values) < window:
                        print(f"{category}-{indicator} ì¹´í…Œê³ ë¦¬-ì§€í‘œ ìŒì€ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")
                        continue
                    
                    # ê³µí†µ ì¸ë±ìŠ¤ ì°¾ê¸°
                    common_dates = prices.index.intersection(indicator_values.index)
                    
                    # ê³µí†µ ì¸ë±ìŠ¤ê°€ ì¶©ë¶„í•œì§€ í™•ì¸
                    if len(common_dates) < window:
                        print(f"{category}-{indicator} ì¹´í…Œê³ ë¦¬-ì§€í‘œ ìŒì€ ê³µí†µ ì¸ë±ìŠ¤ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤: {len(common_dates)} < {window}")
                        continue
                    
                    #print(f"{category}-{indicator} ì¹´í…Œê³ ë¦¬-ì§€í‘œ ìŒì˜ ê³µí†µ ì¸ë±ìŠ¤ ìˆ˜: {len(common_dates)}")
                    
                    # ê³µí†µ ë‚ ì§œì— ëŒ€í•œ ë°ì´í„° ì¤€ë¹„
                    prices_common = prices[common_dates]
                    indicator_common = indicator_values[common_dates]
                    
                    # ì •ê·œí™”ëœ ë°ì´í„° ì‚¬ìš©
                    prices_norm = (prices_common - prices_common.mean()) / prices_common.std()
                    indicator_norm = (indicator_common - indicator_common.mean()) / indicator_common.std()
                    
                    # ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ê²°í•©
                    combined_data = pd.DataFrame({
                        'prices': prices_norm,
                        'indicator': indicator_norm
                    })
                    
                    # NaN í–‰ ì œê±°
                    combined_data = combined_data.dropna()
                    
                    # ë°ì´í„°ê°€ ì¶©ë¶„í•œì§€ ìµœì¢… í™•ì¸
                    if len(combined_data) < window:
                        print(f"{category}-{indicator} ì¹´í…Œê³ ë¦¬-ì§€í‘œ ìŒì€ ìµœì¢… ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤: {len(combined_data)} < {window}")
                        continue
                    
                    # ê° ì‹œë¦¬ì¦ˆì˜ í‘œì¤€í¸ì°¨ê°€ 0ì¸ì§€ í™•ì¸
                    if combined_data['prices'].std() < 1e-8 or combined_data['indicator'].std() < 1e-8:
                        print(f"{category}-{indicator} ì¹´í…Œê³ ë¦¬-ì§€í‘œ ìŒ ì¤‘ í•˜ë‚˜ê°€ ìƒìˆ˜ê°’ì…ë‹ˆë‹¤.")
                        continue
                    
                    #print(f"Between {category}-{indicator} combined & available data : {len(combined_data)}")
                    
                    # ë¡¤ë§ ìƒê´€ê³„ìˆ˜ ê³„ì‚°
                    min_periods = max(5, window // 3)
                    rolling_corr = combined_data['prices'].rolling(window=window, min_periods=min_periods).corr(combined_data['indicator'])
                    
                    # NaN ë° ë¬´í•œê°’ ì œê±°
                    rolling_corr = rolling_corr.replace([np.inf, -np.inf], np.nan)
                    
                    # # í•µì‹¬ ë””ë²„ê¹… ì •ë³´: ë¡¤ë§ ìƒê´€ê³„ìˆ˜ í†µê³„
                    # valid_corr = rolling_corr.dropna()
                    # if len(valid_corr) > 0:
                    #     print(f"ìœ íš¨í•œ ë¡¤ë§ ìƒê´€ê³„ìˆ˜: {len(valid_corr)}ê°œ, í‰ê· : {valid_corr.mean():.4f}, ìµœì†Œ: {valid_corr.min():.4f}, ìµœëŒ€: {valid_corr.max():.4f}")
                    # else:
                    #     print("ìœ íš¨í•œ ë¡¤ë§ ìƒê´€ê³„ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    #     continue
                    
                    # ë¡¤ë§ ìƒê´€ê³„ìˆ˜ì˜ ë³€í™”ëŸ‰ ê³„ì‚°
                    corr_change = rolling_corr.diff().abs()  # ì ˆëŒ€ê°’ìœ¼ë¡œ ë³€í™”ëŸ‰ ê³„ì‚°
                    
                    # NaN ë° ë¬´í•œê°’ ì œê±°
                    corr_change = corr_change.replace([np.inf, -np.inf], np.nan).dropna()
                    
                    # ìƒê´€ê´€ê³„ ë³€í™”ëŸ‰ì´ ì„ê³„ê°’ì„ ì´ˆê³¼í•˜ëŠ” ë‚ ì§œ ì°¾ê¸°
                    breakdown_dates = corr_change[corr_change > threshold].index
                    
                    #print(f"{category}-{indicator} ì¹´í…Œê³ ë¦¬-ì§€í‘œ ìŒì—ì„œ ê°ì§€ëœ ìƒê´€ê´€ê³„ ë³€í™” ì´ë²¤íŠ¸ ìˆ˜: {len(breakdown_dates)}")
                    
                    # ë³€í™”ëŸ‰ í†µê³„
                    if len(corr_change) > 0:
                        print(f"ìƒê´€ê´€ê³„ ë³€í™”ëŸ‰ í†µê³„: í‰ê· : {corr_change.mean():.4f}, ìµœëŒ€: {corr_change.max():.4f}, 95% ë°±ë¶„ìœ„: {corr_change.quantile(0.95):.4f}")
                    
                    # ì´ë²¤íŠ¸ ìƒì„±
                    for date in breakdown_dates:
                        try:
                            # í˜„ì¬ ë‚ ì§œì™€ ì´ì „ ë‚ ì§œê°€ rolling_corrì— ìˆëŠ”ì§€ í™•ì¸
                            if date not in rolling_corr.index:
                                continue
                                
                            # ì´ì „ ë‚ ì§œ ê°€ì ¸ì˜¤ê¸°
                            date_idx = rolling_corr.index.get_loc(date)
                            if date_idx == 0:  # ì²« ë²ˆì§¸ í–‰ì´ë©´ ì´ì „ ë‚ ì§œê°€ ì—†ìŒ
                                continue
                                
                            prev_date = rolling_corr.index[date_idx - 1]
                            
                            # ì´ì „ ë° í˜„ì¬ ìƒê´€ê³„ìˆ˜ ê°’ ê°€ì ¸ì˜¤ê¸°
                            if prev_date not in rolling_corr.index or pd.isna(rolling_corr.loc[prev_date]):
                                continue
                                
                            old_corr = rolling_corr.loc[prev_date]
                            new_corr = rolling_corr.loc[date]
                            change = abs(new_corr - old_corr)  # ì ˆëŒ€ê°’ ë³€í™”ëŸ‰
                            
                            # ê°’ì´ ìœ íš¨í•œì§€ í™•ì¸
                            if not np.isfinite(old_corr) or not np.isfinite(new_corr) or not np.isfinite(change):
                                continue
                            
                            # ë³€í™” ë°©í–¥ ê²°ì •
                            if new_corr < old_corr:
                                direction = "ê°ì†Œ"
                                event_type = "Correlation Breakdown"
                            else:
                                direction = "ì¦ê°€"
                                event_type = "Correlation Spike"
                            
                            # ì´ë²¤íŠ¸ ì •ë³´ ì €ì¥
                            correlation_events.append({
                                'date': date,
                                'pair': f'{category}-{indicator}',
                                'event_type': event_type,
                                'old_correlation': float(old_corr),
                                'new_correlation': float(new_corr),
                                'change': float(change),
                                'description': f'Correlation between {category} and {indicator} : {direction} ({old_corr:.2f} â†’ {new_corr:.2f}, Change: {change:.2f})'
                            })
                            
                            #print(f"ìƒê´€ê´€ê³„ ë³€í™” ì´ë²¤íŠ¸ ê°ì§€: {category}ì™€ {indicator} ê°„ì˜ ìƒê´€ê´€ê³„ {direction} ({old_corr:.2f} â†’ {new_corr:.2f}, ë³€í™”ëŸ‰: {change:.2f})")
                        except Exception as e:
                            #print(f"{category}-{indicator} ìƒê´€ê´€ê³„ ì´ë²¤íŠ¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                            continue
                except Exception as e:
                    print(f"{category}-{indicator} ìƒê´€ê´€ê³„ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                    continue
   
        # detect_correlations_breakdown í•¨ìˆ˜ ë§ˆì§€ë§‰ ë¶€ë¶„ ìˆ˜ì •
    if correlation_events:
        try:
            correlation_breakdown_df = pd.DataFrame(correlation_events)  # ë³€ìˆ˜ëª… ë³€ê²½
            #print(f"\nì´ {len(correlation_breakdown_df)}ê°œì˜ ìƒê´€ê´€ê³„ ë³€í™” ì´ë²¤íŠ¸ê°€ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
            # ê²°ì¸¡ì¹˜ ì§ì ‘ í™•ì¸
            for col in correlation_breakdown_df.columns:
                print(f"{col} ì»¬ëŸ¼ì˜ ê²°ì¸¡ì¹˜ ìˆ˜: {correlation_breakdown_df[col].isna().sum()}")
            
            # NaN ê°’ ì²˜ë¦¬ - íŠ¹íˆ numeric ì»¬ëŸ¼ì— ëŒ€í•´
            numeric_cols = ['old_correlation', 'new_correlation', 'change']
            for col in numeric_cols:
                correlation_breakdown_df[col] = correlation_breakdown_df[col].apply(
                    lambda x: float(x) if pd.notna(x) and np.isfinite(x) else np.nan
                )
            
            # ë‚ ì§œ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
            correlation_breakdown_df = correlation_breakdown_df.sort_values('date', ascending=False)
            return correlation_breakdown_df    
        except Exception as e:
            print(f"ê²°ê³¼ ë°ì´í„°í”„ë ˆì„ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return pd.DataFrame(columns=['date', 'pair', 'event_type', 'old_correlation', 
                                    'new_correlation', 'change', 'description'])
    else:
        print("\nê°ì§€ëœ ìƒê´€ê´€ê³„ ë³€í™” ì´ë²¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return pd.DataFrame(columns=['date', 'pair', 'event_type', 'old_correlation', 
                                    'new_correlation', 'change', 'description'])

def generate_extreme_events_features(df, extreme_events_df, black_swan_df, correlation_breakdown_df=None):
    """
    ê·¹ë‹¨ì  ì´ë²¤íŠ¸ íŠ¹ì„±ì„ ìƒì„±í•©ë‹ˆë‹¤ (ëª¨ë¸ í›ˆë ¨ì— ì‚¬ìš©).
    
    Parameters:
    -----------
    df : DataFrame
        ì›ë³¸ ë°ì´í„°í”„ë ˆì„
    extreme_events_df : DataFrame
        ê°ì§€ëœ ê·¹ë‹¨ì  ì´ë²¤íŠ¸ ë°ì´í„°í”„ë ˆì„
    black_swan_df : DataFrame
        ê°ì§€ëœ ë¸”ë™ ìŠ¤ì™„ ì´ë²¤íŠ¸ ë°ì´í„°í”„ë ˆì„
    correlation_breakdown_df : DataFrame, optional
        ê°ì§€ëœ ìƒê´€ê´€ê³„ ë¶•ê´´ ì´ë²¤íŠ¸ ë°ì´í„°í”„ë ˆì„
    
    Returns:
    --------
    df : DataFrame
        ê·¹ë‹¨ì  ì´ë²¤íŠ¸ íŠ¹ì„±ì´ ì¶”ê°€ëœ ë°ì´í„°í”„ë ˆì„
    """
    if df is None or df.empty:
        return df
    
    # ë‚ ì§œ ì»¬ëŸ¼ datetimeìœ¼ë¡œ ê°•ì œ ë³€í™˜
    try:
        if 'date' in df.columns and not pd.api.types.is_datetime64_any_dtype(df['date']):
            df['date'] = pd.to_datetime(df['date'], errors='coerce')
        if black_swan_df is not None and 'date' in black_swan_df.columns and not pd.api.types.is_datetime64_any_dtype(black_swan_df['date']):
            black_swan_df['date'] = pd.to_datetime(black_swan_df['date'], errors='coerce')
        if extreme_events_df is not None and 'date' in extreme_events_df.columns and not pd.api.types.is_datetime64_any_dtype(extreme_events_df['date']):
            extreme_events_df['date'] = pd.to_datetime(extreme_events_df['date'], errors='coerce')
        if correlation_breakdown_df is not None and 'date' in correlation_breakdown_df.columns and not pd.api.types.is_datetime64_any_dtype(correlation_breakdown_df['date']):
            correlation_breakdown_df['date'] = pd.to_datetime(correlation_breakdown_df['date'], errors='coerce')
    except Exception as e:
        st.error(f"An error occurred during date conversion: {str(e)}")
        return df

    # 1. ê·¹ë‹¨ì  ì´ë²¤íŠ¸ íŠ¹ì„±
    try:
        if extreme_events_df is not None and not extreme_events_df.empty:
            # ê²½ì œ ì§€í‘œ ê·¹ë‹¨ê°’ í‘œì‹œ
            economic_extreme_events = extreme_events_df[
                (extreme_events_df['event_type'] == 'Extreme High') | 
                (extreme_events_df['event_type'] == 'Extreme Low')
            ]
            
            if not economic_extreme_events.empty:
                # ê·¹ë‹¨ ì´ë²¤íŠ¸ ë°œìƒ ì—¬ë¶€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” íŠ¹ì„± ìƒì„±
                extreme_dates = economic_extreme_events['date'].unique()
                
                # ë°ì´í„°í”„ë ˆì„ì— ê·¹ë‹¨ê°’ í‘œì‹œì ì¶”ê°€
                df['extreme_economic_event'] = df['date'].isin(extreme_dates).astype(int)
                
                # ì—°ì†ëœ ê·¹ë‹¨ ì´ë²¤íŠ¸ ì¼ìˆ˜ ê³„ì‚°
                df['extreme_event_days'] = 0
                current_count = 0
                
                for i, row in df.sort_values('date').iterrows():
                    if row['extreme_economic_event'] == 1:
                        current_count += 1
                    else:
                        current_count = 0
                    df.at[i, 'extreme_event_days'] = current_count
    except Exception as e:
        st.warning(f"Error processing extreme events: {str(e)}")

    # 2. ë¸”ë™ ìŠ¤ì™„ ì´ë²¤íŠ¸ íŠ¹ì„±
    try:
        if black_swan_df is not None and not black_swan_df.empty:
            # ë¸”ë™ ìŠ¤ì™„ ë°œìƒ ì—¬ë¶€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” íŠ¹ì„± ìƒì„±
            black_swan_dates = black_swan_df['date'].unique()
            
            # ë°ì´í„°í”„ë ˆì„ì— ë¸”ë™ ìŠ¤ì™„ í‘œì‹œì ì¶”ê°€
            df['black_swan_event'] = df['date'].isin(black_swan_dates).astype(int)
            
            # ë¸”ë™ ìŠ¤ì™„ í›„ ê²½ê³¼ ì¼ìˆ˜ ê³„ì‚°
            df = df.sort_values('date').reset_index(drop=True)
            df['days_since_black_swan'] = 999  # ê¸°ë³¸ê°’ ì„¤ì • default
            last_black_swan_date = None
            
            for date in df['date'].unique():
                if date in black_swan_dates:
                    last_black_swan_date = date
                    df.loc[df['date'] == date, 'days_since_black_swan'] = 0
                elif last_black_swan_date is not None:
                    days_diff = (date - last_black_swan_date).days
                    df.loc[df['date'] == date, 'days_since_black_swan'] = days_diff
    except Exception as e:
        st.warning(f"Error processing black swan events: {str(e)}")
    # 3. ê° ì¹´í…Œê³ ë¦¬ë³„ ìµœê·¼ ê·¹ë‹¨ì  ì´ë²¤íŠ¸ ê±°ë¦¬ ê³„ì‚°
    try:
        if 'category' in df.columns and black_swan_df is not None and not black_swan_df.empty:
            categories = df['category'].unique()

            df['days_since_category_balck_swan'] = 999
            
            for category in categories:
                # ì¹´í…Œê³ ë¦¬ë³„ ë¸”ë™ ìŠ¤ì™„ ì´ë²¤íŠ¸
                category_black_swans = black_swan_df[black_swan_df['category'] == category]

                if not category_black_swans.empty:
                        # ê° ë‚ ì§œë³„ë¡œ ê°€ì¥ ê°€ê¹Œìš´ ë¸”ë™ ìŠ¤ì™„ ì´ë²¤íŠ¸ì™€ì˜ ê±°ë¦¬ ê³„ì‚°
                        df_category = df[df['category'] == category].sort_values('date')
                        
                        for idx in df_category.index:
                            current_date = df_category.loc[idx, 'date']
                            
                            # ì´ ë‚ ì§œ ì´ì „ì˜ ê°€ì¥ ìµœê·¼ ë¸”ë™ ìŠ¤ì™„ ì°¾ê¸°
                            prev_black_swans = category_black_swans[category_black_swans['date'] <= current_date]
                            
                            if not prev_black_swans.empty:
                                latest_black_swan = prev_black_swans['date'].max()
                                days_diff = (current_date - latest_black_swan).days
                                df.at[i, 'days_since_category_black_swan'] = days_diff
    except Exception as e:
        st.warning(f"Error processing category-specific black swan events: {str(e)}")

    # 4. ìƒê´€ê´€ê³„ ë¶•ê´´ ì´ë²¤íŠ¸ íŠ¹ì„±
    try:
        if correlation_breakdown_df is not None and not correlation_breakdown_df.empty:
            # ìƒê´€ê´€ê³„ ë¶•ê´´ ë°œìƒ ì—¬ë¶€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” íŠ¹ì„± ìƒì„±
            breakdown_dates = correlation_breakdown_df['date'].unique()
            
            # ë°ì´í„°í”„ë ˆì„ì— ìƒê´€ê´€ê³„ ë¶•ê´´ í‘œì‹œì ì¶”ê°€
            df['correlation_breakdown'] = df['date'].isin(breakdown_dates).astype(int)
    except Exception as e:
        st.warning(f"Error processing correlation breakdown events: {str(e)}")
    
    # ìµœì¢…ì ìœ¼ë¡œ ì±„ì›Œì§€ì§€ ì•Šì€ ê°’ì„ 0ìœ¼ë¡œ ì„¤ì •
    for col in ['extreme_economic_event', 'black_swan_event', 'correlation_breakdown']:
        if col in df.columns:
            df[col] = df[col].fillna(0)
    
    # ê²°ì¸¡ì¹˜ê°€ ìˆëŠ” ë‹¤ë¥¸ ì—´ë“¤ì€ ì ì ˆí•œ ê°’ìœ¼ë¡œ ì±„ì›€
    for col in ['extreme_event_days', 'days_since_black_swan', 'days_since_category_black_swan']:
        if col in df.columns:
            df[col] = df[col].fillna(999)  # ë†’ì€ ê°’ìœ¼ë¡œ ì„¤ì • (ì˜¤ë˜ ì „ì— ë°œìƒ)
    
    return df
def visualize_extreme_events(extreme_events_df, black_swan_df, daily_avg_prices, daily_indicators, technical_patterns_df=None, category=None):
    """
    ê·¹ë‹¨ì  ì´ë²¤íŠ¸ì™€ ë¸”ë™ ìŠ¤ì™„ ì´ë²¤íŠ¸ë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤.
    
    Parameters:
    -----------
    extreme_events_df : DataFrame
        ê°ì§€ëœ ê·¹ë‹¨ì  ì´ë²¤íŠ¸ ë°ì´í„°í”„ë ˆì„
    black_swan_df : DataFrame
        ê°ì§€ëœ ë¸”ë™ ìŠ¤ì™„ ì´ë²¤íŠ¸ ë°ì´í„°í”„ë ˆì„
    daily_avg_prices : DataFrame
        ì¼ë³„ í‰ê·  ê°€ê²© ë°ì´í„°
    daily_indicators : DataFrame
        ì¼ë³„ ì§€í‘œ ë°ì´í„°
    technical_patterns_df : DataFrame, optional
        ê°ì§€ëœ ê¸°ìˆ ì  íŒ¨í„´ ë°ì´í„°í”„ë ˆì„
    category : str, optional
        íŠ¹ì • ì¹´í…Œê³ ë¦¬ì— ëŒ€í•œ ì‹œê°í™” (ê¸°ë³¸ê°’: None)
    """
    st.subheader("Extreme Event and Visualization")

    try:
        if daily_avg_prices is None or daily_indicators is None:
            st.warning("No Price data or Indicator data")
            return
        
        if extreme_events_df is not None and 'date' in extreme_events_df.columns and not pd.api.types.is_datetime64_any_dtype(extreme_events_df['date']):
            extreme_events_df['date'] = pd.to_datetime(extreme_events_df['date'], errors='coerce')

        if black_swan_df is not None and 'date' in black_swan_df.columns and not pd.api.types.is_datetime64_any_dtype(black_swan_df['date']):
            black_swan_df['date'] = pd.to_datetime(black_swan_df['date'], errors='coerce')

        if daily_avg_prices is not None and not pd.api.types.is_datetime64_any_dtype(daily_avg_prices.index):
            daily_avg_prices.index = pd.to_datetime(daily_avg_prices.index, errors='coerce')

        if daily_indicators is not None and not pd.api.types.is_datetime64_any_dtype(daily_indicators.index):
            daily_indicators.index = pd.to_datetime(daily_indicators.index, errors='coerce')

        # í•„í„°ë§
        if category:
            if black_swan_df is not None and not black_swan_df.empty:
                black_swan_df = black_swan_df[black_swan_df['category'] == category]

    except Exception as e:
        st.error(f"An error occurred during data preprocessing: {str(e)}")
        return
    # 1. Price Trends Around Black Swan Events
    # 1. Price Trends Around Black Swan Events
    if daily_avg_prices is not None and not daily_avg_prices.empty:
        st.write("### Price Trends Around Black Swan Events")
        
        categories_to_plot = [category] if category else daily_avg_prices.columns[:min(4, len(daily_avg_prices.columns))]
        
        # ê²©ì í˜•ì‹ìœ¼ë¡œ í‘œì‹œí•˜ê¸° ìœ„í•œ ì¤€ë¹„
        num_categories = len(categories_to_plot)
        cols_per_row = 2  # í•œ ì¤„ì— 2ê°œì˜ ê·¸ë˜í”„
        num_rows = (num_categories + cols_per_row - 1) // cols_per_row  # ì˜¬ë¦¼ ë‚˜ëˆ—ì…ˆ
        
        # ê° í–‰ë§ˆë‹¤ 2ê°œì˜ ì»¬ëŸ¼ ìƒì„±
        for row in range(num_rows):
            cols = st.columns(cols_per_row)
            
            for col_idx in range(cols_per_row):
                cat_idx = row * cols_per_row + col_idx
                
                # ì¹´í…Œê³ ë¦¬ ì¸ë±ìŠ¤ê°€ ìœ íš¨í•œì§€ í™•ì¸
                if cat_idx < num_categories:
                    cat = categories_to_plot[cat_idx]
                    
                    with cols[col_idx]:
                        try:
                            fig, ax = plt.subplots(figsize=(8, 6))
                            
                            # ê°€ê²© ë°ì´í„° í”Œë¡¯
                            ax.plot(daily_avg_prices.index, daily_avg_prices[cat], label=cat, color='blue')
                            
                            # ë¸”ë™ ìŠ¤ì™„ ì´ë²¤íŠ¸ í‘œì‹œ
                            if black_swan_df is not None and not black_swan_df.empty:
                                cat_black_swans = black_swan_df[black_swan_df['category'] == cat]
                                
                                for _, event in cat_black_swans.iterrows():
                                    event_date = event['date']
                                    if event_date in daily_avg_prices.index:
                                        price_at_event = daily_avg_prices.loc[event_date, cat]
                                        event_type = event['event_type']
                                        color = 'red' if 'Down' in event_type else 'green'
                                        
                                        ax.scatter(event_date, price_at_event, color=color, s=100, zorder=5)
                                        ax.annotate(f"{event_type} ({event['z_score']:.1f})", 
                                                (event_date, price_at_event),
                                                textcoords="offset points", 
                                                xytext=(0, 10), 
                                                ha='center')
                            
                            # 20ì¼ ì´ë™í‰ê· ì„  ì¶”ê°€
                            ma_20 = daily_avg_prices[cat].rolling(window=20).mean()
                            ax.plot(daily_avg_prices.index, ma_20, label='20-day moving average', color='orange', linestyle='--')
                            
                            # 50ì¼ ì´ë™í‰ê· ì„  ì¶”ê°€
                            ma_50 = daily_avg_prices[cat].rolling(window=50).mean()
                            ax.plot(daily_avg_prices.index, ma_50, label='50-day moving average', color='green', linestyle='--')
                            
                            ax.set_title(f'{cat} Price trend & black swan event', fontsize=16)
                            ax.set_xlabel('Date', fontsize=14)
                            ax.set_ylabel('Price', fontsize=14)
                            ax.legend()
                            ax.grid(True, alpha=0.3)
                            
                            # xì¶• ë‚ ì§œ í˜•ì‹ ê°œì„ 
                            plt.xticks(rotation=45)
                            fig.tight_layout()
                            st.pyplot(fig)
                            plt.close()
                            
                        except Exception as e:
                            st.error(f"{cat} ì‹œê°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
    
    st.markdown("""
    # ğŸ“Š Analyse von Black-Swan-Ereignissen nach Anlageklassen und globalen Themen

    ---

    ## ğŸ›¡ï¸ Verteidigungssektor (Defense)

    | ğŸ“… Datum | ğŸ“ˆ Ereignistyp | ğŸ“Š Z-Score | ğŸŒ Globale Themen |
    |:--------|:--------------|:----------|:-----------------|
    | 2023-04 | Extremer RÃ¼ckgang | -3,1 | Stillstand im Russland-Ukraine-Krieg, Unsicherheit Ã¼ber westliche UnterstÃ¼tzung |
    | 2023-10 | Extremer Anstieg | 5,9 | Ausbruch des Israel-Hamas-Krieges, Aussicht auf erhÃ¶hte globale Verteidigungsausgaben |
    | 2024-01 | Extremer RÃ¼ckgang | -3,6 | Nachlassende Besorgnis Ã¼ber Nahost-Konflikt, Ukraine-UnterstÃ¼tzungsmÃ¼digkeit |
    | 2024-07 | Extremer Anstieg | 3,2 | NATO-Erweiterung, VerlÃ¤ngerung des Ukraine-Krieges |
    | 2024-11 | Extremer Anstieg | 4,3 | VerstÃ¤rkte Verteidigungsbudgets nach US-Wahl |
    | 2025-01 | Extremer Anstieg | 4,1 | Verteidigungsorientierte Politik der neuen Regierung |

    **ğŸ” Anlageimplikationen:**  
    - Strukturelles AufwÃ¤rtsmomentum bei geopolitischen Krisen  
    - AufwÃ¤rtsereignisse stÃ¤rker ausgeprÃ¤gt als RÃ¼ckgÃ¤nge

    ---

    ## ğŸª™ Goldmarkt (Gold)

    | ğŸ“… Datum | ğŸ“ˆ Ereignistyp | ğŸ“Š Z-Score | ğŸŒ Globale Themen |
    |:--------|:--------------|:----------|:-----------------|
    | 2023-09 | Extremer Anstieg | 3,2 | HÃ¶hepunkt der ZinserhÃ¶hungen, Fed-Lockerungserwartungen, Nahost-Spannungen |
    | 2024-05 | Extremer RÃ¼ckgang | -3,6 | Steigende Inflation, starke US-Dollar-Performance |
    | 2024-11 | Extremer RÃ¼ckgang | -3,6 | US-Wahlergebnis, UnsicherheitsrÃ¼ckgang, Risikoanlagen bevorzugt |

    **ğŸ” Anlageimplikationen:**  
    - Hohe SensitivitÃ¤t gegenÃ¼ber geopolitischen und geldpolitischen Faktoren  
    - Langfristiger AufwÃ¤rtstrend trotz kurzfristiger Schwankungen

    ---

    ## ğŸ¤– KI-Technologieaktien (Tech_AI)

    | ğŸ“… Datum | ğŸ“ˆ Ereignistyp | ğŸ“Š Z-Score | ğŸŒ Globale Themen |
    |:--------|:--------------|:----------|:-----------------|
    | 2023-06 | Extremer RÃ¼ckgang | -3,8 | AbkÃ¼hlung der KI-Euphorie, anhaltende Fed-Straffung |
    | 2023-12 | Extremer RÃ¼ckgang | -3,0 | Gewinnmitnahmen zum Jahresende, schwache KI-Gewinne |
    | 2024-03 | Extremer Anstieg | 3,3 | Starke KI-Firmenzahlen, Zinssenkungserwartungen |
    | 2024-06 | Extremer RÃ¼ckgang | -4,0 | RegulierungsÃ¤ngste im KI-Sektor, Inflation steigt |
    | 2024-10 | Extremer Anstieg | 3,5 | Tech-Rally vor US-Wahl |
    | 2025-02 | Extremer RÃ¼ckgang | -3,4 | RegulierungsÃ¤ngste, Ãœberbewertungsbedenken |

    **ğŸ” Anlageimplikationen:**  
    - HÃ¶chste VolatilitÃ¤t, hÃ¶chste Ertragschancen  
    - Extrem abhÃ¤ngig von Zinspolitik, Innovation und Regulierung

    ---

    ## ğŸ“ˆ Vergleich der Anlageklassen & Investmentstrategie

    | ğŸ¦ Anlageklasse | ğŸ“ˆ Anstieg (2023â€“2025) | ğŸ•Šï¸ Black-Swan-Charakteristik | ğŸ¯ SensitivitÃ¤tsfaktoren |
    |:---------------|:----------------------|:----------------------------|:------------------------|
    | Verteidigungssektor | ca. 93% | Ãœberwiegend AufwÃ¤rtsereignisse | Geopolitische Spannungen, Verteidigungsbudgets |
    | Gold | ca. 67% | Gemischte Auf- und AbwÃ¤rtsereignisse | Inflation, Geldpolitik, Krisen |
    | KI-Technologieaktien | ca. 200% | HÃ¶chste VolatilitÃ¤t | Zinspolitik, Innovation, Regulierung |

    **ğŸš€ Cross-Asset-Investmentstrategie:**  
    - ğŸ”º Geopolitische Spannungen/Wirtschaftliche Unsicherheit â†‘ â†’ Defense + Gold aufstocken  
    - ğŸ“ˆ Innovationsgetriebene Boom-Phase erwartet â†’ Tech-Anteil erhÃ¶hen  
    - ğŸ›¡ï¸ Black-Swan-Risiken â” Durch **breite Diversifikation** abfedern

    ---
    """)

        
    # 2. ê·¹ë‹¨ì  ê²½ì œ ì§€í‘œ ì´ë²¤íŠ¸ ì‹œê°í™”
    if extreme_events_df is not None and not extreme_events_df.empty and daily_indicators is not None:
        economic_extremes = extreme_events_df[
            (extreme_events_df['event_type'] == 'Extreme High') | 
            (extreme_events_df['event_type'] == 'Extreme Low')
        ]
        
        if not economic_extremes.empty:
            st.write("### Extreme Event & Economic Indicator")
            
            # ìƒìœ„ ë¹ˆë„ ì§€í‘œ ì„ íƒ
            top_indicators = economic_extremes['indicator'].value_counts().head(4).index.tolist()
            
            # ê²©ì í˜•ì‹ìœ¼ë¡œ í‘œì‹œí•˜ê¸° ìœ„í•œ ì¤€ë¹„
            num_indicators = len(top_indicators)
            cols_per_row = 2  # í•œ ì¤„ì— 2ê°œì˜ ê·¸ë˜í”„
            num_rows = (num_indicators + cols_per_row - 1) // cols_per_row  # ì˜¬ë¦¼ ë‚˜ëˆ—ì…ˆ
            
            # ê° í–‰ë§ˆë‹¤ 2ê°œì˜ ì»¬ëŸ¼ ìƒì„±
            for row in range(num_rows):
                cols = st.columns(cols_per_row)
                
                for col_idx in range(cols_per_row):
                    ind_idx = row * cols_per_row + col_idx
                    
                    # ì§€í‘œ ì¸ë±ìŠ¤ê°€ ìœ íš¨í•œì§€ í™•ì¸
                    if ind_idx < num_indicators:
                        indicator = top_indicators[ind_idx]
                        
                        with cols[col_idx]:
                            try:
                                # ì§€í‘œëª…ì—ì„œ '_norm'ë¥¼ ì œê±°
                                clean_indicator = indicator.replace('_norm', '')
                                
                                # ì›ë³¸ ì§€í‘œ ë°ì´í„° ì¶”ì¶œ
                                indicator_data = daily_indicators[indicator] if indicator in daily_indicators.columns else None
                                
                                if indicator_data is None:
                                    st.warning(f"{indicator} ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                                    continue
                                
                                fig, ax = plt.subplots(figsize=(8, 6))
                                
                                # ì§€í‘œ ë°ì´í„° í”Œë¡¯
                                ax.plot(daily_indicators.index, indicator_data, label=clean_indicator, color='blue')
                                
                                # ê·¹ë‹¨ì  ì´ë²¤íŠ¸ í‘œì‹œ
                                indicator_extremes = economic_extremes[economic_extremes['indicator'] == indicator]
                                
                                for _, event in indicator_extremes.iterrows():
                                    event_date = event['date']
                                    if event_date in daily_indicators.index:
                                        value_at_event = indicator_data.loc[event_date]
                                        event_type = event['event_type']
                                        color = 'red' if event_type == 'Extreme High' else 'green'
                                        
                                        ax.scatter(event_date, value_at_event, color=color, s=60, zorder=2.5)
                                
                                # 90% ë° 10% ë¶„ìœ„ìˆ˜ ë¼ì¸ ì¶”ê°€
                                high_threshold = indicator_data.quantile(0.90)
                                low_threshold = indicator_data.quantile(0.10)
                                
                                ax.axhline(y=high_threshold, color='red', linestyle='--', 
                                        label=f'top 10% threshold ({high_threshold:.2f})')
                                ax.axhline(y=low_threshold, color='green', linestyle='--', 
                                        label=f'low 10% threshold ({low_threshold:.2f})')
                                
                                ax.set_title(f'{clean_indicator} trend & extreme event', fontsize=16)
                                ax.set_xlabel('date', fontsize=14)
                                ax.set_ylabel(clean_indicator, fontsize=14)
                                ax.legend()
                                ax.grid(True, alpha=0.3)
                                
                                # xì¶• ë‚ ì§œ í˜•ì‹ ê°œì„ 
                                plt.xticks(rotation=45)
                                fig.tight_layout()
                                
                                st.pyplot(fig)
                                plt.close()
                            except Exception as e:
                                st.error(f"{indicator} ì‹œê°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

    # 3. ë‹¤ì°¨ì› ë¶„ì„: ë¸”ë™ ìŠ¤ì™„ ì´ë²¤íŠ¸ì™€ ì§€í‘œ ìƒê´€ê´€ê³„
    if black_swan_df is not None and not black_swan_df.empty:
        st.write("### ğŸ“Š Indicator Trends Before & After Black Swan Events")

        try:
            # ì¸ë±ìŠ¤ê°€ datetime í˜•ì‹ì¸ì§€ í™•ì¸í•˜ê³  ë³€í™˜
            if not pd.api.types.is_datetime64_any_dtype(daily_indicators.index):
                daily_indicators.index = pd.to_datetime(daily_indicators.index, errors='coerce')

            # ë¸”ë™ ìŠ¤ì™„ ì´ë²¤íŠ¸ ë‚ ì§œ ë³€í™˜
            black_swan_df['date'] = pd.to_datetime(black_swan_df['date'], errors='coerce')
            bs_dates = black_swan_df['date'].dropna().tolist()
            bs_types = black_swan_df['event_type'].tolist() if len(black_swan_df) > 0 else []
            
            # ë¸”ë™ ìŠ¤ì™„ ì´ë²¤íŠ¸ ì¹´í…Œê³ ë¦¬ ì¶”ê°€
            bs_categories = black_swan_df['category'].tolist() if 'category' in black_swan_df.columns and len(black_swan_df) > 0 else []

            # ë¶„ì„ ë²”ìœ„
            window_before = 20
            window_after = 20

            if daily_indicators is not None and len(bs_dates) > 0:
                # ì£¼ìš” ì§€í‘œ ì„ íƒ
                key_indicators = [col for col in daily_indicators.columns if any(
                    ind in col for ind in [
                        'GDP_norm', 'CPI_norm', 'Dollar_Index_norm',
                        'fear_greed_value_norm', 'sentiment_score_mean_norm'
                    ]
                )][:4]

                if key_indicators:
                    # ê²©ì í˜•ì‹ìœ¼ë¡œ í‘œì‹œí•˜ê¸° ìœ„í•œ ì¤€ë¹„
                    num_indicators = len(key_indicators)
                    cols_per_row = 2  # í•œ ì¤„ì— 2ê°œì˜ ê·¸ë˜í”„
                    num_rows = (num_indicators + cols_per_row - 1) // cols_per_row  # ì˜¬ë¦¼ ë‚˜ëˆ—ì…ˆ
                    
                    # ê° í–‰ë§ˆë‹¤ 2ê°œì˜ ì»¬ëŸ¼ ìƒì„±
                    for row in range(num_rows):
                        cols = st.columns(cols_per_row)
                        
                        for col_idx in range(cols_per_row):
                            ind_idx = row * cols_per_row + col_idx
                            
                            # ì§€í‘œ ì¸ë±ìŠ¤ê°€ ìœ íš¨í•œì§€ í™•ì¸
                            if ind_idx < num_indicators:
                                indicator = key_indicators[ind_idx]
                                ind_name = indicator.split('_')[0]  # ì§€í‘œ ì´ë¦„
                                
                                with cols[col_idx]:
                                    try:
                                        fig, ax = plt.subplots(figsize=(8, 6))
                                        
                                        # ëª…í™•í•œ ìƒ‰ìƒ ì„¤ì •
                                        event_colors = ['blue', 'orange', 'green', 'red', 'purple', 'brown']
                                        
                                        # ê° ì´ë²¤íŠ¸ë³„ ë°ì´í„° ì‹œê°í™”
                                        max_events = min(len(bs_dates), 3)  # ìµœëŒ€ 3ê°œ ì´ë²¤íŠ¸ë§Œ í‘œì‹œ
                                        
                                        for j in range(max_events):
                                            if j < len(bs_dates) and j < len(bs_types):
                                                event_date = bs_dates[j]
                                                event_type = bs_types[j]
                                                event_category = bs_categories[j] if j < len(bs_categories) else "Unknown"
                                                
                                                # ìœ íš¨í•œ datetimeì¸ì§€ í™•ì¸
                                                if pd.isnull(event_date) or not isinstance(event_date, pd.Timestamp):
                                                    continue

                                                start_date = event_date - pd.Timedelta(days=window_before)
                                                end_date = event_date + pd.Timedelta(days=window_after)

                                                # ë‚ ì§œ ë²”ìœ„ì˜ ì§€í‘œ ë°ì´í„° ì¶”ì¶œ
                                                mask = (daily_indicators.index >= start_date) & (daily_indicators.index <= end_date)
                                                period_data = daily_indicators.loc[mask, indicator]

                                                if not period_data.empty:
                                                    days_from_event = [(d - event_date).days for d in period_data.index]
                                                    
                                                    # ëª…í™•í•œ ìƒ‰ìƒ í• ë‹¹ ë° ë” êµ¬ì²´ì ì¸ ë ˆì´ë¸” ì‚¬ìš©
                                                    color = event_colors[j % len(event_colors)]
                                                    event_label = f'Event {j+1}: {event_type} ({event_category})'
                                                    
                                                    ax.plot(days_from_event, period_data.values,
                                                        label=event_label, color=color, linewidth=2)

                                                    # ì´ë²¤íŠ¸ ë°œìƒì¼ ê°’ í‘œì‹œ
                                                    if event_date in period_data.index:
                                                        event_idx = period_data.index.get_loc(event_date)
                                                        value_at_event = period_data.iloc[event_idx]
                                                        
                                                        ax.scatter(0, value_at_event, 
                                                                color=color, s=100, zorder=5)

                                        # Yì¶• ìë™ ì¡°ì • (ë°ì´í„° ë²”ìœ„ì— ë§ê²Œ)
                                        if len(ax.get_lines()) > 0:  # ë¼ì¸ì´ ìˆëŠ”ì§€ í™•ì¸
                                            y_data = [line.get_ydata() for line in ax.get_lines()]
                                            if y_data and all(len(data) > 0 for data in y_data):
                                                # ëª¨ë“  ë¼ì¸ì˜ yê°’ ë²”ìœ„ ê³„ì‚°
                                                all_y = np.concatenate(y_data)
                                                if len(all_y) > 0:
                                                    # Yì¶• ë²”ìœ„ ì„¤ì • (ì•½ê°„ì˜ ì—¬ë°±ì„ ë‘ )
                                                    y_min, y_max = np.nanmin(all_y), np.nanmax(all_y)
                                                    y_range = y_max - y_min
                                                    if y_range > 0.0001:  # ë³€ë™ì´ ìˆëŠ” ê²½ìš°ì—ë§Œ ì¡°ì •
                                                        ax.set_ylim(y_min - y_range*0.1, y_max + y_range*0.1)
                                        
                                        ax.set_title(f'{ind_name} Before & After Black Swan Events', fontsize=14)
                                        ax.set_xlabel('Days from Event (0 = Event)', fontsize=12)
                                        ax.set_ylabel(ind_name, fontsize=12)
                                        ax.axvline(x=0, color='black', linestyle='--', alpha=0.5)
                                        ax.legend()
                                        ax.grid(True, alpha=0.3)
                                        
                                        plt.tight_layout()
                                        st.pyplot(fig)
                                        plt.close()
                                    except Exception as e:
                                        st.error(f"âŒ {ind_name} ì‹œê°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        except Exception as e:
            st.error(f"âŒ Error while analyzing before/after Black Swan events: {str(e)}")

    # 4. ê¸°ìˆ ì  ì§€í‘œ íŒ¨í„´ê³¼ ê·¹ë‹¨ì  ì´ë²¤íŠ¸ì˜ ê´€ê³„
    if isinstance(technical_patterns_df, pd.DataFrame) and not technical_patterns_df.empty:
        if category:
            technical_patterns_df = technical_patterns_df[technical_patterns_df['category'] == category]
        
        if not technical_patterns_df.empty:
            st.write("### How Technical Patterns Relate to Extreme Market Events")
            
            try:
                # ì‹œê°í™”ë¥¼ ë‘ ì»¬ëŸ¼ìœ¼ë¡œ ë‚˜ëˆ„ê¸°
                col1, col2 = st.columns(2)
                
                with col1:
                    # íŒ¨í„´ ìœ í˜•ë³„ ë¹ˆë„ìˆ˜
                    pattern_counts = technical_patterns_df['pattern'].value_counts()
                    
                    # ë§‰ëŒ€ ê·¸ë˜í”„ë¡œ ì‹œê°í™”
                    fig, ax = plt.subplots(figsize=(8, 6))
                    pattern_counts.plot(kind='bar', color='teal', ax=ax)
                    
                    plt.title('Frequency of technical pattern occurrence', fontsize=16)
                    plt.xlabel('Pattern Types', fontsize=14)
                    plt.ylabel('Frequency of occurrence', fontsize=14)
                    plt.xticks(rotation=45, ha='right')
                    plt.tight_layout()
                    
                    st.pyplot(fig)
                    plt.close()

                # ê¸°ìˆ ì  íŒ¨í„´ ë°œìƒ ë‚ ì§œì™€ ë¸”ë™ ìŠ¤ì™„ ì´ë²¤íŠ¸ì˜ ê´€ê³„ ë¶„ì„
                if isinstance(black_swan_df, pd.DataFrame) and not black_swan_df.empty:
                    try:
                        with col2:
                            # ë‚ ì§œ ì»¬ëŸ¼ì„ ì•ˆì „í•˜ê²Œ datetimeìœ¼ë¡œ ë³€í™˜
                            technical_patterns_df['date'] = pd.to_datetime(technical_patterns_df['date'], errors='coerce')
                            black_swan_df['date'] = pd.to_datetime(black_swan_df['date'], errors='coerce')

                            # ìœ íš¨í•œ ë‚ ì§œë§Œ í•„í„°ë§
                            pattern_dates = technical_patterns_df['date'].dropna().unique()
                            black_swan_dates = black_swan_df['date'].dropna().unique()

                            # ë¶„ì„ì— ì‚¬ìš©í•  ì¼ ìˆ˜ ë²”ìœ„
                            days_thresholds = [1, 3, 5, 10, 20]
                            pattern_types = technical_patterns_df['pattern'].unique()
                            black_swan_counts = {}

                            for pattern in pattern_types:
                                black_swan_counts[pattern] = []
                                # í•´ë‹¹ íŒ¨í„´ì— í•´ë‹¹í•˜ëŠ” ë‚ ì§œë§Œ ì¶”ì¶œí•˜ê³  datetimeìœ¼ë¡œ ë³€í™˜
                                pattern_specific_dates = pd.to_datetime(
                                    technical_patterns_df[technical_patterns_df['pattern'] == pattern]['date'],
                                    errors='coerce'
                                ).dropna()

                                for days in days_thresholds:
                                    count = 0
                                    for date in pattern_specific_dates:
                                        end_date = date + pd.Timedelta(days=days)
                                        for bs_date in black_swan_dates:
                                            if date <= bs_date <= end_date:
                                                count += 1
                                                break
                                    black_swan_counts[pattern].append(count)

                            # ê²°ê³¼ ì‹œê°í™”
                            fig, ax = plt.subplots(figsize=(8, 6))
                            width = 0.15
                            x = np.arange(len(pattern_types))

                            for i, days in enumerate(days_thresholds):
                                counts = [black_swan_counts[pattern][i] for pattern in pattern_types]
                                ax.bar(x + i * width, counts, width, label=f'Within {days} Days')

                            ax.set_title('Black Swan Events After Pattern', fontsize=16)
                            ax.set_xlabel('Pattern Type', fontsize=14)
                            ax.set_ylabel('Number of Black Swan Events', fontsize=14)
                            ax.set_xticks(x + width * (len(days_thresholds) - 1) / 2)
                            ax.set_xticklabels(pattern_types, rotation=45, ha='right')
                            ax.legend()
                            plt.tight_layout()
                            st.pyplot(fig)
                            plt.close()
                        
                        # ì¶”ê°€ ë¶„ì„ ì„¹ì…˜ (íŒ¨í„´ë³„ ìƒì„¸ ë¶„ì„) - í•œ ì¤„ì— ë‘ ê°œì”© í‘œì‹œ
                        st.write("### Pattern-Specific Analysis")
                        
                        # íŒ¨í„´ ìœ í˜• ê·¸ë£¹í™” (ìµœëŒ€ 6ê°œê¹Œì§€ë§Œ í‘œì‹œ)
                        patterns_to_analyze = pattern_types[:min(6, len(pattern_types))]
                        num_patterns = len(patterns_to_analyze)
                        rows_needed = (num_patterns + 1) // 2  # ì˜¬ë¦¼ ë‚˜ëˆ—ì…ˆ
                        
                        for row in range(rows_needed):
                            # ê° í–‰ë§ˆë‹¤ 2ê°œì˜ ì»¬ëŸ¼ ìƒì„±
                            cols = st.columns(2)
                            
                            for col_idx in range(2):
                                pattern_idx = row * 2 + col_idx
                                
                                # íŒ¨í„´ ì¸ë±ìŠ¤ê°€ ìœ íš¨í•œì§€ í™•ì¸
                                if pattern_idx < num_patterns:
                                    pattern = patterns_to_analyze[pattern_idx]
                                    
                                    with cols[col_idx]:
                                        st.subheader(f"{pattern} Pattern Analysis")
                                        
                                        # í•´ë‹¹ íŒ¨í„´ì˜ ë°œìƒ ë¹ˆë„ì™€ ë¸”ë™ ìŠ¤ì™„ ì´ë²¤íŠ¸ ê´€ê³„
                                        pattern_dates = technical_patterns_df[technical_patterns_df['pattern'] == pattern]['date']
                                        
                                        # ê²°ê³¼ ì‹œê°í™” (íŒŒì´ ì°¨íŠ¸)
                                        fig, ax = plt.subplots(figsize=(8, 6))
                                        
                                        # ì´ íŒ¨í„´ ë°œìƒ í›„ 20ì¼ ì´ë‚´ì— ë¸”ë™ ìŠ¤ì™„ ì´ë²¤íŠ¸ê°€ ë°œìƒí•œ ë¹„ìœ¨ ê³„ì‚°
                                        pattern_occurrences = len(pattern_dates)
                                        followed_by_black_swan = black_swan_counts[pattern][-1]  # 20ì¼ ì´ë‚´ ë¸”ë™ ìŠ¤ì™„ ì´ë²¤íŠ¸
                                        not_followed = pattern_occurrences - followed_by_black_swan
                                        
                                        # íŒŒì´ ì°¨íŠ¸ ë°ì´í„°
                                        labels = ['Followed by Black Swan', 'Not Followed by Black Swan']
                                        sizes = [followed_by_black_swan, not_followed]
                                        colors = ['red', 'lightgray']
                                        explode = (0.1, 0)  # ì²« ë²ˆì§¸ ì¡°ê°ë§Œ ì•½ê°„ ë¶„ë¦¬
                                        
                                        ax.pie(sizes, explode=explode, labels=labels, colors=colors, autopct='%1.1f%%',
                                            shadow=True, startangle=90)
                                        ax.axis('equal')  # ì›í˜• íŒŒì´ ì°¨íŠ¸ë¥¼ ìœ„í•´
                                        
                                        plt.title(f'{pattern}: 20-Day Black Swan Follow-up', fontsize=14)
                                        plt.tight_layout()
                                        
                                        st.pyplot(fig)
                                        plt.close()
                                        
                                        # íŒ¨í„´ ë°œìƒ í›„ ì¼ë³„ ëˆ„ì  ë¸”ë™ ìŠ¤ì™„ ë°œìƒ ì¶”ì´
                                        if followed_by_black_swan > 0:
                                            cumulative_counts = [black_swan_counts[pattern][i] for i in range(len(days_thresholds))]
                                            
                                            fig, ax = plt.subplots(figsize=(8, 4))
                                            ax.plot(days_thresholds, cumulative_counts, marker='o', color='red', linewidth=2)
                                            
                                            ax.set_title(f'{pattern}: Cumulative Black Swan Events', fontsize=14)
                                            ax.set_xlabel('Days After Pattern', fontsize=12)
                                            ax.set_ylabel('Number of Events', fontsize=12)
                                            ax.grid(True, alpha=0.3)
                                            
                                            plt.tight_layout()
                                            st.pyplot(fig)
                                            plt.close()

                    except Exception as e:
                        st.error(f"âŒ Error analyzing technical patterns and black swan events: {str(e)}")
            except Exception as outer_e:
                st.error(f"âŒ Error generating technical pattern section: {str(outer_e)}")
def analyze_extreme_events_impact(df, extreme_events_df, black_swan_df, category=None):
    """
    ê·¹ë‹¨ì  ì´ë²¤íŠ¸ì™€ ë¸”ë™ ìŠ¤ì™„ ì´ë²¤íŠ¸ê°€ ìˆ˜ìµë¥ ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ë¶„ì„í•©ë‹ˆë‹¤.
    
    Parameters:
    -----------
    df : DataFrame
        ì›ë³¸ ë°ì´í„°í”„ë ˆì„
    extreme_events_df : DataFrame
        ê°ì§€ëœ ê·¹ë‹¨ì  ì´ë²¤íŠ¸ ë°ì´í„°í”„ë ˆì„
    black_swan_df : DataFrame
        ê°ì§€ëœ ë¸”ë™ ìŠ¤ì™„ ì´ë²¤íŠ¸ ë°ì´í„°í”„ë ˆì„
    category : str, optional
        ë¶„ì„í•  íŠ¹ì • ì¹´í…Œê³ ë¦¬ (ê¸°ë³¸ê°’: None)
    """
    st.subheader("Impact Analysis of Extreme Events on Returns")
    
    if df is None or df.empty:
        st.warning("ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # í•„ìš”í•œ ì»¬ëŸ¼ í™•ì¸
    required_cols = ['date', 'category', 'close']
    missing_cols = [col for col in required_cols if col not in df.columns]
    if missing_cols:
        st.error(f"í•„ìš”í•œ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤: {missing_cols}")
        return
    
    # ì¹´í…Œê³ ë¦¬ í•„í„°ë§
    if category:
        df_filtered = df[df['category'] == category].copy()
    else:
        df_filtered = df.copy()
    
    if len(df_filtered) == 0:
        st.warning("No data after filtering.")
        return
    
    # ìˆ˜ìµë¥  ê³„ì‚°
    df_filtered['daily_return'] = df_filtered.groupby('category')['close'].pct_change() * 100
    
    # 1. ë¸”ë™ ìŠ¤ì™„ ì´ë²¤íŠ¸ ì „í›„ ëˆ„ì  ìˆ˜ìµë¥  ë¶„ì„
    if black_swan_df is not None and not black_swan_df.empty:
        st.write("### Cumulative Returns Before and After Black Swan Events")
        
        try:
            # ì¹´í…Œê³ ë¦¬ ë° ë‚ ì§œë³„ë¡œ ë°ì´í„° ê·¸ë£¹í™”
            grouped = df_filtered.groupby(['category', 'date'])['daily_return'].mean().reset_index()
            
            # ì¹´í…Œê³ ë¦¬ ì„ íƒ
            categories_to_analyze = [category] if category else grouped['category'].unique()[:min(4, len(grouped['category'].unique()))]
            
            for cat in categories_to_analyze:
                # í•´ë‹¹ ì¹´í…Œê³ ë¦¬ì˜ ë¸”ë™ ìŠ¤ì™„ ì´ë²¤íŠ¸ í•„í„°ë§
                cat_black_swans = black_swan_df[(black_swan_df['category'] == cat) | 
                                              (black_swan_df['category'] == 'Economic Indicator')]
                
                if cat_black_swans.empty:
                    st.info(f"There are no black swan events available for the {cat} category.")
                    continue
                
                # í•´ë‹¹ ì¹´í…Œê³ ë¦¬ì˜ ë°ì´í„°
                cat_data = grouped[grouped['category'] == cat].sort_values('date')
                
                # ê° ë¸”ë™ ìŠ¤ì™„ ì´ë²¤íŠ¸ì— ëŒ€í•´ ì „í›„ 20ì¼ ê°„ì˜ ëˆ„ì  ìˆ˜ìµë¥  ê³„ì‚°
                window_size = 20
                
                # ê° ì¹´í…Œê³ ë¦¬ë§ˆë‹¤ ìƒˆë¡œìš´ figure ìƒì„±
                fig, ax = plt.subplots(figsize=(14, 8))
                
                for i, (_, event) in enumerate(cat_black_swans.head(5).iterrows()):  # ìµœëŒ€ 5ê°œ ì´ë²¤íŠ¸
                    event_date = event['date']
                    event_type = event['event_type']
                    
                    # ì´ë²¤íŠ¸ ì „í›„ ë°ì´í„° ì„ íƒ
                    mask = (cat_data['date'] >= event_date - pd.Timedelta(days=window_size)) & \
                           (cat_data['date'] <= event_date + pd.Timedelta(days=window_size))
                    event_window_data = cat_data.loc[mask].copy()
                    
                    if len(event_window_data) > 1:  # ì¶©ë¶„í•œ ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°
                        # ì´ë²¤íŠ¸ ë‚ ì§œë¥¼ 0ìœ¼ë¡œ ì¡°ì •
                        event_window_data['days_from_event'] = (event_window_data['date'] - event_date).dt.days
                        
                        # ëˆ„ì  ìˆ˜ìµë¥  ê³„ì‚° (ì´ë²¤íŠ¸ ë‚ ì§œë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë³µë¦¬í™”)
                        event_window_data['cum_return'] = 100 * ((1 + event_window_data['daily_return']/100).cumprod() - 1)
                        
                        # ì´ë²¤íŠ¸ ë‚ ì§œ ê¸°ì¤€ìœ¼ë¡œ ì¬ì¡°ì •
                        try: 
                            event_idx = event_window_data[event_window_data['days_from_event'] == 0].index
                        
                            if len(event_idx) > 0:
                                if 'cum_return' not in event_window_data.columns:
                                    st.warning("No cumulative return data available")
                                    continue
                                    
                                event_return = event_window_data.loc[event_idx[0], 'cum_return']
                                event_window_data['adjusted_cum_return'] = event_window_data['cum_return'] - event_return
                                
                                # í”Œë¡¯
                                ax.plot(
                                    event_window_data['days_from_event'], 
                                    event_window_data['adjusted_cum_return'],
                                    label=f"{event_date.strftime('%Y-%m-%d')}: {event_type}",
                                    marker='o', markersize=3
                                )
                        except Exception as e:
                            st.warning(f"ì´ë²¤íŠ¸ ìˆ˜ìµë¥  ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
                            continue 
                
                # ì´ë²¤íŠ¸ ë°œìƒì¼ í‘œì‹œ
                ax.axvline(x=0, color='red', linestyle='--', alpha=0.7, label='Event Date')
                ax.axhline(y=0, color='black', linestyle='-', alpha=0.3)
                
                ax.set_title(f'Cumulative Returns Before and After Black Swan Events for {cat}', fontsize=16)
                ax.set_xlabel('Days Relative to Event', fontsize=14)
                ax.set_ylabel('Cumulative Return (%)', fontsize=14)
                ax.legend(fontsize=10)
                ax.grid(True, alpha=0.3)
                
                plt.tight_layout()
                st.pyplot(fig)
                plt.close()  # ê° ì¹´í…Œê³ ë¦¬ ê·¸ë˜í”„ í›„ figure ë‹«ê¸°
                
        except Exception as e:
            st.error(f"ëˆ„ì  ìˆ˜ìµë¥  ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
    
    # 2. ê·¹ë‹¨ì  ê²½ì œ ì§€í‘œì™€ ì¹´í…Œê³ ë¦¬ë³„ ìˆ˜ìµë¥ ì˜ ìƒê´€ê´€ê³„
    if extreme_events_df is not None and not extreme_events_df.empty:
        st.write("### Correlation Between Extreme Economic Events and Returns")
        
        try:
            # ê²½ì œ ì§€í‘œ ê·¹ë‹¨ê°’ í•„í„°ë§
            economic_extremes = extreme_events_df[
                ((extreme_events_df['event_type'] == 'Extreme High') | 
                 (extreme_events_df['event_type'] == 'Extreme Low')) &
                (extreme_events_df['indicator'].str.contains('_norm'))
            ]
            
            if economic_extremes.empty:
                st.info("There are no extreme economic events available for analysis.")
            else:
                # ê·¹ë‹¨ì  ì´ë²¤íŠ¸ê°€ ë°œìƒí•œ ë‚ ì§œ
                extreme_dates = economic_extremes['date'].unique()
                
                # ì´ë²¤íŠ¸ ë°œìƒ ì´í›„ 1, 3, 5, 10ì¼ ê°„ì˜ í‰ê·  ìˆ˜ìµë¥  ê³„ì‚°
                holding_periods = [1, 3, 5, 10]
                
                # ì¹´í…Œê³ ë¦¬ ë° ì§€í‘œë³„ ì´ë²¤íŠ¸ í›„ ìˆ˜ìµë¥ 
                results = []
                
                for indicator in economic_extremes['indicator'].unique():
                    indicator_name = indicator.split('_')[0]  # ì§€í‘œëª… ì¶”ì¶œ
                    
                    high_events = economic_extremes[
                        (economic_extremes['indicator'] == indicator) & 
                        (economic_extremes['event_type'] == 'Extreme High')
                    ]
                    
                    low_events = economic_extremes[
                        (economic_extremes['indicator'] == indicator) & 
                        (economic_extremes['event_type'] == 'Extreme Low')
                    ]
                    
                    for cat in df_filtered['category'].unique():
                        cat_data = df_filtered[df_filtered['category'] == cat].sort_values('date')
                        
                        # ê° ë³´ìœ  ê¸°ê°„ì— ëŒ€í•´
                        for days in holding_periods:
                            # ê·¹ë‹¨ì  ê³ ì  ì´ë²¤íŠ¸ í›„ ìˆ˜ìµë¥ 
                            high_returns = []
                            for event_date in high_events['date']:
                                future_data = cat_data[cat_data['date'] > event_date].head(days)
                                if len(future_data) == days:  # ì¶©ë¶„í•œ ë¯¸ë˜ ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°
                                    cum_return = 100 * ((1 + future_data['daily_return']/100).prod() - 1)
                                    high_returns.append(cum_return)
                            
                            high_mean = np.mean(high_returns) if high_returns else np.nan
                            
                            # ê·¹ë‹¨ì  ì €ì  ì´ë²¤íŠ¸ í›„ ìˆ˜ìµë¥ 
                            low_returns = []
                            for event_date in low_events['date']:
                                future_data = cat_data[cat_data['date'] > event_date].head(days)
                                if len(future_data) == days:  # ì¶©ë¶„í•œ ë¯¸ë˜ ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°
                                    cum_return = 100 * ((1 + future_data['daily_return']/100).prod() - 1)
                                    low_returns.append(cum_return)
                            
                            low_mean = np.mean(low_returns) if low_returns else np.nan
                            
                            # ê²°ê³¼ ì €ì¥
                            results.append({
                                'indicator': indicator_name,
                                'category': cat,
                                'holding_period': days,
                                'high_event_return': high_mean,
                                'low_event_return': low_mean,
                                'high_event_count': len(high_returns),
                                'low_event_count': len(low_returns)
                            })
                
                # ê²°ê³¼ë¥¼ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜
                if results:
                    result_df = pd.DataFrame(results)
                    
                    # ì§€í‘œë³„ í‰ê·  ìˆ˜ìµë¥  ì‹œê°í™”
                    for period in holding_periods:
                        period_data = result_df[result_df['holding_period'] == period]
                        
                        if not period_data.empty:
                            fig, ax = plt.subplots(figsize=(14, 8))
                            
                            # ì§€í‘œë³„ë¡œ ê·¸ë£¹í™”
                            indicator_means = period_data.groupby('indicator').agg({
                                'high_event_return': 'mean',
                                'low_event_return': 'mean'
                            }).reset_index()
                            
                            x = np.arange(len(indicator_means))
                            width = 0.35
                            
                            ax.bar(x - width/2, indicator_means['high_event_return'], width, 
                                   label='Returns After Extreme High Events', color='red', alpha=0.7)
                            ax.bar(x + width/2, indicator_means['low_event_return'], width, 
                                   label='Returns After Extreme Low Events', color='blue', alpha=0.7)
                            
                            ax.set_title(f'Average Returns After Extreme Economic Events - {period}-Day Holding Period', fontsize=16)
                            ax.set_xlabel('Economic Indicator', fontsize=14)
                            ax.set_ylabel('Avg Return (%)', fontsize=14)
                            ax.set_xticks(x)
                            ax.set_xticklabels(indicator_means['indicator'], rotation=45, ha='right')
                            ax.legend()
                            ax.grid(True, alpha=0.3)
                            ax.axhline(y=0, color='black', linestyle='-', alpha=0.5)
                            
                            plt.tight_layout()
                            st.pyplot(fig)
                            plt.close()
                    
                    # ê²°ê³¼ í…Œì´ë¸” í‘œì‹œ
                    st.write("#### Correlation Between Extreme Economic Indicators and Returns")
                    st.dataframe(result_df.round(2))
                    
        except Exception as e:
            st.error(f"ê·¹ë‹¨ì  ê²½ì œ ì§€í‘œ ì˜í–¥ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
def create_extreme_enhanced_data(model_data, extreme_events_df, black_swan_df, technical_patterns_df=None, correlation_breakdown_df=None, predictions_data=None):
    """
    ê·¹ë‹¨ì  ì´ë²¤íŠ¸ íŠ¹ì„±ì„ ëª¨ë¸ ë°ì´í„°ì— í†µí•©í•˜ê³  ë¨¸ì‹ ëŸ¬ë‹ ì˜ˆì¸¡ ê²°ê³¼ë„ ì¶”ê°€í•©ë‹ˆë‹¤.
    
    Parameters:
    -----------
    model_data : dict ë˜ëŠ” DataFrame
        ì¹´í…Œê³ ë¦¬ë³„ ëª¨ë¸ ë°ì´í„° ë”•ì…”ë„ˆë¦¬ ë˜ëŠ” ë‹¨ì¼ DataFrame
    extreme_events_df : DataFrame
        ê°ì§€ëœ ê·¹ë‹¨ì  ì´ë²¤íŠ¸ ë°ì´í„°í”„ë ˆì„
    black_swan_df : DataFrame
        ê°ì§€ëœ ë¸”ë™ ìŠ¤ì™„ ì´ë²¤íŠ¸ ë°ì´í„°í”„ë ˆì„
    technical_patterns_df : DataFrame, optional
        ê°ì§€ëœ ê¸°ìˆ ì  íŒ¨í„´ ë°ì´í„°í”„ë ˆì„
    correlation_breakdown_df : DataFrame, optional
        ê°ì§€ëœ ìƒê´€ê´€ê³„ ë¶•ê´´ ì´ë²¤íŠ¸ ë°ì´í„°í”„ë ˆì„
    predictions_data : dict, optional
        ë¨¸ì‹ ëŸ¬ë‹ ì˜ˆì¸¡ ê²°ê³¼ (ì¹´í…Œê³ ë¦¬ë³„ predictions ì»¬ëŸ¼ í¬í•¨í•œ ë°ì´í„°í”„ë ˆì„)
    
    Returns:
    --------
    extreme_enhanced_model_data : dict
        ê·¹ë‹¨ì  ì´ë²¤íŠ¸ íŠ¹ì„±ê³¼ ì˜ˆì¸¡ ê²°ê³¼ê°€ ì¶”ê°€ëœ ëª¨ë¸ ë°ì´í„° ë”•ì…”ë„ˆë¦¬
    """
    # ì…ë ¥ ë°ì´í„° ê²€ì¦
    if model_data is None:
        return model_data
        
    # DataFrameì„ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
    if isinstance(model_data, pd.DataFrame):
        if model_data.empty:
            return {}
            
        if 'category' in model_data.columns:
            model_data_dict = {}
            for category in model_data['category'].unique():
                model_data_dict[category] = model_data[model_data['category'] == category].copy()
            model_data = model_data_dict
        else:
            model_data = {'all': model_data.copy()}
    elif isinstance(model_data, dict):
        if len(model_data) == 0:
            return {}
    else:
        st.warning(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” model_data íƒ€ì…: {type(model_data)}")
        return {}
        
    extreme_enhanced_model_data = {}
    
    # ì§„í–‰ ìƒí™© í‘œì‹œ
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    categories = list(model_data.keys())
    
    for i, category in enumerate(categories):
        #status_text.text(f"{category} ëª¨ë¸ ë°ì´í„° í–¥ìƒ ì¤‘...")
        
        try:
            # ì¹´í…Œê³ ë¦¬ë³„ ëª¨ë¸ ë°ì´í„° ë³µì‚¬
            df = model_data[category].copy()
            
            # 1. ë¸”ë™ ìŠ¤ì™„ ì´ë²¤íŠ¸ íŠ¹ì„± ì¶”ê°€
            if black_swan_df is not None and not black_swan_df.empty:
                category_black_swans = black_swan_df[black_swan_df['category'] == category]
                
                if not category_black_swans.empty:
                    # ë¸”ë™ ìŠ¤ì™„ ì´ë²¤íŠ¸ ë°œìƒ ì—¬ë¶€
                    black_swan_dates = category_black_swans['date'].tolist()
                    df['black_swan_event'] = df.index.isin(black_swan_dates).astype(int)
                    
                    # ìµœê·¼ ë¸”ë™ ìŠ¤ì™„ ì´ë²¤íŠ¸ê¹Œì§€ì˜ ê±°ë¦¬
                    df['days_since_black_swan'] = 999  # ê¸°ë³¸ê°’
                    
                    for date in df.index:
                        date_ts = pd.Timestamp(date) if not isinstance(date, pd.Timestamp) else date
                        
                        # ì´ ë‚ ì§œ ì´ì „ì˜ ëª¨ë“  ë¸”ë™ ìŠ¤ì™„
                        prev_black_swans = []
                        for d in black_swan_dates:
                            d_ts = pd.Timestamp(d) if not isinstance(d, pd.Timestamp) else d
                            if d_ts <= date_ts:
                                prev_black_swans.append(d_ts)
                        
                        if prev_black_swans:
                            latest_black_swan = max(prev_black_swans)
                            days_diff = (date_ts - latest_black_swan).days
                            df.at[date, 'days_since_black_swan'] = days_diff
                    
                    # ë¸”ë™ ìŠ¤ì™„ Z-score íŠ¹ì„±
                    df['black_swan_zscore'] = 0
                    
                    for _, event in category_black_swans.iterrows():
                        if event['date'] in df.index:
                            df.at[event['date'], 'black_swan_zscore'] = event['z_score']
            
            # 2. ê·¹ë‹¨ì  ê²½ì œ ì§€í‘œ ì´ë²¤íŠ¸ íŠ¹ì„± ì¶”ê°€
            if extreme_events_df is not None and not extreme_events_df.empty:
                # ê²½ì œ ì§€í‘œ ê·¹ë‹¨ê°’ ì´ë²¤íŠ¸
                economic_extremes = extreme_events_df[
                    ((extreme_events_df['event_type'] == 'Extreme High') | 
                    (extreme_events_df['event_type'] == 'Extreme Low')) &
                    (extreme_events_df['indicator'].str.contains('_norm'))
                ]
                
                if not economic_extremes.empty:
                    # ê²½ì œ ì§€í‘œ ê·¹ë‹¨ ì´ë²¤íŠ¸ ë°œìƒ ì—¬ë¶€
                    extreme_dates = economic_extremes['date'].unique()
                    df['extreme_economic_event'] = df.index.isin(extreme_dates).astype(int)
                    
                    # ìµœê·¼ ê·¹ë‹¨ ì´ë²¤íŠ¸ê¹Œì§€ì˜ ê±°ë¦¬
                    df['days_since_extreme_event'] = 999  # ê¸°ë³¸ê°’
                    
                    for date in df.index:
                        date_ts = pd.Timestamp(date) if not isinstance(date, pd.Timestamp) else date
                         
                        prev_extreme_event = []
                        for d in extreme_dates:
                            d_ts = pd.Timestamp(d) if not isinstance(d, pd.Timestamp) else d
                            if d_ts <= date_ts:
                                prev_extreme_event.append(d_ts)
                        
                        if prev_extreme_event:
                            latest_extreme_event = max(prev_extreme_event)
                            days_diff = (date_ts - latest_extreme_event).days
                            df.at[date, 'days_since_extreme_event'] = days_diff
                    
                    # ì£¼ìš” ê²½ì œ ì§€í‘œë³„ ê·¹ë‹¨ ì´ë²¤íŠ¸ íŠ¹ì„±
                    important_indicators = ['GDP_norm', 'CPI_norm', 'Fed_Funds_Rate_norm', 'Dollar_Index_norm']
                    
                    for indicator in important_indicators:
                        # ê·¹ë‹¨ì  ê³ ì  ì´ë²¤íŠ¸
                        high_events = economic_extremes[
                            (economic_extremes['indicator'] == indicator) & 
                            (economic_extremes['event_type'] == 'Extreme High')
                        ]
                        
                        # ê·¹ë‹¨ì  ì €ì  ì´ë²¤íŠ¸
                        low_events = economic_extremes[
                            (economic_extremes['indicator'] == indicator) & 
                            (economic_extremes['event_type'] == 'Extreme Low')
                        ]
                        
                        # ê° ì§€í‘œë³„ ê·¹ë‹¨ ì´ë²¤íŠ¸ íŠ¹ì„± ì¶”ê°€
                        ind_name = indicator.split('_')[0]
                        
                        # ê³ ì  ì´ë²¤íŠ¸ íŠ¹ì„±
                        df[f'{ind_name}_extreme_high'] = df.index.isin(high_events['date']).astype(int)
                        
                        # ì €ì  ì´ë²¤íŠ¸ íŠ¹ì„±
                        df[f'{ind_name}_extreme_low'] = df.index.isin(low_events['date']).astype(int)
            
            # 3. ê¸°ìˆ ì  íŒ¨í„´ íŠ¹ì„± ì¶”ê°€
            if technical_patterns_df is not None and not technical_patterns_df.empty:
                # í•´ë‹¹ ì¹´í…Œê³ ë¦¬ì˜ ê¸°ìˆ ì  íŒ¨í„´
                category_patterns = technical_patterns_df[technical_patterns_df['category'] == category]
                
                if not category_patterns.empty:
                    # ëª¨ë“  ê¸°ìˆ ì  íŒ¨í„´ ë°œìƒ ì—¬ë¶€
                    pattern_dates = category_patterns['date'].unique()
                    df['technical_pattern_event'] = df.index.isin(pattern_dates).astype(int)
                    
                    # íŒ¨í„´ ìœ í˜•ë³„ íŠ¹ì„± ì¶”ê°€
                    pattern_types = [
                        'Golden Cross (5-20)', 'Golden Cross (20-50)',
                        'Death Cross (5-20)', 'Death Cross (20-50)',
                        'Uptrend Reversal', 'Downtrend Reversal',
                        'Support Test', 'Resistance Test'
                    ]
                    
                    for pattern in pattern_types:
                        pattern_specific = category_patterns[category_patterns['pattern'] == pattern]
                        
                        if not pattern_specific.empty:
                            # íŒ¨í„´ ë°œìƒ ì—¬ë¶€
                            pattern_col_name = pattern.replace(' ', '_').replace('(', '').replace(')', '').replace('-', '_').lower()
                            df[f'pattern_{pattern_col_name}'] = df.index.isin(pattern_specific['date']).astype(int)
            
            # 4. ìƒê´€ê´€ê³„ ë¶•ê´´ ì´ë²¤íŠ¸ íŠ¹ì„± ì¶”ê°€
            if correlation_breakdown_df is not None and not correlation_breakdown_df.empty:
                # í•´ë‹¹ ì¹´í…Œê³ ë¦¬ê°€ í¬í•¨ëœ ìƒê´€ê´€ê³„ ë¶•ê´´ ì´ë²¤íŠ¸
                category_correlations = correlation_breakdown_df[correlation_breakdown_df['pair'].str.contains(category)]
                
                if not category_correlations.empty:
                    # ìƒê´€ê´€ê³„ ë¶•ê´´ ì´ë²¤íŠ¸ ë°œìƒ ì—¬ë¶€
                    correlation_dates = category_correlations['date'].unique()
                    df['correlation_breakdown_event'] = df.index.isin(correlation_dates).astype(int)
                    
                    # ìµœê·¼ ìƒê´€ê´€ê³„ ë¶•ê´´ ì´ë²¤íŠ¸ê¹Œì§€ì˜ ê±°ë¦¬
                    df['days_since_correlation_breakdown'] = 999  # ê¸°ë³¸ê°’
                    
                    for date in df.index:
                        date_ts = pd.Timestamp(date) if not isinstance(date, pd.Timestamp) else date
                        
                        prev_breakdowns = []
                        for d in correlation_dates:
                            d_ts = pd.Timestamp(d) if not isinstance(d, pd.Timestamp) else d
                            if d_ts <= date_ts:
                                prev_breakdowns.append(d_ts)
                    
                        if prev_breakdowns:
                            latest_breakdown = max(prev_breakdowns)
                            days_diff = (date_ts - latest_breakdown).days
                            df.at[date, 'days_since_correlation_breakdown'] = days_diff
            
            # 5. ë¨¸ì‹ ëŸ¬ë‹ ì˜ˆì¸¡ ê²°ê³¼ ì¶”ê°€ - ê°œì„ ëœ ë²„ì „
            if predictions_data is not None and category in predictions_data:
                pred_df = predictions_data[category]
                
                if isinstance(pred_df, pd.DataFrame) and 'predictions' in pred_df.columns:
                    # ì˜ˆì¸¡ ì»¬ëŸ¼ì´ ì•„ì§ dfì— ì—†ìœ¼ë©´ ìƒì„±
                    if 'predictions' not in df.columns:
                        df['predictions'] = np.nan
                    
                    # ì¸ë±ìŠ¤ ì²˜ë¦¬ í†µí•© ë¡œì§
                    try:
                        # 1. ë‚ ì§œ í˜•ì‹ ì¸ë±ìŠ¤ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš° - ì§ì ‘ ë§¤í•‘
                        if (isinstance(df.index[0], (pd.Timestamp, np.datetime64, datetime.date)) if len(df.index) > 0 else False) and \
                           (isinstance(pred_df.index[0], (pd.Timestamp, np.datetime64, datetime.date)) if len(pred_df.index) > 0 else False):
                            
                            # ì¸ë±ìŠ¤ ê¸°ë°˜ ì˜ˆì¸¡ê°’ ë³µì‚¬
                            common_indices = set(df.index) & set(pred_df.index)
                            for idx in common_indices:
                                if pd.notna(pred_df.loc[idx, 'predictions']):
                                    df.at[idx, 'predictions'] = pred_df.loc[idx, 'predictions']
                        
                        # 2. ë‚ ì§œ ì»¬ëŸ¼ ê¸°ë°˜ ë³‘í•© í•„ìš”í•œ ê²½ìš°
                        else:
                            # ì¸ë±ìŠ¤ë¥¼ ì»¬ëŸ¼ìœ¼ë¡œ ë³€í™˜
                            df_reset = df.reset_index()
                            pred_reset = pred_df.reset_index()
                            
                            # ë‚ ì§œ ì»¬ëŸ¼ ì°¾ê¸°
                            date_cols = ['date', 'datetime', 'time', 'index', 'timestamp']
                            
                            date_col_model = next((col for col in df_reset.columns if col.lower() in date_cols), df_reset.columns[0])
                            date_col_pred = next((col for col in pred_reset.columns if col.lower() in date_cols), pred_reset.columns[0])
                            
                            # ë‚ ì§œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                            df_reset[date_col_model] = pd.to_datetime(df_reset[date_col_model], errors='coerce').dt.date
                            pred_reset[date_col_pred] = pd.to_datetime(pred_reset[date_col_pred], errors='coerce').dt.date
                            
                            # null ë‚ ì§œ ì œê±°
                            df_reset = df_reset.dropna(subset=[date_col_model])
                            pred_reset = pred_reset.dropna(subset=[date_col_pred])
                            
                            # ì»¬ëŸ¼ ì´ë¦„ì´ ë‹¤ë¥´ë©´ í†µì¼
                            if date_col_pred != date_col_model:
                                pred_reset = pred_reset.rename(columns={date_col_pred: date_col_model})
                            
                            # ë³‘í•©
                            merged_df = pd.merge(
                                df_reset,
                                pred_reset[[date_col_model, 'predictions']],
                                on=date_col_model,
                                how='left',
                                suffixes=('', '_pred')
                            )
                            
                            # ì˜ˆì¸¡ ê°’ í†µí•©
                            if 'predictions_pred' in merged_df.columns:
                                merged_df['predictions'] = merged_df['predictions'].fillna(merged_df['predictions_pred'])
                                merged_df = merged_df.drop('predictions_pred', axis=1)
                            
                            # ì¸ë±ìŠ¤ ë³µì› (ë‚ ì§œë¡œ)
                            merged_df = merged_df.set_index(date_col_model)
                            df = merged_df
                    
                    except Exception as e:
                        st.warning(f"{category}ì˜ ì˜ˆì¸¡ê°’ ë³‘í•© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            
            # í–¥ìƒëœ ëª¨ë¸ ë°ì´í„° ì €ì¥
            extreme_enhanced_model_data[category] = df
            
        except Exception as e:
            st.error(f"{category} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        
        # ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸
        progress_bar.progress((i + 1) / len(categories))
    
    #status_text.text("ëª¨ë¸ ë°ì´í„° í–¥ìƒ ì™„ë£Œ!")
    progress_bar.empty()
    
    return extreme_enhanced_model_data

                            
import pandas as pd
import numpy as np
from datetime import datetime
def generate_executive_summary(extreme_events_df, black_swan_df, technical_patterns_df, correlation_breakdown_df, 
                              daily_avg_prices, daily_indicators, extreme_enhanced_model_data, categories, precalculated_predictions=None):
    """
    ë°ì´í„°ì— ê¸°ë°˜í•œ íˆ¬ì ìš”ì•½ ë³´ê³ ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    
    Parameters:
    -----------
    extreme_events_df : pandas.DataFrame
        ê·¹ë‹¨ì  ì‹œì¥ ì´ë²¤íŠ¸ ë°ì´í„°
    black_swan_df : pandas.DataFrame
        ë¸”ë™ ìŠ¤ì™„ ì´ë²¤íŠ¸ ë°ì´í„°
    technical_patterns_df : pandas.DataFrame
        ê¸°ìˆ ì  íŒ¨í„´ ë°ì´í„°
    correlation_breakdown_df : pandas.DataFrame
        ìƒê´€ê´€ê³„ ë¶•ê´´ ë°ì´í„°
    daily_avg_prices : pandas.DataFrame
        ì¼ë³„ í‰ê·  ê°€ê²© ë°ì´í„°
    daily_indicators : pandas.DataFrame
        ì¼ë³„ ì§€í‘œ ë°ì´í„°
    extreme_enhanced_model_data : dict
        ì¹´í…Œê³ ë¦¬ë³„ ëª¨ë¸ ì˜ˆì¸¡ ë°ì´í„° ë”•ì…”ë„ˆë¦¬
    categories : list
        ë¶„ì„ ì¹´í…Œê³ ë¦¬ ëª©ë¡
    
    Returns:
    --------
    dict
        íˆ¬ì ìš”ì•½ ë³´ê³ ì„œ ë”•ì…”ë„ˆë¦¬
    """
    summary = {}
    
    # ì´ë²¤íŠ¸ ì¹´ìš´íŠ¸ ê³„ì‚°
    extreme_count = len(extreme_events_df) if extreme_events_df is not None and not extreme_events_df.empty else 0
    black_swan_count = len(black_swan_df) if black_swan_df is not None and not black_swan_df.empty else 0
    pattern_count = len(technical_patterns_df) if technical_patterns_df is not None and not technical_patterns_df.empty else 0
    correlation_count = len(correlation_breakdown_df) if correlation_breakdown_df is not None and not correlation_breakdown_df.empty else 0

    total_events = extreme_count + black_swan_count + pattern_count + correlation_count
    
    # ë¦¬ìŠ¤í¬ ë ˆë²¨ ê²°ì •
    if total_events == 0:
        risk_level = "Low Risk"
        risk_score = 1
    elif total_events < 10:
        risk_level = "Caution"
        risk_score = 2
    elif total_events < 30:
        risk_level = "Alert"
        risk_score = 3
    else:
        risk_level = "High Risk"
        risk_score = 4
    
    # ì¹´í…Œê³ ë¦¬ë³„ ì˜í–¥ë„ ë¶„ì„
    category_impact = {}
    
    for category in categories:
        impact_score = 0

        # ë¸”ë™ ìŠ¤ì™„ ì˜í–¥
        if black_swan_df is not None and not black_swan_df.empty and 'category' in black_swan_df.columns:
            category_black_swans = black_swan_df[black_swan_df['category'] == category]
            impact_score += len(category_black_swans) * 3
        
        # ê¸°ìˆ ì  íŒ¨í„´ ì˜í–¥
        if technical_patterns_df is not None and not technical_patterns_df.empty and 'category' in technical_patterns_df.columns:
            category_patterns = technical_patterns_df[technical_patterns_df['category'] == category]
            impact_score += len(category_patterns)
        
        # ê·¹ë‹¨ì  ì´ë²¤íŠ¸ ì˜í–¥
        if extreme_events_df is not None and not extreme_events_df.empty and 'indicator' in extreme_events_df.columns:
            category_extremes = extreme_events_df[extreme_events_df['indicator'].str.contains(category, na=False)]
            impact_score += len(category_extremes) * 2
        
        category_impact[category] = impact_score
    
    # ê°€ì¥ ì˜í–¥ì„ ë§ì´/ì ê²Œ ë°›ì€ ì¹´í…Œê³ ë¦¬ ì‹ë³„
    if category_impact:
        most_impacted = max(category_impact, key=category_impact.get)
        least_impacted = min(category_impact, key=category_impact.get)
    else:
        most_impacted = "N/A"
        least_impacted = "N/A"

    # ì£¼ìš” ë°œê²¬ì‚¬í•­ ë¶„ì„
    key_findings = []
    
    # ë¸”ë™ ìŠ¤ì™„ ì´ë²¤íŠ¸ ë¶„ì„
    if black_swan_df is not None and not black_swan_df.empty:
        try:
            recent_black_swan = black_swan_df.sort_values('date', ascending=False).iloc[0]
            date_str = recent_black_swan['date'].strftime('%Y-%m-%d') if pd.notnull(recent_black_swan['date']) else "N/A"
            key_findings.append(
                f"ğŸ”¥ Latest black swan event occurred in {recent_black_swan.get('category', 'N/A')} as a "
                f"{recent_black_swan.get('event_type', 'N/A')} on ({date_str})"
            )
        except Exception:
            pass
    
    # ê¸°ìˆ ì  íŒ¨í„´ ë¶„ì„
    if technical_patterns_df is not None and not technical_patterns_df.empty and 'pattern' in technical_patterns_df.columns:
        try:
            if len(technical_patterns_df['pattern'].value_counts()) > 0:
                most_common_pattern = technical_patterns_df['pattern'].mode()[0]
                pattern_count = len(technical_patterns_df[technical_patterns_df['pattern'] == most_common_pattern])
                key_findings.append(
                    f"ğŸ“Š Most common technical pattern: {most_common_pattern} ({pattern_count} times)"
                )
        except Exception:
            pass
    
    # ìƒê´€ê´€ê³„ ë¶•ê´´ ë¶„ì„
    if (correlation_breakdown_df is not None and 
        not correlation_breakdown_df.empty and 
        'change' in correlation_breakdown_df.columns):
        try:
            non_null_change = correlation_breakdown_df['change'].dropna()
            if not non_null_change.empty:
                largest_breakdown_idx = non_null_change.abs().idxmax()
                largest_breakdown = correlation_breakdown_df.loc[largest_breakdown_idx]

                pair = largest_breakdown.get('pair', 'N/A')
                old_corr = largest_breakdown.get('old_correlation', None)
                new_corr = largest_breakdown.get('new_correlation', None)

                if old_corr is not None and new_corr is not None:
                    key_findings.append(
                        f"ğŸ’” The largest Correlation Breakedown: {pair} "
                        f"({old_corr:.2f} â†’ {new_corr:.2f})"
                    )
        except Exception:
            pass

    # ê·¹ë‹¨ì  ì´ë²¤íŠ¸ ë¶„ì„
    if extreme_events_df is not None and not extreme_events_df.empty and 'indicator' in extreme_events_df.columns:
        try:
            economic_indicators = extreme_events_df[extreme_events_df['indicator'].str.contains('_norm', na=False)]
            indicator_counts = economic_indicators['indicator'].value_counts()
            if len(indicator_counts) > 0:
                most_volatile_indicator = indicator_counts.index[0]
                count = indicator_counts.iloc[0]
                key_findings.append(
                    f"âš¡ The most volatile economic indicator is {most_volatile_indicator.replace('_norm', '')}, with {count} extreme events observed."
                )
        except Exception:
            pass
    
    # ìµœê·¼ 30ì¼ ë³€ë™ì„± ë¶„ì„
    if daily_avg_prices is not None and not daily_avg_prices.empty:
        try:
            # ìµœê·¼ 30ì¼ ë°ì´í„° ì¶”ì¶œ
            if isinstance(daily_avg_prices.index, pd.DatetimeIndex):
                last_30_days = daily_avg_prices.last('30D')
            else:
                # ë‚ ì§œ ì¸ë±ìŠ¤ê°€ ì•„ë‹Œ ê²½ìš° ë§ˆì§€ë§‰ 30ê°œ í–‰ ì‚¬ìš©
                last_30_days = daily_avg_prices.iloc[-30:]
                
            if not last_30_days.empty:
                volatility_30d = last_30_days.pct_change().std() * np.sqrt(252)  # ì—°ìœ¨í™”ëœ ë³€ë™ì„±
                if not volatility_30d.isnull().all():
                    most_volatile_category = volatility_30d.idxmax()
                    volatility_value = volatility_30d.max()
                    key_findings.append(
                        f"ğŸ“ˆ Category with the highest 30-day volatility: {most_volatile_category} (Annualized Volatility: {volatility_value:.1%})"
                    )
        except Exception:
            pass
    
    # ëª¨ë¸ ì˜ˆì¸¡ ë¶„ì„
    if precalculated_predictions and isinstance(precalculated_predictions, dict) and 'bullish' in precalculated_predictions and 'bearish' in precalculated_predictions:
        bullish_categories = precalculated_predictions['bullish']
        bearish_categories = precalculated_predictions['bearish']
        prediction_details = precalculated_predictions.get('details', {})
    else:
        # ìì²´ ì˜ˆì¸¡ ë¶„ì„ ì½”ë“œ
        bullish_categories = []
        bearish_categories = []
        prediction_details = {}
        
        if extreme_enhanced_model_data is not None and isinstance(extreme_enhanced_model_data, dict):
            try:
                for category, data in extreme_enhanced_model_data.items():
                    if isinstance(data, pd.DataFrame) and 'predictions' in data.columns:
                        valid_predictions = data['predictions'].dropna()
                        
                        if len(valid_predictions) > 0:
                            # ìµœê·¼ 50ê°œ ì˜ˆì¸¡ ë˜ëŠ” ê°€ëŠ¥í•œ ìµœëŒ€ ìˆ˜
                            recent_predictions = valid_predictions.iloc[-min(50, len(valid_predictions)):]
                            up_ratio = (recent_predictions == 1).mean()
                            
                            # ì˜ˆì¸¡ ì„¸ë¶€ ì •ë³´ ì €ì¥
                            prediction_details[category] = {
                                'samples': len(valid_predictions),
                                'bullish_ratio': float(up_ratio),
                                'direction': 'bullish' if up_ratio >= 0.49 else 'bearish' if up_ratio <= 0.45 else 'neutral'
                            }
                            
                            # ë¶„ë¥˜ ê¸°ì¤€ ì™„í™” (55%+ ìƒìŠ¹ = bullish, 45%- ìƒìŠ¹ = bearish)
                            if up_ratio >= 0.49:
                                bullish_categories.append(category)
                            elif up_ratio <= 0.45:
                                bearish_categories.append(category)
            except Exception:
                pass
    
    # ëª¨ë¸ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ key_findingsì— ì¶”ê°€
    if bullish_categories:
        key_findings.append(f"ğŸš€ Bullish forecast: {', '.join(bullish_categories)}")
    if bearish_categories:
        key_findings.append(f"âš ï¸ Bearish forecast: {', '.join(bearish_categories)}")
    
    # í˜„ì¬ ì‹œì¥ ê°ì„± ë¶„ì„
    current_sentiment = None
    if daily_indicators is not None and not daily_indicators.empty:
        try:
            sentiment_cols = [col for col in daily_indicators.columns if 'sentiment' in col or 'fear_greed' in col]
            if sentiment_cols and len(daily_indicators) > 0:
                latest_sentiment = daily_indicators[sentiment_cols].iloc[-1].mean()
                if pd.notnull(latest_sentiment):
                    if latest_sentiment > 0.6:
                        current_sentiment = "Very Positive"
                    elif latest_sentiment > 0.4:
                        current_sentiment = "Positive"
                    elif latest_sentiment > 0.3:
                        current_sentiment = "Neutral"
                    else:
                        current_sentiment = "Negative"
        except Exception:
            pass
    
    # íˆ¬ì ì œì•ˆì‚¬í•­ ìƒì„±      
    investment_suggestions = []
    
    # ë¦¬ìŠ¤í¬ ìˆ˜ì¤€ì— ë”°ë¥¸ ì œì•ˆ
    if risk_score == 1:
        investment_suggestions.extend([
            "âœ… The current market is stable. Consider maintaining your existing positions.",
            "ğŸ“ˆ Focus on sectors with high growth potential."
        ])
    elif risk_score == 2:
        investment_suggestions.extend([
            "âš ï¸ Caution signals detected in the market. Strengthen your risk management strategies.",
            "ğŸ›¡ï¸ Consider building hedge positions.",
            f"ğŸ“‰ Review your exposure to the {most_impacted} sector."
        ])
    elif risk_score == 3:
        investment_suggestions.extend([
            "âš ï¸âš ï¸ Market is in an alert state. A more conservative approach is recommended.",
            "ğŸ’° Consider increasing cash holdings.",
            f"ğŸ” Reduce exposure to the {most_impacted} sector and consider rotating into {least_impacted}."
        ])
    else:  # risk_score == 4
        investment_suggestions.extend([
            "ğŸš¨ The market is at high risk. Extreme caution is advised.",
            "ğŸ¦ Consider shifting into safer assets.",
            f"âŒ It is recommended to avoid the {most_impacted} sector.",
            "ğŸ“Š Plan to re-enter the market after stabilization."
        ])
    
    # ì‹œì¥ ê°ì„± ì •ë³´ ì¶”ê°€
    if current_sentiment:
        extra_caution = " â€” Additional caution advised." if risk_score > 2 else ""
        investment_suggestions.append(f"ğŸ¯ Current market sentiment: {current_sentiment}{extra_caution}")
    
    # ëª¨ë¸ ê¸°ë°˜ ì¶”ê°€ ì œì•ˆ
    if bullish_categories:
        investment_suggestions.append(f"ğŸ¯ AI model suggests bullish trend in: {', '.join(bullish_categories)}")

    if bearish_categories:
        investment_suggestions.append(f"âš ï¸ AI model flags bearish trend in: {', '.join(bearish_categories)}")
    
    # ì¶”ê°€ ê²½ê³ 
    if black_swan_count > 3:
        investment_suggestions.append("ğŸš¨ Multiple black swan events detected recently. Extreme caution is recommended.")
    
    # ìµœì¢… ìš”ì•½ ìƒì„±
    summary = {
        'risk_level': risk_level,
        'risk_score': risk_score,
        'total_events': total_events,
        'most_impacted': most_impacted,
        'least_impacted': least_impacted,
        'key_findings': key_findings,
        'investment_suggestions': investment_suggestions,
        'event_counts': {
            'black_swan': black_swan_count,
            'extreme': extreme_count,
            'pattern': pattern_count,
            'correlation': correlation_count
        },
        'current_sentiment': current_sentiment,
        'model_predictions': {
            'bullish': bullish_categories,
            'bearish': bearish_categories,
            'details': prediction_details  # ì˜ˆì¸¡ ì„¸ë¶€ ì •ë³´ ì¶”ê°€
        }
    }

    return summary

import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib.dates as mdates
import traceback
from matplotlib.patches import Patch

def fix_feature_importance(model_results):
    """
    Fixes and standardizes feature importance data in model results.
    Instead of skipping invalid entries, this function converts or repairs them.
    
    Parameters:
    -----------
    model_results : dict
        Original model results data
        
    Returns:
    --------
    dict
        Model results with standardized feature importance data
    """
    if not isinstance(model_results, dict):
        return {}
    
    fixed_results = {}
    
    for category, result in model_results.items():
        if not result or not isinstance(result, dict):
            continue
            
        # Copy the result to avoid modifying original
        fixed_result = result.copy()
        
        # Fix feature_importance if it exists
        if 'feature_importance' in fixed_result:
            feature_importance = fixed_result['feature_importance']
            
            # Case 1: If feature_importance is a float/number (common error case)
            if isinstance(feature_importance, (float, int, np.float64, np.int64)):
                # Convert to a dictionary with a single "overall" feature
                fixed_result['feature_importance'] = {
                    "overall_importance": float(feature_importance)
                }
            
            # Case 2: If feature_importance is None or empty
            elif feature_importance is None or feature_importance == {}:
                # Create an empty dictionary
                fixed_result['feature_importance'] = {}
            
            # Case 3: If feature_importance is already a dictionary, validate its values
            elif isinstance(feature_importance, dict):
                # Validate and convert all values to float
                valid_features = {}
                for k, v in feature_importance.items():
                    try:
                        value = float(v)
                        if pd.notna(value) and np.isfinite(value):
                            valid_features[k] = value
                    except (ValueError, TypeError):
                        # Skip this feature if value can't be converted to float
                        pass
                        
                fixed_result['feature_importance'] = valid_features
            
            # Case 4: Any other type, create an empty dictionary
            else:
                fixed_result['feature_importance'] = {}
                
        # If feature_importance doesn't exist, add an empty one
        else:
            fixed_result['feature_importance'] = {}
            
        fixed_results[category] = fixed_result
    
    return fixed_results

def is_valid_dataframe(df):
    """Checks if df is a valid DataFrame"""
    return isinstance(df, pd.DataFrame) and not df.empty

def classify_feature_type(feature_name):
    """Classifies feature type based on feature name"""
    if any(eco in feature_name for eco in ["GDP", "CPI", "Dollar", "Fed", "Interest", "Industrial", 
                                        "Treasury", "WTI", "Natural_Gas", "Consumer", "Gov_Spending"]):
        return "Economic"
    elif any(sent in feature_name for sent in ["sentiment", "fear", "greed", "positive", "negative", "neutral"]):
        return "Sentiment"
    elif any(tech in feature_name for tech in ["lag", "rolling", "mean", "MA", "RSI", "MACD", "volatility", 
                                            "change", "acceleration", "cross", "std"]):
        return "Technical"
    elif "corr" in feature_name:
        return "Correlation"
    else:
        return "Other"

def calculate_feature_type_importance(features):
    """Calculates importance by feature type"""
    if not isinstance(features, dict):
        return {}
    
    feature_types = {
        "Economic": [],
        "Sentiment": [],
        "Technical": [],
        "Correlation": [],
        "Other": []
    }
    
    # Classify each feature by type
    for feature in features.keys():
        feature_type = classify_feature_type(feature)
        feature_types[feature_type].append(feature)
    
    # Calculate sum of importance by type
    type_importance = {
        feature_type: sum(features.get(f, 0) for f in feature_list)
        for feature_type, feature_list in feature_types.items()
    }
    
    return type_importance

# --- ë©”ì¸ ëŒ€ì‹œë³´ë“œ í•¨ìˆ˜ ---


def generate_focused_ml_insights(model_results, model_data, daily_avg_prices, daily_indicators):
    """
    ìƒì„±ëœ ML ëª¨ë¸ì˜ í†µí•© ì¸ì‚¬ì´íŠ¸ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤.
    ê° ì¹´í…Œê³ ë¦¬ë³„ íƒ­ì„ ìƒì„±í•˜ê³  ëª¨ë¸ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.
    
    Args:
        model_results: ì¹´í…Œê³ ë¦¬ë³„ ëª¨ë¸ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        model_data: ëª¨ë¸ì´ í•™ìŠµí•œ ë°ì´í„°
        daily_avg_prices: ì¼ë³„ í‰ê·  ê°€ê²© ë°ì´í„°
        daily_indicators: ì¼ë³„ ì§€í‘œ ë°ì´í„°
    """
    st.header("ğŸ¤– Machine Learning Model Results")
    
    # ì„¸ì…˜ì— ì¹´í…Œê³ ë¦¬ë³„ ë¶„ì„ ê²°ê³¼ ì €ì¥
    if 'category_analyses' not in st.session_state:
        st.session_state['category_analyses'] = {}
    
    # ì „ì²´ ì„±ëŠ¥ ë°ì´í„°í”„ë ˆì„ ìƒì„±
    performance_data = []
    for category, result in model_results.items():
        if result:  # ê²°ê³¼ê°€ ìˆëŠ” ê²½ìš°ë§Œ ì²˜ë¦¬
            # ìµœê·¼ ì˜ˆì¸¡ í™•ì¸ - model_dataì—ì„œ í•´ë‹¹ ì¹´í…Œê³ ë¦¬ì˜ predictions í™•ì¸
            latest_prediction = "Unknown"
            if category in model_data and 'predictions' in model_data[category].columns:
                recent_preds = model_data[category]['predictions'].dropna()
                if not recent_preds.empty:
                    latest_pred_val = recent_preds.iloc[-1]
                    latest_prediction = "Up" if latest_pred_val == 1 else "Down"
            
            performance_data.append({
                "Category": category,
                "Model Type": result.get('model_name', 'Unknown'),
                "Accuracy": result.get('accuracy', 0),
                "Optimal Threshold Accuracy": result.get('accuracy_optimal', 0),
                "ROC-AUC": result.get('roc_auc', 0),
                "Prediction Confidence": result.get('roc_auc', 0) * 0.5 + result.get('accuracy_optimal', 0) * 0.5,
                "Latest Prediction": latest_prediction
            })
            
            # ê²°ê³¼ë¥¼ ì„¸ì…˜ì— ì €ì¥
            st.session_state['category_analyses'][category] = {
                'model_result': result,
                'performance': {
                    "Model Type": result.get('model_name', 'Unknown'),
                    "Accuracy": result.get('accuracy', 0),
                    "Optimal Threshold Accuracy": result.get('accuracy_optimal', 0),
                    "ROC-AUC": result.get('roc_auc', 0),
                    "Optimal Threshold": result.get('optimal_threshold', 0.5)
                }
            }
    
    # ì„±ëŠ¥ ë°ì´í„°í”„ë ˆì„ ìƒì„± ë° ì„¸ì…˜ ì €ì¥
    performance_df = pd.DataFrame(performance_data)
    st.session_state['full_performance'] = performance_df
    
    # ì„±ëŠ¥ ìš”ì•½ í‘œì‹œ
    st.subheader("ğŸ“Š Model Performance Overview")
    if not performance_df.empty:
        st.dataframe(performance_df.style.format({
            "Accuracy": "{:.2%}",
            "Optimal Threshold Accuracy": "{:.2%}",
            "ROC-AUC": "{:.2%}",
            "Prediction Confidence": "{:.2%}"
        }))
        
        # ì„±ëŠ¥ ë¹„êµ ì°¨íŠ¸
        st.subheader("ğŸ“ˆ Model Performance Comparison")
        perf_fig = create_performance_comparison_chart(performance_df)
        st.plotly_chart(perf_fig, use_container_width=True)
    else:
        st.info("No model performance data available yet. Run the analysis first.")
    
    # ì¹´í…Œê³ ë¦¬ë³„ íƒ­ ìƒì„±
    categories = list(model_results.keys())
    if categories:
        tabs = st.tabs([f"ğŸ“Š {category}" for category in categories])
        
        # ê° ì¹´í…Œê³ ë¦¬ë³„ íƒ­ ë‚´ìš© ì„¤ì •
        for i, category in enumerate(categories):
            with tabs[i]:
                if category in model_results and model_results[category]:
                    result = model_results[category]
                    
                    # ëª¨ë¸ ì •ë³´ ë° ì„±ëŠ¥ ì§€í‘œ
                    st.subheader(f"{category} Category Analysis")
                    col1, col2, col3 = st.columns(3)
                    with col1:
                        st.metric("Model Type", result.get('model_name', 'Unknown'))
                    with col2:
                        st.metric("Accuracy", f"{result.get('accuracy', 0):.2%}")
                    with col3:
                        st.metric("ROC-AUC", f"{result.get('roc_auc', 0):.2%}")
                    
                    # ì‹œê°í™” ì„¹ì…˜
                    st.subheader("ğŸ“ˆ Model Visualizations")
                    
                    # ì‹œê°í™” í‘œì‹œë¥¼ ìœ„í•œ ê·¸ë¦¬ë“œ
                    col1, col2 = st.columns(2)
                    
                    # ì‹œê°í™” ë°ì´í„° ì ‘ê·¼ ë° í‘œì‹œ
                    with col1:
                        # íŠ¹ì„± ì¤‘ìš”ë„
                        st.markdown("### ğŸ¯ Feature Importance")
                        if 'visualizations' in st.session_state and f"{category}_feature_importance" in st.session_state['visualizations']:
                            st.pyplot(st.session_state['visualizations'][f"{category}_feature_importance"])
                        else:
                            # íŒŒì¼ì—ì„œ ì´ë¯¸ì§€ ë¡œë“œ ì‹œë„
                            feature_img_path = os.path.join("ML_results", 'categories', category, 'feature_importance.png')
                            if os.path.exists(feature_img_path):
                                st.image(feature_img_path)
                            else:
                                st.info("Feature importance visualization not available")
                        
                        # ROC ê³¡ì„ 
                        st.markdown("### ğŸ“ˆ ROC Curve")
                        if 'visualizations' in st.session_state and f"{category}roc_curve" in st.session_state['visualizations']:
                            st.pyplot(st.session_state['visualizations'][f"{category}roc_curve"])
                        else:
                            # íŒŒì¼ì—ì„œ ì´ë¯¸ì§€ ë¡œë“œ ì‹œë„
                            roc_img_path = os.path.join("ML_results", 'categories', category, 'roc_curve.png')
                            if os.path.exists(roc_img_path):
                                st.image(roc_img_path)
                            else:
                                st.info("ROC curve visualization not available")
                    
                    with col2:
                        # í˜¼ë™ í–‰ë ¬
                        st.markdown("### ğŸ§© Confusion Matrix")
                        if 'visualizations' in st.session_state and f"{category}_Confusion Matrix" in st.session_state['visualizations']:
                            st.pyplot(st.session_state['visualizations'][f"{category}_Confusion Matrix"])
                        else:
                            # íŒŒì¼ì—ì„œ ì´ë¯¸ì§€ ë¡œë“œ ì‹œë„
                            cm_img_path = os.path.join("ML_results", 'categories', category, 'confusion_matrix.png')
                            if os.path.exists(cm_img_path):
                                st.image(cm_img_path)
                            else:
                                st.info("Confusion matrix visualization not available")
                        
                        # í™•ë¥  ë¶„í¬
                        st.markdown("### ğŸ§® Prediction Probability")
                        if 'visualizations' in st.session_state and f"{category}probability_distribution" in st.session_state['visualizations']:
                            st.pyplot(st.session_state['visualizations'][f"{category}probability_distribution"])
                        else:
                            # íŒŒì¼ì—ì„œ ì´ë¯¸ì§€ ë¡œë“œ ì‹œë„
                            prob_img_path = os.path.join("ML_results", 'categories', category, 'probability_distribution.png')
                            if os.path.exists(prob_img_path):
                                st.image(prob_img_path)
                            else:
                                st.info("Probability distribution visualization not available")
                    
                    # ì‹œê°„ì  ì˜ˆì¸¡ íŒ¨í„´
                    st.markdown("### â³ Temporal Prediction Trend")
                    if 'visualizations' in st.session_state and f"{category}temporal_prediction" in st.session_state['visualizations']:
                        st.pyplot(st.session_state['visualizations'][f"{category}temporal_prediction"])
                    else:
                        # íŒŒì¼ì—ì„œ ì´ë¯¸ì§€ ë¡œë“œ ì‹œë„
                        temporal_img_path = os.path.join("ML_results", 'categories', category, 'temporal_prediction.png')
                        if os.path.exists(temporal_img_path):
                            st.image(temporal_img_path)
                        else:
                            st.info("Temporal prediction visualization not available")
                    
                    # ëª¨ë¸ ì„±ëŠ¥ ë¶„ì„ ë° ì¸ì‚¬ì´íŠ¸
                    st.subheader("ğŸ” Performance Analysis & Insights")
                    
                    # ì¤‘ìš” íŠ¹ì„± ë¶„ì„
                    if 'feature_importance' in result and result['feature_importance']:
                        feature_importance = pd.Series(result['feature_importance'])
                        top_features = feature_importance.nlargest(min(5, len(feature_importance)))
                        
                        # ì„¸ì…˜ì— ì¤‘ìš” íŠ¹ì„± ì €ì¥
                        if category in st.session_state['category_analyses']:
                            st.session_state['category_analyses'][category]['top_features'] = top_features.to_dict()
                        
                        st.write("#### ğŸ”‘ Key Driving Factors")
                        st.write(f"The most important factors for predicting {category} price movement are:")
                        
                        # ì¤‘ìš” íŠ¹ì„± í•´ì„
                        st.write("#### ğŸ’¡ Interpretation")
                        st.write(f"The model for {category} relies heavily on:")
                        for i, (feature, importance) in enumerate(top_features.items()):
                            st.write(f"{i+1}. **{feature}** (Importance: {importance:.3f})")
                        
                        # ëª¨ë¸ ì„±ëŠ¥ ë° ì‹ ë¢°ë„
                        st.write("#### ğŸ¯ Model Reliability")
                        confidence = result.get('roc_auc', 0) * 0.5 + result.get('accuracy_optimal', 0) * 0.5
                        
                        # ì„¸ì…˜ì— ì‹ ë¢°ë„ ì €ì¥
                        if category in st.session_state['category_analyses']:
                            st.session_state['category_analyses'][category]['reliability'] = {
                                'score': float(confidence),
                                'level': 'High' if confidence >= 0.8 else ('Moderate' if confidence >= 0.65 else 'Low')
                            }
                        
                        if confidence >= 0.8:
                            reliability = "High"
                            color = "green"
                        elif confidence >= 0.65:
                            reliability = "Moderate"
                            color = "orange"
                        else:
                            reliability = "Low"
                            color = "red"
                        
                        st.markdown(f"Model reliability: <span style='color:{color};font-weight:bold'>{reliability}</span> ({confidence:.2%})", unsafe_allow_html=True)
                        
                        # ìµœì  ì„ê³„ê°’ ì •ë³´
                        st.write(f"The optimal threshold for predictions is **{result.get('optimal_threshold', 0.5):.3f}** " +
                                f"(vs. standard 0.5), improving accuracy from {result.get('accuracy', 0):.2%} to {result.get('accuracy_optimal', 0):.2%}.")
                else:
                    st.warning(f"No model results available for {category}")
        
        # ì˜ˆì¸¡ ê²°ê³¼ í‘œì‹œ
        st.subheader("ğŸ”® Predictions Summary")
        if not performance_df.empty:
            # ì˜ˆì¸¡ ê²°ê³¼ í…Œì´ë¸”
            st.dataframe(
                performance_df[['Category', 'Latest Prediction', 'Prediction Confidence']],
                column_config={
                    "Category": st.column_config.TextColumn("Category"),
                    "Latest Prediction": st.column_config.TextColumn("Latest Prediction", help="Most recent prediction (Up/Down)"),
                    "Prediction Confidence": st.column_config.ProgressColumn(
                        "Prediction Confidence",
                        format="%.1f%%",
                        min_value=0,
                        max_value=1,
                    )
                }
            )
            
            # ì˜ˆì¸¡ íŒ¨ë„ ìƒì„±
            st.markdown("### ğŸ¯ Current Predictions")
            prediction_cols = st.columns(min(4, len(categories)))
            
            for i, category in enumerate(categories[:min(4, len(categories))]):
                with prediction_cols[i % len(prediction_cols)]:
                    if category in performance_df['Category'].values:
                        pred_row = performance_df[performance_df['Category'] == category].iloc[0]
                        pred_text = pred_row['Latest Prediction']
                        conf_value = pred_row['Prediction Confidence']
                        
                        # ì˜ˆì¸¡ ë°©í–¥ì— ë”°ë¥¸ ìƒ‰ìƒ ë° ì•„ì´ì½˜
                        if pred_text == "Up":
                            color = "green"
                            icon = "ğŸ“ˆ"
                        elif pred_text == "Down":
                            color = "red"
                            icon = "ğŸ“‰"
                        else:
                            color = "gray"
                            icon = "â“"
                        
                        # ì¹´ë“œ í˜•íƒœë¡œ í‘œì‹œ
                        st.markdown(f"""
                        <div style="padding: 15px; border-radius: 5px; background-color: {color}0F; border: 1px solid {color};">
                            <h3 style="color: {color}; margin-top: 0;">{category} {icon}</h3>
                            <p style="font-size: 24px; font-weight: bold; color: {color}; margin: 5px 0;">{pred_text}</p>
                            <p style="color: #666; margin-bottom: 0;">Confidence: {conf_value:.1%}</p>
                        </div>
                        """, unsafe_allow_html=True)
    else:
        st.info("No model results available yet. Run the analysis first.")
    
    # í˜„ì¬ ì„ íƒëœ ì¹´í…Œê³ ë¦¬ ì—…ë°ì´íŠ¸
    st.session_state['current_category'] = categories[0] if categories else None


# --- ì¹´í…Œê³ ë¦¬ë³„ ë¶„ì„ í•¨ìˆ˜ ---
def show_category_analysis(filtered_df, fixed_model_results, cat_name, model_data, daily_avg_prices, daily_indicators):
    try:
        result = fixed_model_results.get(cat_name)
        if not result:
            st.warning("No model result available.")
            return

        st.metric("Accuracy", f"{filtered_df.iloc[0]['Accuracy']:.2%}")
        st.metric("ROC-AUC", f"{filtered_df.iloc[0]['ROC-AUC']:.2%}")
        st.metric("Prediction Confidence", f"{filtered_df.iloc[0]['Prediction Confidence']:.2%}")

        features = result.get('feature_importance', {})
        if features:
            top_features = dict(sorted(features.items(), key=lambda x: x[1], reverse=True)[:10])
            feature_df = pd.DataFrame({"Feature": list(top_features.keys()), "Importance": list(top_features.values())})
            fig, ax = plt.subplots(figsize=(8, 6))
            ax.barh(feature_df['Feature'], feature_df['Importance'], color="skyblue")
            ax.invert_yaxis()
            ax.set_xlabel("Importance")
            ax.set_title(f"Top 10 Important Features: {cat_name}")
            st.pyplot(fig)
            plt.close()

            feature_type_importance = calculate_feature_type_importance(features)
            non_zero_types = {k: v for k, v in feature_type_importance.items() if v > 0}
            if non_zero_types:
                fig, ax = plt.subplots(figsize=(6, 6))
                wedges, texts, autotexts = ax.pie(
                    non_zero_types.values(),
                    labels=non_zero_types.keys(),
                    autopct="%1.1f%%",
                    startangle=90,
                    colors=["salmon", "lightgreen", "lightblue", "purple", "gray"]
                )
                ax.set_title(f"Feature Type Distribution: {cat_name}")
                plt.tight_layout()
                st.pyplot(fig)
                plt.close()

            top3_features = dict(sorted(features.items(), key=lambda x: x[1], reverse=True)[:3])
            for feature_name in top3_features:
                feature_series = None
                price_series = None
                
                # Try to find feature in daily_indicators
                if daily_indicators is not None and feature_name in daily_indicators.columns:
                    feature_series = daily_indicators[feature_name]
                
                # Try to find price series in daily_avg_prices
                if daily_avg_prices is not None and cat_name in daily_avg_prices.columns:
                    price_series = daily_avg_prices[cat_name]
                
                if feature_series is not None and price_series is not None:
                    common_index = feature_series.index.intersection(price_series.index)
                    if len(common_index) > 30:
                        fig, ax1 = plt.subplots(figsize=(10, 5))
                        ax1.set_title(f"{feature_name} vs {cat_name} Price")
                        ax1.plot(feature_series[common_index], color='blue', label=feature_name)
                        ax1.set_ylabel(feature_name, color='blue')
                        ax2 = ax1.twinx()
                        ax2.plot(price_series[common_index], color='red', label=f"{cat_name} Price")
                        ax2.set_ylabel(f"{cat_name} Price", color='red')
                        fig.autofmt_xdate()
                        st.pyplot(fig)
                        plt.close()
    except Exception as e:
        st.error(f"Error analyzing {cat_name}: {e}")
        st.error(traceback.format_exc())

# --- Investment Summary ---
def show_investment_and_summary(performance_df):
    try:
        if performance_df.empty:
            st.info("No performance data for investment insights.")
            return
        
        st.subheader("Model-Based Investment Strategy Insights")
        high_conf = performance_df[performance_df['Prediction Confidence'] >= 0.6]
        med_conf = performance_df[(performance_df['Prediction Confidence'] >= 0.5) & (performance_df['Prediction Confidence'] < 0.6)]

        if not high_conf.empty or not med_conf.empty:
            st.write("### ğŸ“ˆ Investment Portfolio Suggestions Based on Confidence")
            col1, col2 = st.columns(2)

            with col1:
                st.markdown("**High Confidence Categories (Over 60%):**")
                if not high_conf.empty:
                    for _, row in high_conf.iterrows():
                        cat = row['Category']
                        prediction = row['Latest Prediction']
                        arrow = "â¬†ï¸" if prediction == "Up" else "â¬‡ï¸" if prediction == "Down" else "â¡ï¸"
                        st.markdown(f"- **{cat}** {arrow}")
                else: 
                    st.info("No high confidence categories.")

                st.markdown("**Medium Confidence Categories (50-60%):**")
                if not med_conf.empty:
                    for _, row in med_conf.iterrows():
                        cat = row['Category']
                        prediction = row['Latest Prediction']
                        arrow = "â¬†ï¸" if prediction == "Up" else "â¬‡ï¸" if prediction == "Down" else "â¡ï¸"
                        st.markdown(f"- **{cat}** {arrow}")
                else:
                    st.info("No medium confidence categories.")
            
            with col2:
                # 1. Fixed_model_results preparation
                if 'fixed_model_results' not in st.session_state:
                    if 'model_results' in st.session_state:
                        fixed_model_results = fix_feature_importance(st.session_state['model_results'])
                        st.session_state['fixed_model_results'] = fixed_model_results
                    else:
                        st.error("No model results available.")
                        return
                else:
                    fixed_model_results = st.session_state['fixed_model_results']

                # 3. Create cleaned_type_importance
                if 'cleaned_type_importance' not in st.session_state:
                    cleaned_type_importance = {}
                    for category, result in fixed_model_results.items():
                        features = result.get('feature_importance', {})
                        if features:
                            raw_importance = calculate_feature_type_importance(features)
                            cleaned_importance = {
                                "Economic": raw_importance.get("Economic", 0),
                                "Sentiment": raw_importance.get("Sentiment", 0),
                                "Technical": raw_importance.get("Technical", 0),
                                "Correlation": raw_importance.get("Correlation", 0),
                                "Other": raw_importance.get("Other", 0)
                            }
                            cleaned_type_importance[category] = cleaned_importance
                    st.session_state['cleaned_type_importance'] = cleaned_type_importance
                else:
                    cleaned_type_importance = st.session_state['cleaned_type_importance']

                if cleaned_type_importance:
                    st.markdown("**Prediction Factor-Based Strategies:**")
                    
                    # Economic indicator focus
                    econ_cats = sorted([(cat, imp.get("Economic", 0)) for cat, imp in cleaned_type_importance.items()],
                                       key=lambda x: x[1], reverse=True)[:3]
                    if econ_cats:
                        st.markdown("**Economic Indicator Monitoring:**")
                        for cat, _ in econ_cats:
                            st.markdown(f"- **{cat}**")

                    # Sentiment indicator focus
                    sent_cats = sorted([(cat, imp.get("Sentiment", 0)) for cat, imp in cleaned_type_importance.items()],
                                       key=lambda x: x[1], reverse=True)[:3]
                    if sent_cats:
                        st.markdown("**Market Sentiment Monitoring:**")
                        for cat, _ in sent_cats:
                            st.markdown(f"- **{cat}**")

                    # Technical indicator focus
                    tech_cats = sorted([(cat, imp.get("Technical", 0)) for cat, imp in cleaned_type_importance.items()],
                                       key=lambda x: x[1], reverse=True)[:3]
                    if tech_cats:
                        st.markdown("**Technical Signal Monitoring:**")
                        for cat, _ in tech_cats:
                            st.markdown(f"- **{cat}**")
                else:
                    st.info("Insufficient feature type importance data.")

        # --- ì‹œë‚˜ë¦¬ì˜¤ ì œì•ˆ ---
        st.write("### ğŸ¯ Machine Learning-Based Investment Scenarios")
        scenarios = []

        # Find high confidence up and down predictions
        high_conf_up = performance_df[(performance_df['Prediction Confidence'] >= 0.6) & 
                                     (performance_df['Latest Prediction'] == "Up")]
        high_conf_down = performance_df[(performance_df['Prediction Confidence'] >= 0.6) & 
                                       (performance_df['Latest Prediction'] == "Down")]

        if not high_conf_up.empty:
            scenarios.append({
                "Title": "Upward Momentum Scenario",
                "Strategy": f"Focus investment on high confidence upward categories: {', '.join(high_conf_up['Category'])}",
                "Risk Level": "Medium",
                "Suitable For": "Aggressive investors"
            })

        if not high_conf_down.empty:
            scenarios.append({
                "Title": "Downside Protection Scenario",
                "Strategy": f"Exclude or hedge categories with high confidence downward predictions: {', '.join(high_conf_down['Category'])}",
                "Risk Level": "Low",
                "Suitable For": "Conservative investors"
            })

        if not scenarios:
            st.info("No clear investment scenarios based on current predictions.")
        else:
            for i, sc in enumerate(scenarios, 1):
                st.markdown(f"**Scenario {i}: {sc['Title']}**")
                st.markdown(f"- **Strategy**: {sc['Strategy']}")
                st.markdown(f"- **Risk Level**: {sc['Risk Level']}")
                st.markdown(f"- **Suitable For**: {sc['Suitable For']}")
                st.write("---")

        # --- Risk warnings ---
        st.write("### âš ï¸ Risk Considerations")
        st.markdown("""
        1. **Model Confidence Limitations**
           - Historical data-based models may not adapt perfectly to future market shifts.
           - Even high confidence predictions can be wrong.

        2. **Feature Dependency Risks**
           - Over-reliance on a specific type of indicator (e.g., economic or sentiment) increases vulnerability.
           - Diversified feature importance across types offers more stability.

        3. **Market Environment Changes**
           - Macro-economic changes, policy shifts, or black swan events can override model signals.
           - Regular model retraining and monitoring are essential.
        """)

        # --- Final Executive Summary ---
        st.subheader("Executive Summary")
        avg_accuracy = performance_df['Accuracy'].mean()
        avg_roc_auc = performance_df['ROC-AUC'].mean()
        avg_confidence = performance_df['Prediction Confidence'].mean()

        st.markdown(f"- **Average Accuracy**: {avg_accuracy:.2%}")
        st.markdown(f"- **Average ROC-AUC**: {avg_roc_auc:.2%}")
        st.markdown(f"- **Average Prediction Confidence**: {avg_confidence:.2%}")

    except Exception as e:
        st.error(f"Error generating investment strategy insights: {e}")
        st.error(traceback.format_exc())

def display_executive_dashboard(summary, daily_avg_prices, daily_indicators, black_swan_df, extreme_events_df):
    """
    Displays a comprehensive dashboard.
    """
    st.markdown("### ğŸ¯ Current Market Risk") 
    color_map = {
        "Low Risk": "green",
        "Caution": "orange",
        "Alert": "red",
        "High Risk": "darkred"
    }
    risk_color = color_map.get(summary['risk_level'], "gray")

    st.markdown(f"""
        <div style='background-color: {risk_color}; padding: 20px; border-radius: 10px; text-align: center;'>
            <h1 style='color: white; margin: 0;'>{summary['risk_level']}</h1>
            <p style='color: white; font-size: 20px; margin: 10px 0 0 0;'>Risk Level: {summary['risk_score']}/4</p>            
        </div>
        """, unsafe_allow_html=True)
    
    st.markdown("### Key Market Indicators")

    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("ğŸ¦¢ Black Swan Events", summary['event_counts']['black_swan'])
    with col2:
        st.metric("âš¡ Extreme Events", summary['event_counts']['extreme'])
    with col3:
        st.metric("ğŸ“ˆ Technical Patterns", summary['event_counts']['pattern'])
    with col4:
        st.metric("ğŸ”„ Correlation Breakdowns", summary['event_counts']['correlation'])

    st.markdown("### ğŸ¯ Category Impact Overview")

    col1, col2 = st.columns(2)
    with col1:
        st.markdown(f"""
        <div style='background-color: #f8d7da; padding: 15px; border-radius: 5px;'>
            <h4 style='color: #721c24; margin: 0;'>Most Impacted Sector</h4>
            <p style='font-size: 24px; font-weight: bold; color: #721c24; margin: 10px 0 0 0;'>{summary['most_impacted']}</p>
        </div>
        """, unsafe_allow_html=True)

    with col2:
        st.markdown(f"""
        <div style='background-color: #d4edda; padding: 15px; border-radius: 5px;'>
                <h4 style='color: #155724; margin: 0;'>Most Stable Sector</h4>
                <p style='font-size: 24px; font-weight: bold; color: #155724; margin: 10px 0 0 0;'>{summary['least_impacted']}</p>
        </div>
        """, unsafe_allow_html=True)
    
    # Current market sentiment
    if summary.get('current_sentiment'):
        st.markdown("### ğŸ§  Market Sentiment Overview")
        sentiment_color = {
            "Very Positive": "green",
            "Positive": "lightgreen",
            "Neutral": "gray",
            "Negative": "red"
        }.get(summary['current_sentiment'], "gray")

        st.markdown(f"""
                    <div style='background-color: {sentiment_color}; padding: 10px; border-radius: 5px; text-align: center;'>
                        <h3 style='color: white; margin: 0;'>{summary['current_sentiment']}</h3>
                    </div>
                    """, unsafe_allow_html=True)
    if summary.get('model_predictions'):
        st.markdown("### ğŸ¤– AI Model Predictions")
        
        # ìƒìŠ¹/í•˜ë½ ì„¹í„° êµ¬ë¶„í•˜ì—¬ ë³´ê¸° ì¢‹ê²Œ í‘œì‹œ
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("""
            <div style='background-color: rgba(0,128,0,0.1); 
                        padding: 15px; 
                        border-radius: 8px; 
                        border-left: 5px solid #4CAF50;'>
                <h4 style='color: #155724; margin-top: 0; margin-bottom: 10px;'>Bullish Sectors</h4>
            """, unsafe_allow_html=True)
            
            if summary['model_predictions']['bullish']:
                sectors_html = ""
                for sector in summary['model_predictions']['bullish']:
                    sectors_html += f"<div style='font-size: 18px; margin: 5px 0;'><span style='color: #155724; font-weight: bold;'>ğŸ“ˆ {sector}</span></div>"
                st.markdown(sectors_html, unsafe_allow_html=True)
            else:
                st.markdown("<p style='color: #666; font-style: italic;'>No bullish sectors predicted.</p>", unsafe_allow_html=True)
            
            st.markdown("</div>", unsafe_allow_html=True)

        with col2:
            st.markdown("""
            <div style='background-color: rgba(220,53,69,0.1); 
                        padding: 15px; 
                        border-radius: 8px; 
                        border-left: 5px solid #dc3545;'>
                <h4 style='color: #721c24; margin-top: 0; margin-bottom: 10px;'>Bearish Sectors</h4>
            """, unsafe_allow_html=True)
            
            if summary['model_predictions']['bearish']:
                sectors_html = ""
                for sector in summary['model_predictions']['bearish']:
                    sectors_html += f"<div style='font-size: 18px; margin: 5px 0;'><span style='color: #721c24; font-weight: bold;'>ğŸ“‰ {sector}</span></div>"
                st.markdown(sectors_html, unsafe_allow_html=True)
            else:
                st.markdown("<p style='color: #666; font-style: italic;'>No bearish sectors predicted.</p>", unsafe_allow_html=True)
            
            st.markdown("</div>", unsafe_allow_html=True)
        
        # ì˜ˆì¸¡ ì„¸ë¶€ ì •ë³´ í…Œì´ë¸” ì¶”ê°€ (ìˆëŠ” ê²½ìš°)
        if 'details' in summary['model_predictions'] and summary['model_predictions']['details']:
            st.markdown("<br>", unsafe_allow_html=True)
            st.markdown("#### Prediction Details")
            
            # í…Œì´ë¸” ë°ì´í„° ì¤€ë¹„
            table_data = []
            for category, details in summary['model_predictions']['details'].items():
                direction = details.get('direction', 'N/A')
                bullish_ratio = details.get('bullish_ratio', 0)
                
                # ë°©í–¥ì— ë”°ë¼ ìƒ‰ìƒ ì§€ì •
                color = "#4CAF50" if direction == 'bullish' else "#dc3545" if direction == 'bearish' else "#6c757d"
                icon = "ğŸ“ˆ" if direction == 'bullish' else "ğŸ“‰" if direction == 'bearish' else "â¡ï¸"
                
                # í¬ë§·ëœ ë¹„ìœ¨ ë¬¸ìì—´
                ratio_str = f"{bullish_ratio:.2%}"
                
                table_data.append({
                    "Category": category,
                    "Samples": details.get('samples', 0),
                    "Bullish Ratio": ratio_str,
                    "Prediction": f"{icon} {direction.capitalize()}"
                })
            
            if table_data:
                # ë°ì´í„°í”„ë ˆì„ ìƒì„±
                table_df = pd.DataFrame(table_data)
                
                # í…Œì´ë¸” í‘œì‹œ - ìŠ¤íŠ¸ë¦¼ë¦¿ ë°ì´í„°í”„ë ˆì„ ì´ìš©
                st.dataframe(
                    table_df,
                    column_config={
                        "Category": st.column_config.TextColumn("Category", width="medium"),
                        "Samples": st.column_config.NumberColumn("Samples", width="small"),
                        "Bullish Ratio": st.column_config.TextColumn("Bullish Ratio", width="small"),
                        "Prediction": st.column_config.TextColumn("Prediction", width="medium")
                    },
                    use_container_width=True,
                    hide_index=True
                )
     # ì£¼ìš” ë°œê²¬ì‚¬í•­       
    st.markdown("### ğŸ” Key Findings")
    for finding in summary['key_findings']:
        st.info(finding)

    # íˆ¬ì ì œì•ˆì‚¬í•­   
    st.markdown("### ğŸ’¼ Investment Recommendations")
    
    for suggestion in summary['investment_suggestions']:
        st.success(suggestion)
    
    # ìœ„í—˜ë„ ì¶”ì´ ì°¨íŠ¸
    if daily_avg_prices is not None and not daily_avg_prices.empty:
        st.markdown("### ğŸ“ˆ Risk Trend Over Time")
        
        try:
            # datetime ì¸ë±ìŠ¤ í™•ì¸
            if not pd.api.types.is_datetime64_any_dtype(daily_avg_prices.index):
                daily_avg_prices.index = pd.to_datetime(daily_avg_prices.index, errors='coerce')
            
            # ìœ íš¨í•œ ë‚ ì§œë§Œ ì„ íƒ
            valid_dates = daily_avg_prices.index[~daily_avg_prices.index.isna()]
            
            if len(valid_dates) > 0:
                risk_timeline = pd.DataFrame(index=valid_dates)
                risk_timeline['risk_score'] = 0
                
                # ê° ì´ë²¤íŠ¸ ë‚ ì§œì— ìœ„í—˜ë„ ì ìˆ˜ ë¶€ì—¬
                if black_swan_df is not None and not black_swan_df.empty and 'date' in black_swan_df.columns:
                    for _, event in black_swan_df.iterrows():
                        try:
                            event_date = pd.to_datetime(event['date'], errors='coerce')
                            if pd.notnull(event_date) and event_date in risk_timeline.index:
                                risk_timeline.loc[event_date, 'risk_score'] += 3
                        except:
                            continue
                
                if extreme_events_df is not None and not extreme_events_df.empty and 'date' in extreme_events_df.columns:
                    for _, event in extreme_events_df.iterrows():
                        try:
                            event_date = pd.to_datetime(event['date'], errors='coerce')
                            if pd.notnull(event_date) and event_date in risk_timeline.index:
                                risk_timeline.loc[event_date, 'risk_score'] += 1
                        except:
                            continue
                
                # 30ì¼ ì´ë™í‰ê· ìœ¼ë¡œ ìœ„í—˜ë„ ì¶”ì´ ê³„ì‚°
                risk_timeline['smoothed_risk'] = risk_timeline['risk_score'].rolling(window=30, min_periods=1).mean()
                
                # ì‹œê°í™”
                fig, ax = plt.subplots(figsize=(10, 8))

                ax.plot(risk_timeline.index, risk_timeline['smoothed_risk'], label='Risk Level', color='red', linewidth=2)
                ax.fill_between(risk_timeline.index, 0, risk_timeline['smoothed_risk'], alpha=0.2, color='red')
                
                # ë¸”ë™ ìŠ¤ì™„ ì´ë²¤íŠ¸ í‘œì‹œ
                if black_swan_df is not None and not black_swan_df.empty and 'date' in black_swan_df.columns:
                    for _, event in black_swan_df.iterrows():
                        try:
                            event_date = pd.to_datetime(event['date'], errors='coerce')
                            if pd.notnull(event_date) and event_date in risk_timeline.index:
                                ax.scatter(event_date, risk_timeline.loc[event_date, 'smoothed_risk'],
                                          color='black', s=100, zorder=5, marker='*')
                        except:
                            continue
                
                ax.set_title('Risk Trend Over Time', fontsize=16)
                ax.set_xlabel('Date', fontsize=14)
                ax.set_ylabel('Risk Level', fontsize=14)
                ax.legend()
                ax.grid(True, alpha=0.3)

                plt.xticks(rotation=45)
                plt.tight_layout()
                st.pyplot(fig)
                plt.close()
        except Exception as e:
            st.warning(f"ìœ„í—˜ë„ ì¶”ì´ ì°¨íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")
    
    # Report download section
    st.markdown("### ğŸ“¥ Download Report")

    report_text = f"""
    # ğŸ§¾ Executive Report on Market Extreme Events

    **Generated on: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}**

    ## 1. Market Risk Overview
    - Current Risk Level: {summary['risk_level']} (Score: {summary['risk_score']}/4)
    - Total Events Detected: {summary['total_events']}
    - Market Sentiment: {summary.get('current_sentiment', 'N/A')}

    ## 2. Category Analysis
    - Most Impacted Category: {summary['most_impacted']}
    - Most Stable Category: {summary['least_impacted']}

    ## 3. Event Breakdown
    - Black Swan Events: {summary['event_counts']['black_swan']}
    - Extreme Events: {summary['event_counts']['extreme']}
    - Technical Patterns: {summary['event_counts']['pattern']}
    - Correlation Breakdowns: {summary['event_counts']['correlation']}

    ## 4. Key Findings
    {chr(10).join(['- ' + finding for finding in summary['key_findings']])}
    
    ## 5. Investment Suggestions
    {chr(10).join(['- ' + suggestion for suggestion in summary['investment_suggestions']])}
    """

    st.download_button(
        label="ğŸ“„ Download Report",
        data=report_text,
        file_name=f"extreme_events_report_{pd.Timestamp.now().strftime('%Y%m%d')}.txt",
        mime="text/plain"
    )
def extreme_events_dashboard(df, daily_avg_prices, daily_indicators, ml_predictions=None):
    """
    ê·¹ë‹¨ì  ì´ë²¤íŠ¸ ëŒ€ì‹œë³´ë“œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    
    Parameters:
    -----------
    df : DataFrame
        ì›ë³¸ ë°ì´í„°í”„ë ˆì„
    daily_avg_prices : DataFrame
        ì¼ë³„ í‰ê·  ê°€ê²© ë°ì´í„°
    daily_indicators : DataFrame
        ì¼ë³„ ì§€í‘œ ë°ì´í„°
    ml_predictions : dict, optional
        ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë“œì—ì„œ ìƒì„±ëœ ì˜ˆì¸¡ ê²°ê³¼
    """
    st.header("ğŸ” Extreme Event Dashboard")
    
    # ë¨¸ì‹ ëŸ¬ë‹ ì˜ˆì¸¡ ë°ì´í„° í™•ì¸
    ml_predictions = ml_predictions or st.session_state.get('enhanced_model_data')
    
    if ml_predictions:
        st.success("âœ…ML predictions found. Including them in the analysis.")
    else:
        st.info("â„¹ï¸ No ML predictions detected. Please run the ML mode for a full analysis.")
    
    # ëŒ€ì‹œë³´ë“œ ì„¤ëª…
    st.markdown("""
    Analyze black swan evets, market shocks, technical patterns, and more - all to power smarter investment decisions.
    """)
    #ë°ì´í„° ìœ íš¨ì„± ê²€ì‚¬
    if df is None or df.empty:
        st.warning("No data for analysis")
        return
    if daily_avg_prices is None or daily_avg_prices.empty:
        st.warning("no daily avg prices data")
        return
    if daily_indicators is None or daily_indicators.empty:
        st.warning(" no daily indicator data")
        return
    
    #date type 
    try: 
        if 'date' in df.columns and not pd.api.types.is_datetime64_any_dtype(df['date']):
            df['date'] = pd.to_datetime(df['date'], errors='coerce')
            st.success("Date column converted to datetime")
    except Exception as e:
        st.error(f"An error occurred during date conversion: {str(e)}")
        return
            
   # ê°ì§€ ì„¤ì •
    st.subheader("âš™ï¸ Detection Settings")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        black_swan_threshold = st.slider(
            "Black Swan Threshold(Z-score)",
            min_value=2.0,
            max_value=5.0,
            value=3.0,
            step=0.1,
            help="Extreme events are identified when values exceed 3.0 (99.7% confidence interval)"
        )
    
    with col2:
        extreme_percentile = st.slider(
            "Extreme economic indicator percentile",
            min_value=5,
            max_value=20,
            value=10,
            step=1,
            help="Top/Bottom n% events are flagged as extreme."
        )
    
    with col3:
        correlation_threshold = st.slider(
            "Correlation breakdown threshold",
            min_value=0.3,
            max_value=0.8,
            value=0.6,
            step=0.1,
            help="Sudden correlation shift threshold"
        )
    
    # ì¹´í…Œê³ ë¦¬ ì„ íƒ
    if 'category' in df.columns:
        categories = df['category'].unique().tolist()
        selected_category = st.selectbox(
            "Select Category (Optional)",
            ["All"] + categories
        )
        
        category_filter = None if selected_category == "All" else selected_category
    else:
        category_filter = None
    
    # ê°ì§€ ì‹¤í–‰ ë²„íŠ¼
    detect_events = st.button("ğŸ” Dectect Extreme Event")
    
    if detect_events:
        try:
            with st.spinner("Detecting special conditions and extreme events..."):
                # 1. ê·¹ë‹¨ì  ì´ë²¤íŠ¸ ê°ì§€ detect extreme event
                try:
                    extreme_events_df = detect_extreme_events(df, daily_avg_prices, daily_indicators)
                    if extreme_events_df is None:
                        extreme_events_df = pd.DataFrame(columns=['date', 'event_type', 'indicator', 'value', 'threshold', 'percentile', 'description'])
                        st.warning("No extreme events detected")
                except Exception as e:
                    st.error(f"error during extreme event detection: {str(e)}")
                    extreme_events_df = pd.DataFrame(columns=['date', 'event_type', 'indicator', 'value', 'threshold', 'percentile', 'description'])
                #2. Detect technical pattern 
                try:
                    technical_patterns_df = detect_technical_patterns(daily_avg_prices)
                    if technical_patterns_df is None:
                        technical_patterns_df = pd.DataFrame(columns=['date', 'category', 'pattern', 'description'])
                        st.warning("No extreme events detected")
                except Exception as e:
                    st.error(f"error during technical pattern detection: {str(e)}")
                    technical_patterns_df = pd.DataFrame(columns=['date', 'category', 'pattern', 'description'])
                #3 ë¸”ë™ ìŠ¤ì™„ ì´ë²¤íŠ¸ ê°ì§€. Detect black swan event
                try:
                    black_swan_df = detect_black_swan_events(df, daily_avg_prices, daily_indicators, std_threshold=black_swan_threshold)
                    if black_swan_df is None:
                        black_swan_df = pd.DataFrame(columns=['date', 'category', 'event_type', 'z_score', 'return_pct', 'description'])
                        st.warning("No extreme events detected.")
                except Exception as e:
                    st.error(f"error during extreme event detection: {str(e)}")
                    black_swan_df = pd.DataFrame(columns=['date', 'category', 'event_type', 'z_score', 'return_pct', 'description'])

                # 4. ìƒê´€ê´€ê³„ ë¶•ê´´ ê°ì§€
                try:
                    correlation_breakdown_df = detect_correlations_breakdown(
                        daily_avg_prices,daily_indicators,
                        threshold= correlation_threshold
                    )
                    if correlation_breakdown_df is None:
                        correlation_breakdown_df= pd.DataFrame(columns=['date', 'pair', 'event_type', 'old_correlation', 'new_correlation', 'change', 'description'])
                        st.warning("ìƒê´€ê´€ê³„ ë¶•ê´´ ì´ë²¤íŠ¸ë¥¼ ê°ì§€í•  ìˆ˜ ì—†ì—ˆìŠµë‹ˆë‹¤.")
                except Exception as e:
                    st.error(f"an error while detecting correlationg breake down: {str(e)}")
                    correlation_breakdown_df = pd.DataFrame(columns=['date', 'pair', 'event_type', 'old_correlation', 'new_correlation', 'change', 'description'])

                # ê²°ê³¼ ì €ì¥ save results.
                st.session_state['extreme_events_df']= extreme_events_df
                st.session_state['black_swan_df'] = black_swan_df
                st.session_state['technical_patterns_df'] = technical_patterns_df
                st.session_state['correlation_breakdown_df'] = correlation_breakdown_df

                st.session_state['event_detected'] = True
                
            st.success("âœ… Decected Extreme Event Analysis!")
        except Exception as e:
            st.error(f"ì´ë²¤íŠ¸ ê°ì§€ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
            return
        
        # íƒ­ ìƒì„±
        tab1, tab2, tab3, tab4, tab5, tab6 = st.tabs([
            "ğŸ“Š Event Summary", 
            "ğŸ’¥ Black Swan", 
            "ğŸ“ˆ Technical Patterns", 
            "ğŸ“‰ Extreme Economic Indicators",
            "ğŸ”„ Correlation Breakdown",
            "ğŸ“„ Insight & Report"
        ])
        
        # íƒ­ 1: ì´ë²¤íŠ¸ ìš”ì•½
        with tab1:
            st.subheader("ğŸ“Š Exceptional Market Events Summary")
            
            # ì´ë²¤íŠ¸ ê°œìˆ˜ ìš”ì•½
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                black_swan_count = len(black_swan_df) if black_swan_df is not None and not black_swan_df.empty else 0
                st.metric("Black Swan", black_swan_count)
            
            with col2:
                pattern_count = len(technical_patterns_df) if technical_patterns_df is not None and not technical_patterns_df.empty else 0
                st.metric("Technical Patterns", pattern_count)
            
            with col3:
                extreme_econ_count = len(extreme_events_df) if extreme_events_df is not None and not extreme_events_df.empty else 0
                st.metric("Extreme Economic Indicators", extreme_econ_count)
            
            with col4:
                correlation_count = len(correlation_breakdown_df) if correlation_breakdown_df is not None and not correlation_breakdown_df.empty else 0
                st.metric("Correlation Breakdown", correlation_count)
            
            # # ì¹´í…Œê³ ë¦¬ë³„ ì´ë²¤íŠ¸ ë¶„í¬
            # if 'category' in df.columns:
            #     st.subheader("Category-wise Distribution of Extreme Events")
                
            #     # ê° ì´ë²¤íŠ¸ ìœ í˜•ë³„ ì¹´í…Œê³ ë¦¬ ë¶„í¬ ê³„ì‚°
            #     category_distributions = {}
                
            #     # ë¸”ë™ ìŠ¤ì™„ ë¶„í¬
            #     if black_swan_df is not None and not black_swan_df.empty and 'category' in black_swan_df.columns:
            #         category_distributions['Black Swan'] = black_swan_df['category'].value_counts()
                
            #     # ê¸°ìˆ ì  íŒ¨í„´ ë¶„í¬
            #     if technical_patterns_df is not None and not technical_patterns_df.empty and 'category' in technical_patterns_df.columns:
            #         category_distributions['Technical Patterns'] = technical_patterns_df['category'].value_counts()
                
            #     # ê²°ê³¼ ì‹œê°í™”
            #     if category_distributions:
            #         distribution_df = pd.DataFrame(category_distributions)
            #         # if event_dates and event_types:
            #         distribution_df = distribution_df.fillna(0)
                        
            #             # ë§‰ëŒ€ ê·¸ë˜í”„ ê·¸ë¦¬ê¸°
            #         fig, ax = plt.subplots(figsize=(7, 4))
            #         distribution_df.plot(kind='bar', ax=ax)

            #         plt.title('Category-wise Distribution of Extreme Events', fontsize=16)
            #         plt.xlabel('Category', fontsize=10)
            #         plt.ylabel('Event', fontsize=10)
            #         plt.xticks(rotation=45, ha='right')
            #         plt.legend(title='Event Type', fontsize= 8)
            #         plt.tight_layout()
            #         st.pyplot(fig)
            #         plt.close()
                             
        
            # ì‹œê°„ì— ë”°ë¥¸ ì´ë²¤íŠ¸ ë¶„í¬
            st.subheader("Distribution of Extreme Events Over Time")
            
            # ë°ì´í„° ì¤€ë¹„
            event_dates = []
            event_types = []
            
            # ë¸”ë™ ìŠ¤ì™„ ì´ë²¤íŠ¸
            if black_swan_df is not None and not black_swan_df.empty:
                for _, row in black_swan_df.iterrows():
                    event_dates.append(row['date'])
                    event_types.append('Black Swan')
            
            # ê¸°ìˆ ì  íŒ¨í„´
            if technical_patterns_df is not None and not technical_patterns_df.empty:
                for _, row in technical_patterns_df.iterrows():
                    event_dates.append(row['date'])
                    event_types.append('Technical Patterns')
            
            # ê·¹ë‹¨ì  ê²½ì œ ì§€í‘œ
            if extreme_events_df is not None and not extreme_events_df.empty:
                econ_extremes = extreme_events_df[
                    (extreme_events_df['event_type'] == 'Extreme High') | 
                    (extreme_events_df['event_type'] == 'Extreme Low')
                ]
                
                for _, row in econ_extremes.iterrows():
                    event_dates.append(row['date'])
                    event_types.append('Extreme Economic Indicators')
            
            # ìƒê´€ê´€ê³„ ë¶•ê´´
            if correlation_breakdown_df is not None and not correlation_breakdown_df.empty:
                for _, row in correlation_breakdown_df.iterrows():
                    event_dates.append(row['date'])
                    event_types.append('Correlation Breakedown')
            
            # ì´ë²¤íŠ¸ íƒ€ì„ë¼ì¸ ìƒì„±
            if event_dates and event_types:
                events_df = pd.DataFrame({
                    'date': event_dates,
                    'event_type': event_types
                })
                try:
                    # ëª¨ë“  ë‚ ì§œë¥¼ datetimeìœ¼ë¡œ ê°•ì œ ë³€í™˜
                    events_df['date'] = pd.to_datetime(events_df['date'], errors='coerce')

                    # ë³€í™˜ ì‹¤íŒ¨í•œ ë‚ ì§œëŠ” ê¸°ë³¸ê°’ìœ¼ë¡œ ì±„ìš°ê¸°
                    fallback_date = pd.Timestamp("1900-01-01")  # ë˜ëŠ” ê°€ì¥ ì˜¤ë˜ëœ ë‚ ì§œ ì´ì „
                    events_df['date'] = events_df['date'].fillna(fallback_date)

                    # ì›”ë³„ ì´ë²¤íŠ¸ ìˆ˜ ê³„ì‚°
                    events_df['yearmonth'] = events_df['date'].dt.strftime('%Y-%m')
                    monthly_events = events_df.groupby(['yearmonth', 'event_type']).size().unstack().fillna(0)

                    # ì‹œê°í™”
                    # ì‹œê°í™”
                    fig, ax = plt.subplots(figsize=(12, 6))  # ê°€ë¡œë¥¼ ë„“ê²Œ
                    monthly_events.plot(kind='bar', stacked=True, ax=ax)

                    # ì œëª©, ì¶• ë ˆì´ë¸”
                    plt.title('Monthly Frequency of Extreme Events', fontsize=12)
                    plt.xlabel('Year-Month', fontsize=10)
                    plt.ylabel('Number of Events', fontsize=10)

                    # xì¶• ë ˆì´ë¸” ì„¤ì •
                    plt.xticks(rotation=45, ha='right', fontsize=8)

                    # ë²”ë¡€
                    plt.legend(title='Event Type', fontsize=8)

                    # yì¶• ë²”ìœ„ ì œí•œí•´ì„œ ë„ˆë¬´ ê¸´ ë°” ë°©ì§€
                    ax.set_ylim(0, 25)   # âœ… í•„ìš”ì— ë§ê²Œ ì¡°ì •í•´

                    # ë ˆì´ì•„ì›ƒ ì •ë¦¬
                    plt.tight_layout()

                    # Streamlit ì¶œë ¥
                    st.pyplot(fig)
                    plt.close()


                except Exception as e:
                    st.error(f"ğŸ“› ì´ë²¤íŠ¸ íƒ€ì„ë¼ì¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                # 1. Korrelationseinbruch zwischen Verteidigungsindustrie und CPI (erstes Bild)

                st.markdown("""
                            # 1. Korrelationseinbruch zwischen Verteidigungsindustrie und CPI (erstes Bild)

                            ## Kernaussagen:
                            - Am 10. Mai 2024 stieg die Korrelation von 0,04 auf 0,66 (+0,62).
                            - Vorher: kaum Zusammenhang â†’ Nachher: starke positive Korrelation.
                            - Strukturwandel: Inflation beeinflusst zunehmend Investitionen in die Verteidigungsindustrie.

                            ## Implikationen:
                            - Inflation wird mit geopolitischen Spannungen verknÃ¼pft.
                            - CPI-Anstieg wird als Signal fÃ¼r steigende Aktienkurse der Verteidigungsbranche neu bewertet.

                            """)
        
        
            # ìµœê·¼ ê·¹ë‹¨ ì´ë²¤íŠ¸ ëª©ë¡
            st.subheader("Recently Detected Extreme Events")
            
            # ëª¨ë“  ì´ë²¤íŠ¸ í†µí•©
            all_events = []
            
            # ë¸”ë™ ìŠ¤ì™„ ì´ë²¤íŠ¸
            if black_swan_df is not None and not black_swan_df.empty:
                for _, row in black_swan_df.iterrows():
                    all_events.append({
                        'date': row['date'],
                        'event_type': 'Black swan',
                        'description': row['description'],
                        'category': row['category'] if 'category' in row else None
                    })
            
            # ê¸°ìˆ ì  íŒ¨í„´
            if technical_patterns_df is not None and not technical_patterns_df.empty:
                for _, row in technical_patterns_df.iterrows():
                    all_events.append({
                        'date': row['date'],
                        'event_type': 'Technical Patterns',
                        'description': f"{row['pattern']}: {row['description']}",
                        'category': row['category'] if 'category' in row else None
                    })
            
            # ê·¹ë‹¨ì  ê²½ì œ ì§€í‘œ
            if extreme_events_df is not None and not extreme_events_df.empty:
                econ_extremes = extreme_events_df[
                    (extreme_events_df['event_type'] == 'Extreme High') | 
                    (extreme_events_df['event_type'] == 'Extreme Low')
                ]
                
                for _, row in econ_extremes.iterrows():
                    all_events.append({
                        'date': row['date'],
                        'event_type': 'Extreme Economic Indicators',
                        'description': row['description'],
                        'category': None
                    })
            
            # ìƒê´€ê´€ê³„ ë¶•ê´´
            if correlation_breakdown_df is not None and not correlation_breakdown_df.empty:
                for _, row in correlation_breakdown_df.iterrows():
                    all_events.append({
                        'date': row['date'],
                        'event_type': 'Correlation Breakdown',
                        'description': row['description'],
                        'category': row['pair'] if 'pair' in row else None
                    })
            
            # ëª¨ë“  ì´ë²¤íŠ¸ í‘œì‹œ
            if all_events:
                events_df = pd.DataFrame(all_events)
                events_df = events_df.sort_values('date', ascending=False)
                
                # ì¹´í…Œê³ ë¦¬ í•„í„°ë§ (í•„ìš”ì‹œ)
                if category_filter:
                    events_df = events_df[events_df['category'] == category_filter]
                
                # ìµœê·¼ 20ê°œ ì´ë²¤íŠ¸ë§Œ í‘œì‹œ
                recent_events = events_df.head(20)
                
                st.dataframe(
                    recent_events[['date', 'event_type', 'description', 'category']],
                    column_config={
                        "date": st.column_config.DatetimeColumn("date"),
                        "event_type": st.column_config.TextColumn("event_type"),
                        "description": st.column_config.TextColumn("description"),
                        "category": st.column_config.TextColumn("category")
                    }
                )
            else:
                st.info("ê°ì§€ëœ ê·¹ë‹¨ ì´ë²¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        # íƒ­ 2: ë¸”ë™ ìŠ¤ì™„ ì´ë²¤íŠ¸
        with tab2:
            st.subheader("ğŸ’¥ Black Swan Events")
            
            if black_swan_df is not None and not black_swan_df.empty:
                # í•„í„°ë§ëœ ë°ì´í„°
                filtered_bs = black_swan_df
                if category_filter:
                    filtered_bs = black_swan_df[black_swan_df['category'] == category_filter]
                
                if not filtered_bs.empty:
                    # ë¸”ë™ ìŠ¤ì™„ ì´ë²¤íŠ¸ í‘œì‹œ
                    st.write("detected Black Swan Event:")
                    
                    st.dataframe(
                        filtered_bs.sort_values('date', ascending=False),
                        column_config={
                            "date": st.column_config.DatetimeColumn("date"),
                            "category": st.column_config.TextColumn("category"),
                            "event_type": st.column_config.TextColumn("event_type"),
                            "z_score": st.column_config.NumberColumn("Z-score", format="%.2f"),
                            "return_pct": st.column_config.NumberColumn("return_pct (%)", format="%.2f"),
                            "description": st.column_config.TextColumn("description")
                        }
                    )
                    
                    # ë¸”ë™ ìŠ¤ì™„ ì‹œê°í™”
                    try: 
                        visualize_extreme_events(None, filtered_bs, daily_avg_prices, daily_indicators, st.session_state.get('technical_patterns_df'), category_filter)
                    except Exception as e:
                        st.error(f"ğŸ˜­error occured while visualizing black swan event {str(e)}")
                    # ë¸”ë™ ìŠ¤ì™„ ì´ë²¤íŠ¸ì˜ ìˆ˜ìµë¥  ì˜í–¥ ë¶„ì„
                    try: 
                        analyze_extreme_events_impact(df, None, filtered_bs, category_filter)
                    except Exception as e:
                        st.error(f"ğŸ˜­error occurred while analyzing the impact of black swan event: {str(e)}")
                else:
                    st.info(f"ì„ íƒí•œ ì¹´í…Œê³ ë¦¬ '{category_filter}'ì— ëŒ€í•œ ë¸”ë™ ìŠ¤ì™„ ì´ë²¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
            else:
                st.info("ê°ì§€ëœ ë¸”ë™ ìŠ¤ì™„ ì´ë²¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        # íƒ­ 3: ê¸°ìˆ ì  íŒ¨í„´
        with tab3:
            st.subheader("ğŸ“ˆ Technical Patterns")
            
            if technical_patterns_df is not None and not technical_patterns_df.empty:
                # í•„í„°ë§ëœ ë°ì´í„°
                filtered_patterns = technical_patterns_df
                if category_filter:
                    filtered_patterns = technical_patterns_df[technical_patterns_df['category'] == category_filter]
                
                if not filtered_patterns.empty:
                    # ê¸°ìˆ ì  íŒ¨í„´ í‘œì‹œ
                    st.write("Detected Technical Patterns:")
                    
                    st.dataframe(
                        filtered_patterns.sort_values('date', ascending=False),
                        column_config={
                            "date": st.column_config.DatetimeColumn("date"),
                            "category": st.column_config.TextColumn("category"),
                            "pattern": st.column_config.TextColumn("pattern"),
                            "description": st.column_config.TextColumn("description")
                        }
                    )
                    
                    # íŒ¨í„´ ìœ í˜•ë³„ ë¶„í¬ ì‹œê°í™”ì™€ í•¨ê»˜ í‘œì‹œí•  ë‹¤ë¥¸ ë‚´ìš© ì¤€ë¹„
                    col1, col2 = st.columns(2)
                    
                    # ì²« ë²ˆì§¸ ì»¬ëŸ¼: íŒ¨í„´ ìœ í˜•ë³„ ë¶„í¬
                    with col1:
                        try: 
                            if 'pattern' not in filtered_patterns.columns or filtered_patterns['pattern'].empty:
                                st.info("pattern data is missing or empty")
                            else:
                                pattern_counts = filtered_patterns['pattern'].value_counts()

                            if pattern_counts.empty:
                                st.info("impossible to calculate Pattern distributionğŸ˜­")
                            else:
                                fig, ax = plt.subplots(figsize=(8, 6))
                                pattern_counts.plot(kind='bar', ax=ax, color='teal')
                                
                                plt.title('Frequency of Technical Pattern by Type', fontsize=16)
                                plt.xlabel('pattern', fontsize=14)
                                plt.ylabel('frequency', fontsize=14)
                                plt.xticks(rotation=45, ha='right')
                                plt.tight_layout()
                                
                                st.pyplot(fig)
                                plt.close()
                        except Exception as e:
                            st.error(f"error occurred while visualization pattern distribution")
                    
                    # ë‘ ë²ˆì§¸ ì»¬ëŸ¼: ì¹´í…Œê³ ë¦¬ë³„ íŒ¨í„´ ë¶„í¬
                    with col2:
                        try:
                            if 'category' in filtered_patterns.columns and 'pattern' in filtered_patterns.columns:
                                # ì¹´í…Œê³ ë¦¬ë³„ íŒ¨í„´ ë¶„í¬ ê³„ì‚°
                                category_pattern_counts = filtered_patterns.groupby(['category', 'pattern']).size().unstack().fillna(0)
                                
                                if not category_pattern_counts.empty and category_pattern_counts.shape[0] > 0 and category_pattern_counts.shape[1] > 0:
                                    fig, ax = plt.subplots(figsize=(8, 6))
                                    category_pattern_counts.plot(kind='bar', stacked=True, ax=ax, colormap='viridis')
                                    
                                    plt.title('Patterns Distribution by Category', fontsize=16)
                                    plt.xlabel('Category', fontsize=14)
                                    plt.ylabel('Count', fontsize=14)
                                    plt.xticks(rotation=45, ha='right')
                                    plt.legend(title='Pattern', bbox_to_anchor=(1.05, 1), loc='upper left')
                                    plt.tight_layout()
                                    
                                    st.pyplot(fig)
                                    plt.close()
                                else:
                                    st.info("Not enough data to visualize patterns by category")
                            else:
                                st.info("Required columns 'category' or 'pattern' not found")
                        except Exception as e:
                            st.error(f"Error analyzing patterns by category: {str(e)}")
                    
                    # ëŒ€í‘œì ì¸ íŒ¨í„´ì— ëŒ€í•œ ê°€ê²© ì°¨íŠ¸ ì‹œê°í™”
                    if daily_avg_prices is not None and not daily_avg_prices.empty:
                        st.subheader("Main Technical Pattern Visualization")
                        
                        try:
                            # ê°€ì¥ ë§ì´ ë°œìƒí•œ íŒ¨í„´ ì„ íƒ
                            if 'pattern' in filtered_patterns.columns:
                                pattern_counts = filtered_patterns['pattern'].value_counts()
                                if not pattern_counts.empty:
                                    top_patterns = pattern_counts.head(min(6, len(pattern_counts))).index.tolist()
                                    
                                    # íŒ¨í„´ ë°ì´í„° ì²˜ë¦¬ ë° ì‹œê°í™”
                                    pattern_examples = []
                                    
                                    # ëª¨ë“  íŒ¨í„´ ì˜ˆì‹œ ë°ì´í„° ìˆ˜ì§‘
                                    for pattern in top_patterns:
                                        pattern_data = filtered_patterns[filtered_patterns['pattern'] == pattern]
                                        
                                        if not pattern_data.empty:
                                            # ê° íŒ¨í„´ì— ëŒ€í•´ ìµœëŒ€ 2ê°œ ì˜ˆì‹œ ê°€ì ¸ì˜¤ê¸°
                                            for _, row in pattern_data.head(2).iterrows():
                                                pattern_examples.append({
                                                    'pattern': pattern,
                                                    'date': row['date'],
                                                    'category': row['category'],
                                                    'description': row.get('description', '')
                                                })
                                    
                                    # ì˜ˆì‹œê°€ ì§ìˆ˜ê°€ ì•„ë‹ˆë©´ í•˜ë‚˜ë¥¼ ì œê±°í•˜ì—¬ ì§ìˆ˜ë¡œ ë§Œë“¤ê¸°
                                    if len(pattern_examples) % 2 != 0 and len(pattern_examples) > 0:
                                        pattern_examples = pattern_examples[:-1]
                                    
                                    # 2ê°œì”© ë‚˜ëˆ ì„œ ê·¸ë¦¬ë“œ ë ˆì´ì•„ì›ƒìœ¼ë¡œ í‘œì‹œ
                                    for i in range(0, len(pattern_examples), 2):
                                        cols = st.columns(2)
                                        
                                        for j in range(2):
                                            if i + j < len(pattern_examples):
                                                example = pattern_examples[i + j]
                                                
                                                with cols[j]:
                                                    pattern_date = example['date']
                                                    pattern_name = example['pattern']
                                                    pattern_category = example['category']
                                                    pattern_desc = example['description']
                                                    
                                                    # íŒ¨í„´ ì„¤ëª… í‘œì‹œ
                                                    st.markdown(f"**{pattern_name}** - {pattern_date.strftime('%Y-%m-%d')}")
                                                    if pattern_desc:
                                                        st.markdown(f"*{pattern_desc}*", help=pattern_desc)
                                                    
                                                    # íŒ¨í„´ ì „í›„ 30ì¼ ê¸°ê°„
                                                    start_date = pattern_date - pd.Timedelta(days=30)
                                                    end_date = pattern_date + pd.Timedelta(days=30)
                                                    
                                                    # ë°ì´í„° í•„í„°ë§
                                                    mask = (daily_avg_prices.index >= start_date) & (daily_avg_prices.index <= end_date)
                                                    period_data = daily_avg_prices.loc[mask]
                                                    
                                                    if not period_data.empty and pattern_category in period_data.columns:
                                                        # ì´ë™í‰ê· ì„  ê³„ì‚°
                                                        ma5 = period_data[pattern_category].rolling(window=5).mean()
                                                        ma20 = period_data[pattern_category].rolling(window=20).mean()
                                                        ma50 = period_data[pattern_category].rolling(window=50).mean()
                                                        
                                                        # ì°¨íŠ¸ ê·¸ë¦¬ê¸°
                                                        fig, ax = plt.subplots(figsize=(8, 6))
                                                        
                                                        ax.plot(period_data.index, period_data[pattern_category], label=pattern_category, linewidth=2)
                                                        ax.plot(period_data.index, ma5, label='5-Day MA', linestyle='--')
                                                        ax.plot(period_data.index, ma20, label='20-Day MA', linestyle='--')
                                                        ax.plot(period_data.index, ma50, label='50-Day MA', linestyle='--')
                                                        
                                                        # íŒ¨í„´ ë°œìƒì¼ í‘œì‹œ
                                                        if pattern_date in period_data.index:
                                                            pattern_price = period_data.loc[pattern_date, pattern_category]
                                                            ax.scatter(pattern_date, pattern_price, color='red', s=100, zorder=5)
                                                            ax.annotate(
                                                                pattern_name,
                                                                (pattern_date, pattern_price),
                                                                textcoords="offset points",
                                                                xytext=(0, 10),
                                                                ha='center',
                                                                fontweight='bold',
                                                                bbox=dict(boxstyle="round,pad=0.3", fc="yellow", alpha=0.7)
                                                            )
                                                        
                                                        ax.set_title(f"{pattern_category} - {pattern_name}", fontsize=14)
                                                        ax.set_xlabel('Date', fontsize=12)
                                                        ax.set_ylabel('Price', fontsize=12)
                                                        ax.legend()
                                                        ax.grid(True, alpha=0.3)
                                                        
                                                        plt.xticks(rotation=45)
                                                        plt.tight_layout()
                                                        
                                                        st.pyplot(fig)
                                                        plt.close()
                                                    else:
                                                        st.warning(f"No data available for {pattern_category} in the selected period")
                                else:
                                    st.warning("No patterns found to visualize")
                            else:
                                st.warning("'pattern' column not found in the data")
                            
                            # ì¶”ê°€ ì‹œê°í™”: íŒ¨í„´ ë°œìƒ ì‹œê°„ëŒ€ë³„ ë¶„í¬
                            st.subheader("Pattern Time Distribution Analysis")
                            
                            col1, col2 = st.columns(2)
                            
                            with col1:
                                try:
                                    if 'date' in filtered_patterns.columns:
                                        # ë…„/ì›”ë³„ íŒ¨í„´ ë°œìƒ ë¹ˆë„
                                        filtered_patterns['year_month'] = filtered_patterns['date'].dt.strftime('%Y-%m')
                                        monthly_counts = filtered_patterns.groupby('year_month').size()
                                        
                                        fig, ax = plt.subplots(figsize=(8, 6))
                                        monthly_counts.plot(kind='line', marker='o', ax=ax, color='purple')
                                        
                                        plt.title('Monthly Pattern Occurrence', fontsize=14)
                                        plt.xlabel('Year-Month', fontsize=12)
                                        plt.ylabel('Number of Patterns', fontsize=12)
                                        plt.xticks(rotation=45)
                                        plt.grid(True, alpha=0.3)
                                        plt.tight_layout()
                                        
                                        st.pyplot(fig)
                                        plt.close()
                                    else:
                                        st.info("Date information not available")
                                except Exception as e:
                                    st.error(f"Error analyzing time distribution: {str(e)}")
                            
                            with col2:
                                try:
                                    if 'date' in filtered_patterns.columns and 'pattern' in filtered_patterns.columns:
                                        # ìµœê·¼ 1ë…„ ë°ì´í„° í•„í„°ë§
                                        last_date = filtered_patterns['date'].max()
                                        one_year_ago = last_date - pd.Timedelta(days=365)
                                        recent_patterns = filtered_patterns[filtered_patterns['date'] >= one_year_ago]
                                        
                                        if not recent_patterns.empty:
                                            # íŒ¨í„´ë³„ ì›”ê°„ ë¶„í¬ (íˆíŠ¸ë§µ)
                                            recent_patterns['month'] = recent_patterns['date'].dt.month
                                            pattern_month_counts = recent_patterns.groupby(['pattern', 'month']).size().unstack().fillna(0)
                                            
                                            fig, ax = plt.subplots(figsize=(8, 6))
                                            sns.heatmap(pattern_month_counts, cmap='YlGnBu', annot=True, fmt='g', ax=ax)
                                            
                                            plt.title('Monthly Pattern Distribution (Last Year)', fontsize=14)
                                            plt.xlabel('Month', fontsize=12)
                                            plt.ylabel('Pattern', fontsize=12)
                                            plt.tight_layout()
                                            
                                            st.pyplot(fig)
                                            plt.close()
                                        else:
                                            st.info("Not enough recent data for monthly pattern analysis")
                                    else:
                                        st.info("Required columns not available")
                                except Exception as e:
                                    st.error(f"Error creating pattern heatmap: {str(e)}")
                            
                        except Exception as e:
                            st.error(f"Error creating technical pattern visualization: {str(e)}")
        
        # íƒ­ 4: ê²½ì œ ì§€í‘œ ê·¹ë‹¨ê°’
        # íƒ­ 4: ê²½ì œ ì§€í‘œ ê·¹ë‹¨ê°’
        with tab4:
            st.subheader("ğŸ“‰ Extreme Economic Indicators")
            
            if extreme_events_df is not None and not extreme_events_df.empty:
                # ê²½ì œ ì§€í‘œ ê·¹ë‹¨ê°’ í•„í„°ë§
                economic_extremes = extreme_events_df[
                    ((extreme_events_df['event_type'] == 'Extreme High') | 
                    (extreme_events_df['event_type'] == 'Extreme Low')) &
                    (extreme_events_df['indicator'].str.contains('_norm'))
                ]
                
                if not economic_extremes.empty:
                    # ê²½ì œ ì§€í‘œ ê·¹ë‹¨ê°’ í‘œì‹œ
                    st.write("Detected Extreme Economic Indicators:")
                    
                    st.dataframe(
                        economic_extremes.sort_values('date', ascending=False),
                        column_config={
                            "date": st.column_config.DatetimeColumn("date"),
                            "event_type": st.column_config.TextColumn("event_type"),
                            "indicator": st.column_config.TextColumn("indicator"),
                            "value": st.column_config.NumberColumn("value", format="%.2f"),
                            "threshold": st.column_config.NumberColumn("threshold", format="%.2f"),
                            "percentile": st.column_config.NumberColumn("percentile"),
                            "description": st.column_config.TextColumn("description")
                        }
                    )
                    
                    # ê²½ì œ ì§€í‘œ ê·¹ë‹¨ê°’ ì‹œê°í™” ê°œì„  - ì§ì ‘ êµ¬í˜„
                    st.subheader("Extreme Economic Indicators Visualization")
                    
                    # ìƒìœ„ ì§€í‘œ ì„ íƒ
                    top_indicators = economic_extremes['indicator'].value_counts().head(6).index.tolist()
                    num_indicators = len(top_indicators)
                    
                    # í–‰ê³¼ ì—´ ê³„ì‚° (2ê°œì˜ ì—´)
                    num_rows = (num_indicators + 1) // 2
                    
                    for row in range(num_rows):
                        cols = st.columns(2)
                        
                        for col_idx in range(2):
                            indicator_idx = row * 2 + col_idx
                            
                            if indicator_idx < num_indicators:
                                indicator = top_indicators[indicator_idx]
                                clean_indicator = indicator.replace('_norm', '')
                                
                                with cols[col_idx]:
                                    try:
                                        # ì§€í‘œì— í•´ë‹¹í•˜ëŠ” ê·¹ë‹¨ ì´ë²¤íŠ¸ ì¶”ì¶œ
                                        indicator_events = economic_extremes[economic_extremes['indicator'] == indicator]
                                        
                                        if not indicator_events.empty and daily_indicators is not None:
                                            # ì›ë³¸ ì§€í‘œ ë°ì´í„° ì¶”ì¶œ
                                            indicator_data = daily_indicators[indicator] if indicator in daily_indicators.columns else None
                                            
                                            if indicator_data is not None:
                                                fig, ax = plt.subplots(figsize=(8, 6))
                                                
                                                # ì§€í‘œ ë°ì´í„° í”Œë¡¯
                                                ax.plot(daily_indicators.index, indicator_data, label=clean_indicator, color='blue')
                                                
                                                # ê·¹ë‹¨ì  ì´ë²¤íŠ¸ í‘œì‹œ
                                                for _, event in indicator_events.iterrows():
                                                    event_date = event['date']
                                                    if event_date in daily_indicators.index:
                                                        value_at_event = indicator_data.loc[event_date]
                                                        event_type = event['event_type']
                                                        color = 'red' if event_type == 'Extreme High' else 'green'
                                                        
                                                        ax.scatter(event_date, value_at_event, color=color, s=80, zorder=5)
                                                        
                                                
                                                # 90% ë° 10% ë¶„ìœ„ìˆ˜ ë¼ì¸ ì¶”ê°€
                                                high_threshold = indicator_data.quantile(0.90)
                                                low_threshold = indicator_data.quantile(0.10)
                                                
                                                ax.axhline(y=high_threshold, color='red', linestyle='--', 
                                                        label=f'90% threshold ({high_threshold:.2f})')
                                                ax.axhline(y=low_threshold, color='green', linestyle='--', 
                                                        label=f'10% threshold ({low_threshold:.2f})')
                                                
                                                ax.set_title(f'{clean_indicator} Extreme Events', fontsize=14)
                                                ax.set_xlabel('Date', fontsize=12)
                                                ax.set_ylabel(clean_indicator, fontsize=12)
                                                ax.legend()
                                                ax.grid(True, alpha=0.3)
                                                
                                                plt.xticks(rotation=45)
                                                plt.tight_layout()
                                                
                                                st.pyplot(fig)
                                                plt.close()
                                            else:
                                                st.warning(f"No data found for indicator: {indicator}")
                                        else:
                                            st.info(f"No extreme events for {clean_indicator}")
                                    except Exception as e:
                                        st.error(f"Error visualizing {clean_indicator}: {str(e)}")
                    
                    # ì˜í–¥ ë¶„ì„ ì„¹ì…˜ - ë‘ ì»¬ëŸ¼ìœ¼ë¡œ í‘œì‹œ
                    st.subheader("Impact Analysis of Extreme Economic Indicators")
                    
                    # ê²½ì œ ì§€í‘œ ìœ í˜•ë³„ ì˜í–¥ ë¶„ì„
                    if economic_extremes is not None and not economic_extremes.empty and df is not None:
                        try:
                            # ì£¼ìš” ê²½ì œ ì§€í‘œ ê·¸ë£¹ ì„ íƒ
                            indicator_groups = []
                            for ind in economic_extremes['indicator'].unique():
                                base_name = ind.split('_')[0]  # '_norm' ì œê±°
                                if base_name not in indicator_groups:
                                    indicator_groups.append(base_name)
                            
                            # ìµœëŒ€ 4ê°œ ê·¸ë£¹ë§Œ ì„ íƒ
                            indicator_groups = indicator_groups[:min(4, len(indicator_groups))]
                            num_groups = len(indicator_groups)
                            rows_needed = (num_groups + 1) // 2
                            
                            for row in range(rows_needed):
                                cols = st.columns(2)
                                
                                for col_idx in range(2):
                                    group_idx = row * 2 + col_idx
                                    
                                    if group_idx < num_groups:
                                        base_ind = indicator_groups[group_idx]
                                        
                                        with cols[col_idx]:
                                            # í•´ë‹¹ ê²½ì œ ì§€í‘œì— ê´€ë ¨ëœ ëª¨ë“  ê·¹ë‹¨ ì´ë²¤íŠ¸ í•„í„°ë§
                                            related_events = economic_extremes[economic_extremes['indicator'].str.contains(base_ind)]
                                            
                                            if not related_events.empty:
                                                # ì´ë²¤íŠ¸ ìœ í˜•ë³„ ì§‘ê³„
                                                event_types = related_events['event_type'].value_counts()
                                                
                                                # ë°” ì°¨íŠ¸ë¡œ ì‹œê°í™”
                                                fig, ax = plt.subplots(figsize=(8, 6))
                                                event_types.plot(kind='bar', color=['red', 'green'], ax=ax)
                                                
                                                plt.title(f'{base_ind} Extreme Event Types', fontsize=14)
                                                plt.xlabel('Event Type', fontsize=12)
                                                plt.ylabel('Count', fontsize=12)
                                                plt.xticks(rotation=0)
                                                plt.tight_layout()
                                                
                                                st.pyplot(fig)
                                                plt.close()
                                                
                                                # ì´ë²¤íŠ¸ ì „í›„ ìˆ˜ìµë¥  ë¶„ì„
                                                if 'date' in related_events.columns and category_filter:
                                                    dates = related_events['date'].tolist()
                                                    
                                                    # ê° ì´ë²¤íŠ¸ ë‚ ì§œ ì „í›„ ìˆ˜ìµë¥  ê³„ì‚°
                                                    returns_before = []
                                                    returns_after = []
                                                    
                                                    for event_date in dates:
                                                        # ì´ë²¤íŠ¸ ì „ 5ì¼ ìˆ˜ìµë¥ 
                                                        start_before = event_date - pd.Timedelta(days=7)
                                                        price_data = df[(df['date'] >= start_before) & 
                                                                    (df['date'] <= event_date) & 
                                                                    (df['category'] == category_filter)]
                                                        
                                                        if len(price_data) >= 2:
                                                            start_price = price_data.iloc[0]['close']
                                                            end_price = price_data.iloc[-1]['close']
                                                            returns_before.append((end_price / start_price - 1) * 100)
                                                        
                                                        # ì´ë²¤íŠ¸ í›„ 5ì¼ ìˆ˜ìµë¥ 
                                                        end_after = event_date + pd.Timedelta(days=7)
                                                        price_data = df[(df['date'] >= event_date) & 
                                                                    (df['date'] <= end_after) & 
                                                                    (df['category'] == category_filter)]
                                                        
                                                        if len(price_data) >= 2:
                                                            start_price = price_data.iloc[0]['close']
                                                            end_price = price_data.iloc[-1]['close']
                                                            returns_after.append((end_price / start_price - 1) * 100)
                                                    
                                                    # ê²°ê³¼ ì‹œê°í™”
                                                    if returns_before and returns_after:
                                                        fig, ax = plt.subplots(figsize=(8, 6))
                                                        
                                                        # ë°•ìŠ¤í”Œë¡¯ìœ¼ë¡œ ë¹„êµ
                                                        box_data = [returns_before, returns_after]
                                                        ax.boxplot(box_data, labels=['Before Event', 'After Event'])
                                                        
                                                        # í¬ì¸íŠ¸ ì¶”ê°€
                                                        for i, data in enumerate([returns_before, returns_after], 1):
                                                            x = np.random.normal(i, 0.04, size=len(data))
                                                            ax.scatter(x, data, alpha=0.6)
                                                        
                                                        plt.title(f'{base_ind} Impact on {category_filter} Returns', fontsize=14)
                                                        plt.ylabel('Return (%)', fontsize=12)
                                                        plt.grid(True, alpha=0.3)
                                                        plt.tight_layout()
                                                        
                                                        st.pyplot(fig)
                                                        plt.close()
                                                    else:
                                                        st.info(f"Not enough price data to analyze returns for {base_ind}")
                                            else:
                                                st.info(f"No extreme events found for {base_ind}")
                        except Exception as e:
                            st.error(f"Error analyzing indicator impact: {str(e)}")
                    
                    # ê¸°ì¡´ í•¨ìˆ˜ í˜¸ì¶œ (ë°±ì—…)
                    # visualize_extreme_events(economic_extremes, None, daily_avg_prices, daily_indicators, category_filter)
                    # analyze_extreme_events_impact(df, economic_extremes, None, category_filter)
                else:
                    st.info("ê°ì§€ëœ ê²½ì œ ì§€í‘œ ê·¹ë‹¨ê°’ì´ ì—†ìŠµë‹ˆë‹¤.")
            else:
                st.info("ê°ì§€ëœ ê²½ì œ ì§€í‘œ ê·¹ë‹¨ê°’ì´ ì—†ìŠµë‹ˆë‹¤.")

        # íƒ­ 5: ìƒê´€ê´€ê³„ ë¶•ê´´
        with tab5:
            st.subheader("ğŸ”„ Correlation Breakdown")
            
            # ë””ë²„ê¹… ì •ë³´ë¥¼ ìˆ¨ê¸°ê±°ë‚˜ í™•ì¥ ê°€ëŠ¥í•œ ì„¹ì…˜ìœ¼ë¡œ ì •ë¦¬
            with st.expander("Debugging information"):
                st.write(f"correlation_breakdown_df exists: {correlation_breakdown_df is not None}")
                st.write(f"correlation_breakdown_df empty: {correlation_breakdown_df.empty if correlation_breakdown_df is not None else 'N/A'}")
                st.write(f"daily_avg_prices exists: {daily_avg_prices is not None}")
                st.write(f"daily_avg_prices empty: {daily_avg_prices.empty if daily_avg_prices is not None else 'N/A'}")
            
            if correlation_breakdown_df is not None and not correlation_breakdown_df.empty:
                # í•„í„°ë§ëœ ë°ì´í„°
                filtered_corr = correlation_breakdown_df
                if category_filter:
                    filtered_corr = correlation_breakdown_df[correlation_breakdown_df['pair'].str.contains(category_filter)]
                
                # í•„í„°ë§ í›„ ë°ì´í„° ìƒíƒœ í™•ì¸ (í™•ì¥ ê°€ëŠ¥í•œ ì„¹ì…˜ìœ¼ë¡œ ì´ë™)
                with st.expander("Filtered data information"):
                    st.write(f"filtered_corr empty: {filtered_corr.empty}")
                
                if not filtered_corr.empty:
                    # ìƒê´€ê´€ê³„ ë¶•ê´´ ì´ë²¤íŠ¸ í‘œì‹œ
                    st.write("Detected Correlation Breakdown:")
                    
                    st.dataframe(
                        filtered_corr.sort_values('date', ascending=False),
                        column_config={
                            "date": st.column_config.DatetimeColumn("date"),
                            "pair": st.column_config.TextColumn("pair"),
                            "event_type": st.column_config.TextColumn("event_type"),
                            "old_correlation": st.column_config.NumberColumn("old_correlation", format="%.2f"),
                            "new_correlation": st.column_config.NumberColumn("new_correlation", format="%.2f"),
                            "change": st.column_config.NumberColumn("change", format="%.2f"),
                            "description": st.column_config.TextColumn("description")
                        }
                    )
                    
                    # ìƒê´€ê´€ê³„ ë¶•ê´´ ì‹œê°í™”
                    if daily_avg_prices is not None and not daily_avg_prices.empty:
                        st.subheader("Main Correlation Breakdown Visualization")
                        
                        if 'change' in filtered_corr.columns:
                            # ë³€ê²½ëœ ë¶€ë¶„: ìƒìœ„ 4ê°œë¡œ ëŠ˜ë¦¬ê³  ê·¸ë¦¬ë“œ ë ˆì´ì•„ì›ƒ ì ìš©
                            top_changes = filtered_corr.sort_values('change', key=abs, ascending=False).head(4)
                            
                            # 2ê°œì”© ë‚˜ëˆ„ì–´ í‘œì‹œ
                            for i in range(0, len(top_changes), 2):
                                cols = st.columns(2)
                                
                                for j in range(2):
                                    if i + j < len(top_changes):
                                        row = top_changes.iloc[i + j]
                                        
                                        with cols[j]:
                                            pair = row['pair'].split('-')
                                            
                                            if len(pair) == 2:
                                                item1, item2 = pair
                                                date = row['date']
                                                
                                                # ë°ì´í„° ì¤€ë¹„ - ê°œì„ ëœ ë¡œì§
                                                series1 = None
                                                series2 = None
                                                
                                                # item1 ê²€ìƒ‰
                                                if item1 in daily_avg_prices.columns:
                                                    series1 = daily_avg_prices[item1]
                                                elif daily_indicators is not None:
                                                    if item1 in daily_indicators.columns:
                                                        series1 = daily_indicators[item1]
                                                    else:
                                                        # ë¶€ë¶„ ì¼ì¹˜ ê²€ìƒ‰ (GDP_norm -> GDP)
                                                        for col in daily_indicators.columns:
                                                            if item1 in col:
                                                                series1 = daily_indicators[col]
                                                                break
                                                
                                                # item2 ê²€ìƒ‰
                                                if item2 in daily_avg_prices.columns:
                                                    series2 = daily_avg_prices[item2]
                                                elif daily_indicators is not None:
                                                    if item2 in daily_indicators.columns:
                                                        series2 = daily_indicators[item2]
                                                    else:
                                                        # ë¶€ë¶„ ì¼ì¹˜ ê²€ìƒ‰
                                                        for col in daily_indicators.columns:
                                                            if item2 in col:
                                                                series2 = daily_indicators[col]
                                                                break
                                                
                                                # ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ ì‹œê°í™” ì§„í–‰
                                                if series1 is not None and series2 is not None:
                                                    # ìƒê´€ê´€ê³„ ì „í›„ 60ì¼ ê¸°ê°„
                                                    start_date = date - pd.Timedelta(days=60)
                                                    end_date = date + pd.Timedelta(days=60)
                                                    
                                                    # ë°ì´í„° í•„í„°ë§
                                                    mask1 = (series1.index >= start_date) & (series1.index <= end_date)
                                                    mask2 = (series2.index >= start_date) & (series2.index <= end_date)
                                                    
                                                    period_data1 = series1.loc[mask1]
                                                    period_data2 = series2.loc[mask2]
                                                    
                                                    # ê³µí†µ ë‚ ì§œ ì°¾ê¸°
                                                    common_dates = period_data1.index.intersection(period_data2.index)
                                                    
                                                    if len(common_dates) > 10:  # ì¶©ë¶„í•œ ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°
                                                        # ì‹œê³„ì—´ í‘œì¤€í™” (ë¹„êµ ê°€ëŠ¥í•˜ê²Œ)
                                                        normalized1 = (period_data1[common_dates] - period_data1[common_dates].mean()) / period_data1[common_dates].std()
                                                        normalized2 = (period_data2[common_dates] - period_data2[common_dates].mean()) / period_data2[common_dates].std()
                                                        
                                                        # ì°¨íŠ¸ ê·¸ë¦¬ê¸° - ê·¸ë˜í”„ í¬ê¸° ì¡°ì •
                                                        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 6), sharex=True)
                                                        
                                                        # ì‹œê³„ì—´ ì°¨íŠ¸
                                                        ax1.plot(common_dates, normalized1, label=item1)
                                                        ax1.plot(common_dates, normalized2, label=item2)
                                                        
                                                        # ìƒê´€ê´€ê³„ ë¶•ê´´ ë‚ ì§œ í‘œì‹œ
                                                        ax1.axvline(x=date, color='red', linestyle='--', label='Correlation Breakdown')
                                                        
                                                        # íƒ€ì´í‹€ ìˆ˜ì •
                                                        ax1.set_title(f'{item1} vs {item2}', fontsize=14)
                                                        ax1.set_ylabel('Normalized Value', fontsize=12)
                                                        ax1.legend()
                                                        ax1.grid(True, alpha=0.3)
                                                        
                                                        # ë¡¤ë§ ìƒê´€ê³„ìˆ˜
                                                        combined = pd.DataFrame({
                                                            item1: period_data1[common_dates],
                                                            item2: period_data2[common_dates]
                                                        })
                                                        
                                                        rolling_corr = combined[item1].rolling(window=30).corr(combined[item2])
                                                        rolling_corr = rolling_corr.fillna(0)
                                                        
                                                        ax2.plot(common_dates, rolling_corr, color='purple', linewidth=2)
                                                        ax2.axvline(x=date, color='red', linestyle='--')
                                                        ax2.axhline(y=0, color='black', linestyle='-', alpha=0.3)
                                                        
                                                        # ì´ì „ ë° ìƒˆ ìƒê´€ê³„ìˆ˜ í‘œì‹œ
                                                        ax2.axhline(y=row['old_correlation'], color='green', linestyle=':', 
                                                                    label=f'Old: {row["old_correlation"]:.2f}')
                                                        ax2.axhline(y=row['new_correlation'], color='blue', linestyle=':', 
                                                                    label=f'New: {row["new_correlation"]:.2f}')
                                                        
                                                        ax2.set_xlabel('Date', fontsize=12)
                                                        ax2.set_ylabel('30-Day Rolling Correlation', fontsize=12)
                                                        ax2.set_ylim(-1.1, 1.1)
                                                        ax2.legend()
                                                        ax2.grid(True, alpha=0.3)
                                                        
                                                        plt.xticks(rotation=45)
                                                        plt.tight_layout()
                                                        
                                                        st.pyplot(fig)
                                                        plt.close()
                                                    else:
                                                        st.warning(f"Not enough common data points for {item1} and {item2}")
                                                else:
                                                    st.warning(f"Data not found for {item1} or {item2}")
                                            else:
                                                st.warning(f"Invalid pair format: {row['pair']}")
                            
                            # ìƒê´€ê´€ê³„ ë³€í™” ë¶„ì„ ì¶”ê°€ ì„¹ì…˜
                            st.subheader("Correlation Change Analysis")
                            
                            col1, col2 = st.columns(2)
                            
                            with col1:
                                try:
                                    # ìƒê´€ê´€ê³„ ë³€í™” ë¶„í¬ ì‹œê°í™”
                                    fig, ax = plt.subplots(figsize=(8, 6))
                                    sns.histplot(filtered_corr['change'], bins=10, kde=True, ax=ax)
                                    
                                    plt.title('Distribution of Correlation Changes', fontsize=14)
                                    plt.xlabel('Correlation Change', fontsize=12)
                                    plt.ylabel('Frequency', fontsize=12)
                                    plt.axvline(x=0, color='red', linestyle='--')
                                    plt.grid(True, alpha=0.3)
                                    plt.tight_layout()
                                    
                                    st.pyplot(fig)
                                    plt.close()
                                except Exception as e:
                                    st.error(f"Error creating correlation change distribution: {str(e)}")
                            
                            with col2:
                                try:
                                    # ìƒê´€ê´€ê³„ ë³€í™” ì‹œê°„ ì¶”ì„¸
                                    if 'date' in filtered_corr.columns:
                                        filtered_corr = filtered_corr.sort_values('date')
                                        
                                        fig, ax = plt.subplots(figsize=(8, 6))
                                        ax.scatter(filtered_corr['date'], filtered_corr['change'], 
                                                c=filtered_corr['change'].apply(lambda x: 'red' if x < 0 else 'green'),
                                                alpha=0.7, s=50)
                                        
                                        # ì¶”ì„¸ì„  ì¶”ê°€
                                        z = np.polyfit(np.arange(len(filtered_corr)), filtered_corr['change'], 1)
                                        p = np.poly1d(z)
                                        ax.plot(filtered_corr['date'], p(np.arange(len(filtered_corr))), 
                                            "r--", linewidth=2, label=f'Trend: {z[0]:.4f}x + {z[1]:.4f}')
                                        
                                        plt.title('Correlation Changes Over Time', fontsize=14)
                                        plt.xlabel('Date', fontsize=12)
                                        plt.ylabel('Correlation Change', fontsize=12)
                                        plt.axhline(y=0, color='black', linestyle='-', alpha=0.3)
                                        plt.grid(True, alpha=0.3)
                                        plt.legend()
                                        plt.xticks(rotation=45)
                                        plt.tight_layout()
                                        
                                        st.pyplot(fig)
                                        plt.close()
                                    else:
                                        st.info("Date information not available for trend analysis")
                                except Exception as e:
                                    st.error(f"Error creating correlation trend visualization: {str(e)}")
                        else:
                            st.warning("'change' column not found in filtered_corr")
                    else:
                        st.warning("No daily average price data available for correlation breakdown visualization")
                else:
                    st.info(f"No correlation breakdown events found for the selected category '{category_filter}'")

        with tab6:
            st.subheader("ğŸ“„ Insight & Executive Report")
            
            # ì„¸ì…˜ì—ì„œ ë¨¸ì‹ ëŸ¬ë‹ ì˜ˆì¸¡ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
            ml_predictions = st.session_state.get('enhanced_model_data')
            
            # ë””ë²„ê¹… ì •ë³´
            #st.write(f"DEBUG: ë¨¸ì‹ ëŸ¬ë‹ ì˜ˆì¸¡ ê²°ê³¼ ì¡´ì¬ ì—¬ë¶€: {ml_predictions is not None}")
            #if ml_predictions is not None:
                #st.write(f"DEBUG: ì˜ˆì¸¡ ê²°ê³¼ ì¹´í…Œê³ ë¦¬: {list(ml_predictions.keys())}")
            
            # í˜„ì¬ ëª¨ë“œì—ì„œ ì‚¬ìš©í•  ê¸°ë³¸ model_data ì´ˆê¸°í™”
            # ê¸°ë³¸ model_data ìƒì„± (ì´ë¯¸ daily_avg_pricesê°€ ìˆë‹¤ê³  ê°€ì •)
            model_data = {}
            for category in daily_avg_prices.columns:
                model_data[category] = pd.DataFrame(index=daily_avg_prices.index)
                model_data[category]['close'] = daily_avg_prices[category]


            # extreme_enhanced_model_data ìƒì„±
            if 'extreme_events_df' in st.session_state and 'black_swan_df' in st.session_state:
                try:
                    with st.spinner("Creating enhanced data with extreme events..."):
                        # create_extreme_enhanced_data í•¨ìˆ˜ í˜¸ì¶œ
                        extreme_enhanced_model_data = create_extreme_enhanced_data(
                            model_data=model_data,  # ì´ˆê¸°í™”í•œ ê¸°ë³¸ ëª¨ë¸ ë°ì´í„°
                            extreme_events_df=st.session_state['extreme_events_df'],
                            black_swan_df=st.session_state['black_swan_df'],
                            technical_patterns_df=st.session_state.get('technical_patterns_df'),
                            correlation_breakdown_df=st.session_state.get('correlation_breakdown_df'),
                            predictions_data=ml_predictions  # ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë“œì—ì„œ ì €ì¥í•œ ì˜ˆì¸¡ ê²°ê³¼
                        )
                        
                        if extreme_enhanced_model_data:
                            st.success("Enhanced data with extreme events created successfully!")
                            
                            # ìš”ì•½ ìƒì„± - generate_executive_summary í•¨ìˆ˜ í˜¸ì¶œ
                            summary = generate_executive_summary(
                                extreme_events_df=st.session_state['extreme_events_df'],
                                black_swan_df=st.session_state['black_swan_df'],
                                technical_patterns_df=st.session_state.get('technical_patterns_df'),
                                correlation_breakdown_df=st.session_state.get('correlation_breakdown_df'),
                                daily_avg_prices=daily_avg_prices,
                                daily_indicators=daily_indicators,
                                extreme_enhanced_model_data=extreme_enhanced_model_data,  # ì¤‘ìš”: ì—¬ê¸°ì— ìƒì„±ëœ extreme_enhanced_model_dataë¥¼ ì „ë‹¬
                                categories=daily_avg_prices.columns.tolist()
                            )
                            
                            # ìš”ì•½ ëŒ€ì‹œë³´ë“œ í‘œì‹œ
                            display_executive_dashboard(
                                summary=summary,
                                daily_avg_prices=daily_avg_prices,
                                daily_indicators=daily_indicators,
                                black_swan_df=st.session_state['black_swan_df'],
                                extreme_events_df=st.session_state['extreme_events_df']
                            )
                        else:
                            st.error("Failed to create enhanced data with extreme events.")
                except Exception as e:
                    st.error(f"Error generating insights and report: {str(e)}")
                    st.exception(e)
            else:
                st.info("Please detect extreme events first to generate insights and report.")
import os
import pandas as pd
import streamlit as st
import matplotlib.pyplot as plt
from PIL import Image
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import numpy as np
from scipy.stats import norm
def generate_model_results_dashboard():
    """ëª¨ë¸ í•™ìŠµ ê²°ê³¼ë¥¼ ì¢…í•©ì ìœ¼ë¡œ ë³´ì—¬ì£¼ëŠ” ëŒ€ì‹œë³´ë“œ"""
    st.header("ğŸ§® ML Model Result Dashboard")
    
    # ì„¸ì…˜ ìƒíƒœì—ì„œ ì„±ëŠ¥ ë°ì´í„° í™•ì¸
    if ('model_results' not in st.session_state or 
        not st.session_state['model_results'] or 
        'enhanced_model_data' not in st.session_state):
        st.error("âš ï¸ No model performance data available. Please run the analysis first.")
        return
    
    # ëª¨ë¸ ê²°ê³¼ì—ì„œ ì„±ëŠ¥ ë°ì´í„° ì¶”ì¶œ
    model_results = st.session_state['model_results']
    categories = list(model_results.keys())
    
    # ì„±ëŠ¥ ë°ì´í„° í”„ë ˆì„ ìƒì„±
    performance_data = []
    for category, result in model_results.items():
        if result:  # ê²°ê³¼ê°€ ìˆëŠ” ê²½ìš°ë§Œ ì²˜ë¦¬
            performance_data.append({
                "Category": category,
                "Model Type": result.get('model_name', 'Unknown'),
                "Accuracy": result.get('accuracy', 0),
                "Optimal Threshold Accuracy": result.get('accuracy_optimal', 0),
                "ROC-AUC": result.get('roc_auc', 0),
                "Prediction Confidence": result.get('roc_auc', 0) * 0.5 + result.get('accuracy_optimal', 0) * 0.5  # ì˜ˆì¸¡ ì‹ ë¢°ë„ ì ìˆ˜
            })
    
    # ì„±ëŠ¥ ë°ì´í„°í”„ë ˆì„ ìƒì„±
    performance_df = pd.DataFrame(performance_data)
    
    # ì„±ëŠ¥ ë°ì´í„°í”„ë ˆì„ ì €ì¥
    st.session_state['full_performance'] = performance_df
    
    # ì „ì²´ ì„±ëŠ¥ í…Œì´ë¸”
    st.subheader("ğŸ“‹ Overall Model Performance Summary")
    st.dataframe(performance_df.style.format({
        "Accuracy": "{:.2%}",
        "Optimal Threshold Accuracy": "{:.2%}",
        "ROC-AUC": "{:.2%}",
        "Prediction Confidence": "{:.2%}"
    }))
    
    # ì„±ëŠ¥ ë¹„êµ ë°” ì°¨íŠ¸
    try:
        st.subheader("ğŸ“Š Model Performance Comparison Across Categories")
        fig, ax = plt.subplots(figsize=(10, 6))
        chart_df = performance_df.melt(
            id_vars=["Category", "Model Type"],
            value_vars=["Accuracy", "Optimal Threshold Accuracy", "ROC-AUC"],
            var_name="Metric",
            value_name="Score"
        )
        import seaborn as sns
        sns.barplot(x="Category", y="Score", hue="Metric", data=chart_df, ax=ax)
        plt.title("Category-wise Model Performance", fontsize=16)
        plt.ylim(0, 1)
        plt.xticks(rotation=45)
        plt.tight_layout()
        st.pyplot(fig)
        plt.close()
    except Exception as e:
        st.error(f"Error generating performance chart: {e}")
    
    st.markdown("---")
    
    # ì¹´í…Œê³ ë¦¬ë³„ ì„¸ë¶€ ê²°ê³¼
    st.subheader("ğŸ” Category-wise Detailed Results")
    selected_category = st.selectbox("Select a category to view detailed results", categories)
    
    if selected_category and selected_category in model_results:
        # ì‹œê°í™” ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        if 'visualizations' in st.session_state and selected_category in model_results:
            col1, col2 = st.columns(2)
            
            with col1:
                st.markdown(f"### ğŸ¯ Feature Importance - {selected_category}")
                if f"{selected_category}_feature_importance" in st.session_state['visualizations']:
                    st.pyplot(st.session_state['visualizations'][f"{selected_category}_feature_importance"])
                else:
                    st.info("No feature importance visualization available.")
                
                st.markdown(f"### ğŸ“ˆ ROC Curve - {selected_category}")
                if f"{selected_category}roc_curve" in st.session_state['visualizations']:
                    st.pyplot(st.session_state['visualizations'][f"{selected_category}roc_curve"])
                else:
                    st.info("No ROC curve visualization available.")
            
            with col2:
                st.markdown(f"### ğŸ§© Confusion Matrix - {selected_category}")
                if f"{selected_category}_Confusion Matrix" in st.session_state['visualizations']:
                    st.pyplot(st.session_state['visualizations'][f"{selected_category}_Confusion Matrix"])
                else:
                    st.info("No confusion matrix visualization available.")
                
                st.markdown(f"### ğŸ§® Prediction Probability Distribution - {selected_category}")
                if f"{selected_category}probability_distribution" in st.session_state['visualizations']:
                    st.pyplot(st.session_state['visualizations'][f"{selected_category}probability_distribution"])
                else:
                    st.info("No probability distribution visualization available.")
        else:
            # ì„¸ì…˜ì— ì‹œê°í™” ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš° íŒŒì¼ ì‹œë„
            output_dir = "ML_results"  # íŒŒì¼ì´ ì €ì¥ëœ ë””ë ‰í† ë¦¬
            category_dir = os.path.join(output_dir, 'categories', selected_category)
            
            if os.path.exists(category_dir):
                col1, col2 = st.columns(2)
                
                with col1:
                    st.markdown(f"### ğŸ¯ Feature Importance - {selected_category}")
                    feature_img_path = os.path.join(category_dir, 'feature_importance.png')
                    if os.path.exists(feature_img_path):
                        st.image(feature_img_path)
                    else:
                        st.info("No feature importance image available.")
                    
                    st.markdown(f"### ğŸ“ˆ ROC Curve - {selected_category}")
                    roc_img_path = os.path.join(category_dir, 'roc_curve.png')
                    if os.path.exists(roc_img_path):
                        st.image(roc_img_path)
                    else:
                        st.info("No ROC curve image available.")
                
                with col2:
                    st.markdown(f"### ğŸ§© Confusion Matrix - {selected_category}")
                    cm_img_path = os.path.join(category_dir, 'confusion_matrix.png')
                    if os.path.exists(cm_img_path):
                        st.image(cm_img_path)
                    else:
                        st.info("No confusion matrix image available.")
                    
                    st.markdown(f"### ğŸ§® Prediction Probability Distribution - {selected_category}")
                    prob_img_path = os.path.join(category_dir, 'probability_distribution.png')
                    if os.path.exists(prob_img_path):
                        st.image(prob_img_path)
                    else:
                        st.info("No probability distribution image available.")
                
                # Temporal trend ì¶”ê°€
                st.markdown(f"### â³ Temporal Prediction Trend - {selected_category}")
                temporal_img_path = os.path.join(category_dir, 'temporal_prediction.png')
                if os.path.exists(temporal_img_path):
                    st.image(temporal_img_path)
                else:
                    st.info("No temporal trend image available.")
            else:
                st.warning(f"No results directory found for {selected_category}")
    else:
        st.info("Please select a category to view detailed results")

def calculate_correlations(data, category, feature_cols, num_indicators):
    """
    ì£¼ì–´ì§„ ë°ì´í„°ì—ì„œ íŠ¹ì • ì¹´í…Œê³ ë¦¬ì™€ ë‹¤ë¥¸ ë³€ìˆ˜ ê°„ì˜ ìƒê´€ê´€ê³„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    
    Parameters:
    -----------
    data : pandas.DataFrame
        ìƒê´€ê´€ê³„ë¥¼ ê³„ì‚°í•  ë°ì´í„°í”„ë ˆì„
    category : str
        ìƒê´€ê´€ê³„ë¥¼ ê³„ì‚°í•  ëŒ€ìƒ ì¹´í…Œê³ ë¦¬(ì»¬ëŸ¼)
    feature_cols : list
        ìƒê´€ê´€ê³„ë¥¼ ê³„ì‚°í•  íŠ¹ì„± ì»¬ëŸ¼ ëª©ë¡
    num_indicators : int
        ë°˜í™˜í•  ìƒìœ„ ìƒê´€ê´€ê³„ ìˆ˜
    
    Returns:
    --------
    pandas.Series
        ìƒìœ„ Nê°œ ìƒê´€ê´€ê³„ (ì ˆëŒ€ê°’ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ë¨)
    """
    try:
        # ì¹´í…Œê³ ë¦¬ ì»¬ëŸ¼ì´ ë°ì´í„°ì— ìˆëŠ”ì§€ í™•ì¸
        if category not in data.columns:
            return pd.Series()
            
        # íŠ¹ì„± ì»¬ëŸ¼ì´ ë°ì´í„°ì— ìˆëŠ”ì§€ í™•ì¸
        valid_features = [col for col in feature_cols if col in data.columns]
        if not valid_features:
            return pd.Series()
        
        # ìƒê´€ê´€ê³„ ê³„ì‚°
        correlations = data[valid_features].corrwith(data[category]).abs().sort_values(ascending=False)
        
        # ìƒìœ„ Nê°œ ë°˜í™˜
        return correlations.head(num_indicators)
    except Exception as e:
        print(f"ìƒê´€ê´€ê³„ ê³„ì‚° ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return pd.Series()
def generate_market_insights_dashboard(enhanced_model_data, daily_avg_prices, daily_indicators):
    """
    ì‹œì¥ ì¸ì‚¬ì´íŠ¸ë¥¼ ë³´ì—¬ì£¼ëŠ” ëŒ€ì‹œë³´ë“œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    ê° ì¹´í…Œê³ ë¦¬ë§ˆë‹¤ ë³„ë„ì˜ íƒ­ì„ ì œê³µí•˜ê³  ì˜ˆì¸¡ ê¸°ëŠ¥ì„ ê°œì„ í•©ë‹ˆë‹¤.
    
    Args:
        enhanced_model_data: í–¥ìƒëœ ëª¨ë¸ ë°ì´í„° (ì¹´í…Œê³ ë¦¬ë³„ ë°ì´í„°í”„ë ˆì„)
        daily_avg_prices: ì¼ë³„ í‰ê·  ê°€ê²© ë°ì´í„°
        daily_indicators: ì¼ë³„ ì§€í‘œ ë°ì´í„°
    """
    st.header("ğŸ§  Market Insights Dashboard")

    if 'enhanced_model_data' not in st.session_state:
        st.session_state['enhanced_model_data'] = enhanced_model_data
    if not enhanced_model_data:
        st.error("No model data available. Please run the analysis first.")
        return
    
    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”: í˜„ì¬ íŒŒë¼ë¯¸í„° ì €ì¥
    if 'market_dashboard' not in st.session_state:
        st.session_state['market_dashboard'] = {}
    
    # ì¹´í…Œê³ ë¦¬ë³„ ë°ì´í„° ìºì‹± (ê³„ì‚° ê²°ê³¼ ì €ì¥)
    if 'category_cache' not in st.session_state:
        st.session_state['category_cache'] = {}
    
    # ì¹´í…Œê³ ë¦¬ë³„ ì‹œì¥ ì¸ì‚¬ì´íŠ¸ ì €ì¥ì„ ìœ„í•œ ì„¸ì…˜ ìƒíƒœ í™•ì¸
    if 'market_insights' not in st.session_state:
        st.session_state['market_insights'] = {}

    # 3. ì¹´í…Œê³ ë¦¬ íƒ­ ê´€ë ¨ ì„¸ì…˜ ì´ˆê¸°í™”
    categories = list(enhanced_model_data.keys())
    if not categories:
        st.warning("No categories available for analysis.")
        return

    if 'selected_category_tab' not in st.session_state:
        st.session_state['selected_category_tab'] = 0

    selected_tab = st.session_state['selected_category_tab']
    category_tabs = st.tabs([f"ğŸ“Š {category}" for category in categories])

    # 4. ê° ì¹´í…Œê³ ë¦¬ íƒ­ ë‚´ë¶€
    for i, category in enumerate(categories):
        with category_tabs[i]:
            # ğŸ”¥ ì—¬ê¸°ì„œë¶€í„° category ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤

            if category not in st.session_state['market_dashboard']:
                st.session_state['market_dashboard'][category] = {
                    'date_range': None,
                    'ma_days': 20,
                    'num_indicators': 10
                }

            if category not in st.session_state['market_insights']:
                st.session_state['market_insights'][category] = {}

            if 'selected_dashboard_tab' not in st.session_state:
                st.session_state['selected_dashboard_tab'] = {}

            if category not in st.session_state['selected_dashboard_tab']:
                st.session_state['selected_dashboard_tab'][category] = 0

            # 5. ë°ì´í„° ì²´í¬
            category_data = enhanced_model_data[category]
            if category_data is None or len(category_data) == 0:
                st.warning(f"No data available for {category}")
                continue

            if 'date' not in category_data.columns and isinstance(category_data.index, pd.DatetimeIndex):
                category_data = category_data.reset_index()
                category_data.rename(columns={'index': 'date'}, inplace=True)

            # ê° ì¹´í…Œê³ ë¦¬ë³„ ë¶„ì„ íƒ­ ìƒì„±
            dashboard_tabs = st.tabs([
                "ğŸ“ˆ Price Analysis", 
                "ğŸ”„ Correlation Analysis", 
                "ğŸ”® Prediction Analysis",
                "ğŸ’¡ Insights Summary"
            ])
            selected_tab_index = st.session_state.get('selected_dashboard_tab', 0)
            # 3. ê°€ê²© ë¶„ì„ íƒ­
            with dashboard_tabs[0]:
                st.subheader(f"ğŸ“ˆ Price Trend Analysis: {category}")
                
                try:
                    # ê°€ê²© ë°ì´í„° ì¶”ì¶œ
                    if category in daily_avg_prices.columns:
                        price_series = daily_avg_prices[category].dropna()
                        
                        # ì„¸ì…˜ì— ê°€ê²© ë°ì´í„° ì €ì¥
                        st.session_state['market_insights'][category]['price_data'] = price_series.to_dict()
                        
                        # ë‚ ì§œ ë²”ìœ„ ì„ íƒê¸°
                        if not price_series.empty and isinstance(price_series.index, pd.DatetimeIndex):
                            min_date = price_series.index.min().date()
                            max_date = price_series.index.max().date()
                            
                            # ë‚ ì§œ ë²”ìœ„ ê¸°ë³¸ê°’ ì„¤ì •
                            col1, col2 = st.columns(2)
                            with col1:
                                start_date = st.date_input(
                                    "ğŸ“… Start Date",
                                    value=max(min_date, max_date - pd.Timedelta(days=180)),
                                    min_value=min_date,
                                    max_value=max_date,
                                    key=f"start_date_{category}"
                                )
                            with col2:
                                end_date = st.date_input(
                                    "ğŸ“… End Date",
                                    value=max_date,
                                    min_value=min_date,
                                    max_value=max_date,
                                    key=f"end_date_{category}"
                                )
                            
                            # ì„¸ì…˜ì— ë‚ ì§œ ë²”ìœ„ ì €ì¥
                            date_range = (start_date, end_date)
                            st.session_state['market_dashboard'][category]['date_range'] = date_range
                            
                            # ë‚ ì§œ ë²”ìœ„ í•„í„°ë§
                            mask = (price_series.index.date >= start_date) & (price_series.index.date <= end_date)
                            filtered_prices = price_series[mask]
                            
                            # ìºì‹œ í‚¤ ìƒì„± (ì¹´í…Œê³ ë¦¬, ë‚ ì§œ ë²”ìœ„, MA ìœˆë„ìš°)
                            cache_key = f"{category}_price_{start_date}_{end_date}"
                            
                            # ì´ë™ í‰ê·  ìœˆë„ìš°
                            ma_days = st.slider(
                                "ğŸ“Š Moving Average Window (days)", 
                                5, 50, 
                                st.session_state['market_dashboard'][category]['ma_days'],
                                key=f"ma_days_slider_{category}"
                            )
                            st.session_state['market_dashboard'][category]['ma_days'] = ma_days
                            
                            # í•„í„°ë§ëœ ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°
                            if not filtered_prices.empty:
                                # ìºì‹œì— ì—†ìœ¼ë©´ ê³„ì‚°í•˜ê³  ì €ì¥
                                if cache_key not in st.session_state['category_cache']:
                                    price_stats = compute_price_statistics(filtered_prices)
                                    st.session_state['category_cache'][cache_key] = price_stats
                                else:
                                    price_stats = st.session_state['category_cache'][cache_key]
                                
                                # ê°€ê²© ì°¨íŠ¸ì™€ í†µê³„ í‘œì‹œë¥¼ ìœ„í•œ ì—´ ë ˆì´ì•„ì›ƒ
                                price_chart_col, price_stats_col = st.columns([3, 1])
                                
                                with price_chart_col:
                                    # ê°€ê²© íŠ¸ë Œë“œ ì°¨íŠ¸ ìƒì„±
                                    price_chart_fig = create_price_trend_chart(
                                        filtered_prices, 
                                        category, 
                                        ma_days
                                    )
                                    st.plotly_chart(price_chart_fig, use_container_width=True)
                                    
                                with price_stats_col:
                                    # ê°€ê²© í†µê³„ í‘œì‹œ
                                    current_price = filtered_prices.iloc[-1]
                                    price_change = (filtered_prices.iloc[-1] - filtered_prices.iloc[0]) / filtered_prices.iloc[0]
                                    volatility = filtered_prices.pct_change().std() * (252 ** 0.5)
                                    sharpe = (filtered_prices.pct_change().mean() / filtered_prices.pct_change().std()) * (252 ** 0.5) if filtered_prices.pct_change().std() != 0 else 0
                                    
                                    st.metric("Current Price", f"{current_price:.2f}")
                                    st.metric("Period Change", f"{price_change:.2%}", 
                                            delta=f"{price_change:.2%}", 
                                            delta_color="normal")
                                    st.metric("Volatility (Annual)", f"{volatility:.2%}")
                                    st.metric("Sharpe Ratio", f"{sharpe:.2f}")
                                    
                                    # í†µê³„ ì •ë³´ ì„¸ì…˜ì— ì €ì¥
                                    st.session_state['market_insights'][category]['stats'] = {
                                        'current_price': float(current_price),
                                        'price_change': float(price_change),
                                        'volatility': float(volatility),
                                        'sharpe': float(sharpe)
                                    }
                                
                                # ì¶”ê°€ ë¶„ì„: ê°€ê²© ë¶„í¬ ë° ìˆ˜ìµë¥  ë¶„í¬
                                st.subheader("ğŸ“Š Price & Return Distribution")
                                dist_col1, dist_col2 = st.columns(2)
                                
                                with dist_col1:
                                    # ê°€ê²© ë¶„í¬ íˆìŠ¤í† ê·¸ë¨
                                    price_hist_fig = create_price_histogram(filtered_prices, category)
                                    st.plotly_chart(price_hist_fig, use_container_width=True)
                                    
                                with dist_col2:
                                    # ìˆ˜ìµë¥  ë¶„í¬ íˆìŠ¤í† ê·¸ë¨
                                    returns = filtered_prices.pct_change().dropna()
                                    returns_hist_fig = create_returns_histogram(returns, category)
                                    st.plotly_chart(returns_hist_fig, use_container_width=True)
                            else:
                                st.warning("No price data available for the selected date range")
                        else:
                            st.warning("Price data is empty or does not have a proper datetime index")
                    else:
                        st.warning(f"No price data available for {category}")
                except Exception as e:
                    st.error(f"Error while analyzing price trends: {str(e)}")
            
            # 4. ìƒê´€ê´€ê³„ ë¶„ì„ íƒ­
            with dashboard_tabs[1]:
                if 'selected_dashboard_tab' not in st.session_state:
                    st.session_state['selected_dashboard_tab'] = {}
                    st.session_state['selected_dashboard_tab'][category] = 1 
                st.subheader(f"ğŸ”„ Indicator Correlation Analysis: {category}")
                
                try:
                    # ì„ íƒëœ ë²”ì£¼ì˜ ë°ì´í„°ì—ì„œ ê´€ë ¨ ì§€í‘œ ì°¾ê¸°
                    feature_cols = [col for col in category_data.columns if (
                        col != category and 
                        col != 'date' and 
                        col != 'predictions' and
                        not col.startswith('lag_')
                    )]
                    
                    if not feature_cols:
                        st.warning("No indicator features found for correlation analysis")
                        return
            
                    # ìƒìœ„ ìƒê´€ê´€ê³„ ì§€í‘œ ì„ íƒ
                    # ê°œì„ ëœ ì½”ë“œ - ìŠ¬ë¼ì´ë” ìƒíƒœ ê´€ë¦¬
                    if 'market_dashboard' not in st.session_state:
                        st.session_state['market_dashboard'] = {}
                    
                    if category not in st.session_state['market_dashboard']:
                        st.session_state['market_dashboard'][category] = {
                            'num_indicators': 10  # ê¸°ë³¸ê°’
                        }
                    
                    slider_key = f"num_indicators_slider_{category}"
                    
                    with st.form(key=f"indicator_form_{category}"):
                        num_indicators = st.slider(
                            "ğŸ“ Number of Indicators to Show", 
                            5, min(20, len(feature_cols)), 
                            st.session_state['market_dashboard'][category]['num_indicators'],
                            key=slider_key
                        )
                        
                        submit_button = st.form_submit_button("Apply")
                        
                        if submit_button:
                            st.session_state['market_dashboard'][category]['num_indicators'] = num_indicators
                            st.session_state['selected_dashboard_tab'] = 1  # Correlation Analysis íƒ­
                            st.experimental_rerun()
                        # ê°’ì´ ë³€ê²½ë˜ë©´ í˜ì´ì§€ ìƒˆë¡œê³ ì¹¨
                     # ë¼ì´ë” ê²°ê³¼(num_indicators)ë¥¼ êº¼ë‚´ì˜¤ëŠ” ê±°ì•¼.
                    num_indicators = st.session_state['market_dashboard'][category]['num_indicators']
                    # ìƒê´€ê´€ê³„ ê³„ì‚° - feature_cols ì •ì˜ í›„ì— ê³„ì‚°
                    if category in category_data.columns:
                        # ìºì‹œ í‚¤ ìƒì„±
                        sorted_features = sorted(feature_cols)
                        feature_hash = hash(str(sorted_features))
                        corr_cache_key = f"{category}_corr_{num_indicators}_{hash(str(sorted(feature_cols)))}"

                        if 'category_cache' not in st.session_state:
                            st.session_state['category_cache'] = {}

                        # num_indicatorsê¹Œì§€ í‚¤ì— ë°˜ì˜í•´ ìƒˆë¡œ ì²´í¬
                        need_recompute = (corr_cache_key not in st.session_state['category_cache'])

                        if need_recompute:
                            with st.spinner(f"Calculating correlations for {category}..."):
                                top_correlations = calculate_correlations(category_data, category, feature_cols, num_indicators)
                                if not top_correlations.empty:
                                    st.session_state['category_cache'][corr_cache_key] = top_correlations.copy()
                                    st.success(f"Correlation calculation completed for {category}")
                                else:
                                    st.warning("No correlations found")
                        else:
                            top_correlations = st.session_state['category_cache'][corr_cache_key].copy()
                      
                        # ìƒê´€ê´€ê³„ ê²°ê³¼ê°€ ìˆëŠ” ê²½ìš°ë§Œ ì‹œê°í™”
                        if not top_correlations.empty:
                            # ì„¸ì…˜ì— ìƒê´€ê´€ê³„ ë°ì´í„° ì €ì¥
                            correlation_dict = top_correlations.to_dict()
                            
                            if 'market_insights' not in st.session_state:
                                st.session_state['market_insights'] = {}
                            if category not in st.session_state['market_insights']:
                                st.session_state['market_insights'][category] = {}
                            
                            st.session_state['market_insights'][category]['correlations'] = correlation_dict
                            
                            # ìƒê´€ê´€ê³„ ì‹œê°í™”
                            corr_chart_col, corr_details_col = st.columns([3, 2])
                            
                            with corr_chart_col:
                                corr_fig = create_correlation_chart(top_correlations, category)
                                st.plotly_chart(corr_fig, use_container_width=True)
                            
                            with corr_details_col:
                                st.markdown("### ğŸ“Š Top Correlated Indicators")
                                for i, (feature, corr_value) in enumerate(top_correlations.items()):
                                    corr_color = "green" if corr_value > 0.7 else ("orange" if corr_value > 0.5 else "gray")
                                    st.markdown(f"{i+1}. **{feature}**: <span style='color:{corr_color};font-weight:bold'>{corr_value:.3f}</span>", unsafe_allow_html=True)
                            
                            # ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ
                            st.markdown("### ğŸ”¥ Correlation Heatmap")
                            
                            # ìƒìœ„ ì§€í‘œì™€ ê°€ê²© ë°ì´í„° ê²°í•©
                            heatmap_cols = top_correlations.index.tolist() + [category]
                            if all(col in category_data.columns for col in heatmap_cols):
                                # íˆíŠ¸ë§µ ìºì‹œ í‚¤
                                heatmap_cache_key = f"{category}_heatmap_{num_indicators}_{hash(str(feature_cols))}"
                                
                                # ìºì‹œì— ì—†ìœ¼ë©´ ê³„ì‚°
                                if heatmap_cache_key not in st.session_state['category_cache']:
                                    corr_matrix = category_data[heatmap_cols].corr()
                                    st.session_state['category_cache'][heatmap_cache_key] = corr_matrix
                                else:
                                    corr_matrix = st.session_state['category_cache'][heatmap_cache_key]
                                
                                # ì„¸ì…˜ì— ìƒê´€ê´€ê³„ í–‰ë ¬ ì €ì¥
                                st.session_state['market_insights'][category]['correlation_matrix'] = corr_matrix.to_dict()
                                
                                # Plotly íˆíŠ¸ë§µ ìƒì„±
                                heatmap_fig = create_correlation_heatmap(corr_matrix, category)
                                st.plotly_chart(heatmap_fig, use_container_width=True)
                                
                                # ì¸ì‚¬ì´íŠ¸ ìƒì„±
                                st.subheader("ğŸ’¡ Correlation Insights")
                                pos_correlations = top_correlations[top_correlations > 0.5]
                                neg_correlations = top_correlations[top_correlations < -0.5]
                                
                                if not pos_correlations.empty:
                                    st.info(f"**Positive Relationships**: {category} shows strong positive correlation with {', '.join(pos_correlations.index[:3])}. These indicators tend to move in the same direction as the asset price.")
                                    
                                if not neg_correlations.empty:
                                    st.warning(f"**Negative Relationships**: {category} shows strong negative correlation with {', '.join(neg_correlations.index[:3])}. These indicators tend to move in the opposite direction to the asset price.")
                            else:
                                st.warning("Some selected columns are not available in the data")
                        else:
                            st.warning("No significant correlations found")
                    else:
                        st.warning(f"{category} column not found in the data")
                except Exception as e:
                    st.error(f"Error during correlation analysis: {str(e)}")
            
            # 5. ì˜ˆì¸¡ ë¶„ì„ íƒ­
            with dashboard_tabs[2]:
                st.subheader(f"ğŸ”® Prediction Pattern Analysis: {category}")
                
                try:
                    # ì˜ˆì¸¡ ê²°ê³¼ê°€ ìˆëŠ”ì§€ í™•ì¸
                    prediction_data = category_data.copy()
                    
                    # prediction_data ê²€ì‚¬
                    has_predictions = False
                    if 'predictions' in prediction_data.columns:
                        if prediction_data['predictions'].notna().sum() > 0:
                            has_predictions = True
                            st.success(f"Found {prediction_data['predictions'].notna().sum()} valid predictions in the data.")
                    
                    # ì˜ˆì¸¡ê°’ ì—†ìœ¼ë©´ ìƒì„± (ë””ë²„ê¹… ì •ë³´ ì¶œë ¥)
                    if not has_predictions:
                        st.warning("No predictions found or all are NaN. Checking data structure...")
                        
                        # ë””ë²„ê¹… ì •ë³´
                        st.expander("Debugging Information").write({
                            "Columns in category_data": category_data.columns.tolist(),
                            "Has predictions column": 'predictions' in category_data.columns,
                            "Valid predictions count": category_data['predictions'].notna().sum() if 'predictions' in category_data.columns else 0,
                            "Has price column": category in category_data.columns,
                            "Data shape": category_data.shape,
                            "First few rows": category_data.head().to_dict() if not category_data.empty else "Empty DataFrame"
                        })
                        
                        # ì˜ˆì¸¡ ë°ì´í„° ìƒì„± (ê°€ìƒ ì˜ˆì¸¡)
                        st.info("Generating synthetic predictions for demonstration...")
                        
                        if category in prediction_data.columns:
                            # ì‹¤ì œ ë°©í–¥ì„ ê¸°ë°˜ìœ¼ë¡œ ì˜ˆì¸¡ ìƒì„± (ì•½ê°„ì˜ ì˜¤ì°¨ ì¶”ê°€)
                            actual_dir = (prediction_data[category].diff() > 0).astype(int)
                            
                            # 80% ì •í™•ë„ë¥¼ ê°€ì§„ ê°€ìƒ ì˜ˆì¸¡ ìƒì„±
                            np.random.seed(42)  # ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œ ì„¤ì •
                            random_flip = np.random.random(len(actual_dir)) > 0.8  # 20% í™•ë¥ ë¡œ ë’¤ì§‘ê¸°
                            prediction_data['predictions'] = np.where(random_flip, 1 - actual_dir, actual_dir)
                            
                            # NaN ì²˜ë¦¬
                            prediction_data['predictions'] = prediction_data['predictions'].fillna(0)
                            has_predictions = True
                    
                    if has_predictions:
                        # ì˜ˆì¸¡ ë°ì´í„° ì²˜ë¦¬
                        if 'date' in prediction_data.columns or isinstance(prediction_data.index, pd.DatetimeIndex):
                            # ë‚ ì§œ í˜•ì‹ í™•ì¸ ë° ì²˜ë¦¬
                            if 'date' in prediction_data.columns and not pd.api.types.is_datetime64_any_dtype(prediction_data['date']):
                                prediction_data['date'] = pd.to_datetime(prediction_data['date'])
                                prediction_data.set_index('date', inplace=True)
                            elif not isinstance(prediction_data.index, pd.DatetimeIndex) and 'date' not in prediction_data.columns:
                                st.warning("No date information available for prediction analysis")
                                return
                            
                            # ìµœê·¼ ì˜ˆì¸¡ ê²°ê³¼ (ë§ˆì§€ë§‰ 30ê°œ ë˜ëŠ” ì „ì²´ ì¤‘ ì‘ì€ ê°’)
                            recent_count = min(30, len(prediction_data))
                            recent_predictions = prediction_data.iloc[-recent_count:]
                            
                            pred_chart_col, pred_metrics_col = st.columns([3, 1])
                            
                            with pred_chart_col:
                                # ì˜ˆì¸¡ íŠ¸ë Œë“œ ì°¨íŠ¸ (Plotlyë¡œ ê°œì„ )
                                if category in recent_predictions.columns and 'predictions' in recent_predictions.columns:
                                    pred_fig = create_prediction_chart(recent_predictions, category)
                                    st.plotly_chart(pred_fig, use_container_width=True)
                            
                            with pred_metrics_col:
                                # ì˜ˆì¸¡ ì •í™•ë„ ë° ì§€í‘œ
                                if 'predictions' in recent_predictions.columns and category in recent_predictions.columns:
                                    # ì‹¤ì œ ë°©í–¥ ê³„ì‚° (ë‹¹ì¼ ì¢…ê°€ì™€ ì „ì¼ ì¢…ê°€ ë¹„êµ)
                                    actual_direction = (recent_predictions[category].diff() > 0).astype(int)
                                    # NaN ê°’ ì²˜ë¦¬
                                    actual_direction = actual_direction.fillna(0)
                                    
                                    # ì •í™•ë„ ê³„ì‚° (NaN ê°’ ì œì™¸)
                                    valid_indices = (~actual_direction.isna()) & (~recent_predictions['predictions'].isna())
                                    if valid_indices.sum() > 0:
                                        correct_predictions = (actual_direction[valid_indices] == recent_predictions['predictions'][valid_indices]).mean()
                                    else:
                                        correct_predictions = 0
                                    
                                    # ì„¸ì…˜ì— ìµœê·¼ ì˜ˆì¸¡ ì •í™•ë„ ì €ì¥
                                    st.session_state['market_insights'][category]['recent_accuracy'] = float(correct_predictions)
                                    
                                    st.metric("Recent Accuracy", f"{correct_predictions:.2%}")
                                    
                                    # ìƒìŠ¹/í•˜ë½ ì˜ˆì¸¡ ë¹„ìœ¨
                                    up_ratio = recent_predictions['predictions'].mean()
                                    st.metric("Up Prediction Ratio", f"{up_ratio:.2%}")
                                    
                                    # ìµœê·¼ ì˜ˆì¸¡ ë°©í–¥
                                    latest_pred = "Up" if recent_predictions['predictions'].iloc[-1] == 1 else "Down"
                                    pred_color = "green" if latest_pred == "Up" else "red"
                                    st.markdown(f"**Latest Prediction**: <span style='color:{pred_color};font-weight:bold'>{latest_pred}</span>", unsafe_allow_html=True)
                            
                            # ì˜ˆì¸¡ ì„±ê³¼ ì§€í‘œ
                            st.subheader("ğŸ“Š Prediction Performance Analysis")
                            
                            if 'predictions' in prediction_data.columns and category in prediction_data.columns:
                                perf_col1, perf_col2 = st.columns(2)
                                
                                with perf_col1:
                                    # ì‹¤ì œ ë°©í–¥ê³¼ ì˜ˆì¸¡ ë°©í–¥ ê³„ì‚°
                                    # ìˆ˜ì •: ì‹¤ì œ ë°©í–¥ì€ ì „ì¼ ëŒ€ë¹„ ë³€í™”ë¡œ ê³„ì‚°
                                    actual_direction = (prediction_data[category].diff() > 0).astype(int)
                                    actual_direction = actual_direction.fillna(0)  # ì²« í–‰ì˜ NaN ì²˜ë¦¬
                                    
                                    # í˜¼ë™ í–‰ë ¬ ê³„ì‚° (ì „ì²´ ë°ì´í„°ì…‹ ê¸°ì¤€)
                                    actual_up = actual_direction.sum()
                                    actual_down = len(actual_direction) - actual_up
                                    predicted_up = prediction_data['predictions'].sum()
                                    predicted_down = len(prediction_data['predictions']) - predicted_up
                                    
                                    # ì˜ˆì¸¡ ì„±ëŠ¥ ì§€í‘œ
                                    valid_indices = (~actual_direction.isna()) & (~prediction_data['predictions'].isna())
                                    if valid_indices.sum() > 0:
                                        correct_predictions = (actual_direction[valid_indices] == prediction_data['predictions'][valid_indices]).mean()
                                    else:
                                        correct_predictions = 0
                                    
                                    # ì •ë°€ë„ ê³„ì‚° (ì˜ˆì¸¡ì´ ìƒìŠ¹ì¼ ë•Œ ì‹¤ì œë¡œ ìƒìŠ¹ì¸ ë¹„ìœ¨)
                                    up_indices = prediction_data['predictions'] == 1
                                    if up_indices.sum() > 0:
                                        up_precision = (actual_direction[up_indices] == 1).mean()
                                    else:
                                        up_precision = 0
                                    
                                    # ì •ë°€ë„ ê³„ì‚° (ì˜ˆì¸¡ì´ í•˜ë½ì¼ ë•Œ ì‹¤ì œë¡œ í•˜ë½ì¸ ë¹„ìœ¨)
                                    down_indices = prediction_data['predictions'] == 0
                                    if down_indices.sum() > 0:
                                        down_precision = (actual_direction[down_indices] == 0).mean()
                                    else:
                                        down_precision = 0
                                    
                                    # ì„¸ì…˜ì— ì˜ˆì¸¡ ì„±ëŠ¥ ì§€í‘œ ì €ì¥
                                    st.session_state['market_insights'][category]['prediction_metrics'] = {
                                        'accuracy': float(correct_predictions),
                                        'up_precision': float(up_precision),
                                        'down_precision': float(down_precision),
                                        'coverage': float(len(prediction_data[~prediction_data['predictions'].isna()]) / len(prediction_data))
                                    }
                                    
                                    # ì„±ëŠ¥ ì§€í‘œ í‘œì‹œ
                                    metrics_cols = st.columns(2)
                                    with metrics_cols[0]:
                                        st.metric("Overall Accuracy", f"{correct_predictions:.2%}")
                                        st.metric("Up Precision", f"{up_precision:.2%}")
                                    with metrics_cols[1]:
                                        st.metric("Down Precision", f"{down_precision:.2%}")
                                        st.metric("Prediction Coverage", f"{len(prediction_data[~prediction_data['predictions'].isna()]) / len(prediction_data):.2%}")
                                    
                                    # ì‹¤ì œ vs ì˜ˆì¸¡ ì°¨íŠ¸ (Plotlyë¡œ ê°œì„ )
                                    labels = ['Down', 'Up']
                                    actual_counts = [actual_down, actual_up]
                                    predicted_counts = [predicted_down, predicted_up]
                                    
                                    # NaN ê°’ ì²˜ë¦¬
                                    actual_counts = np.nan_to_num(actual_counts)
                                    predicted_counts = np.nan_to_num(predicted_counts)
                                    
                                    direction_fig = create_direction_counts_chart(labels, actual_counts, predicted_counts)
                                    st.plotly_chart(direction_fig, use_container_width=True)
                                
                                with perf_col2:
                                    # í˜¼ë™ í–‰ë ¬ ì‹œê°í™”
                                    from sklearn.metrics import confusion_matrix
                                    
                                    # ìœ íš¨í•œ ì¸ë±ìŠ¤ë§Œ ì‚¬ìš©
                                    valid_mask = ~actual_direction.isna() & ~prediction_data['predictions'].isna()
                                    if valid_mask.sum() > 1:  # ìµœì†Œ 2ê°œ ì´ìƒì˜ ë°ì´í„° í•„ìš”
                                        # í˜¼ë™ í–‰ë ¬ ê³„ì‚°
                                        cm = confusion_matrix(
                                            actual_direction[valid_mask], 
                                            prediction_data['predictions'][valid_mask]
                                        )
                                        
                                        # í˜¼ë™ í–‰ë ¬ ì‹œê°í™”
                                        cm_fig = create_confusion_matrix_chart(cm, category)
                                        st.plotly_chart(cm_fig, use_container_width=True)
                                    else:
                                        st.warning("Not enough valid data for confusion matrix")
                                    
                                    # ì‹œê°„ ê²½ê³¼ì— ë”°ë¥¸ ì •í™•ë„ ì¶”ì´
                                    if len(prediction_data) > 10:
                                        # ìœˆë„ìš° í¬ê¸° ê³„ì‚° (ìµœì†Œ 10ê°œ ë°ì´í„° í¬ì¸íŠ¸)
                                        window_size = max(10, len(prediction_data) // 10)
                                        
                                        # ë¡¤ë§ ì •í™•ë„ ê³„ì‚° (ìœ íš¨í•œ ê°’ë§Œ ì‚¬ìš©)
                                        valid_mask = ~actual_direction.isna() & ~prediction_data['predictions'].isna()
                                        if valid_mask.sum() > window_size:
                                            # ì •í™•/ë¶€ì •í™• ì‹œë¦¬ì¦ˆ ìƒì„± (1: ì •í™•, 0: ë¶€ì •í™•)
                                            accuracy_series = pd.Series(
                                                (actual_direction == prediction_data['predictions']).astype(float),
                                                index=prediction_data.index
                                            )
                                            
                                            # ë¡¤ë§ ì •í™•ë„ ê³„ì‚°
                                            rolling_accuracy = accuracy_series.rolling(window=window_size).mean()
                                            
                                            # ì •í™•ë„ ì¶”ì´ ì°¨íŠ¸
                                            accuracy_trend_fig = create_accuracy_trend_chart(rolling_accuracy, window_size, category)
                                            st.plotly_chart(accuracy_trend_fig, use_container_width=True)
                                        else:
                                            st.warning(f"Not enough valid data points for rolling accuracy (need at least {window_size} valid points)")
                            else:
                                st.warning("Prediction data not available for performance metrics")
                        else:
                            st.warning("No date information available for the prediction analysis")
                    else:
                        st.info("No valid predictions found. Run the analysis to generate predictions.")
                except Exception as e:
                    st.error(f"Error during prediction pattern analysis: {str(e)}")
                    st.exception(e)
            
            # 6. ì¸ì‚¬ì´íŠ¸ ìš”ì•½ íƒ­
            with dashboard_tabs[3]:
                st.subheader(f"ğŸ’¡ Market Insights Summary: {category}")
                
                try:
                    if 'predictions' in category_data.columns and category in category_data.columns:
                        # ìµœê·¼ 20ê±°ë˜ì¼ ì˜ˆì¸¡ ë°ì´í„°
                        recent_data = category_data.iloc[-20:].copy() if len(category_data) >= 20 else category_data.copy()
                        
                        # ì‹¤ì œ ë°©í–¥ê³¼ ì˜ˆì¸¡ ë°©í–¥
                        if category in recent_data.columns:
                            recent_data['actual_direction'] = np.where(recent_data[category].diff() > 0, 'Up', 'Down')
                        
                        if 'predictions' in recent_data.columns:
                            recent_data['predicted_direction'] = np.where(recent_data['predictions'] == 1, 'Up', 'Down')
                        
                        # ì„¸ì…˜ì— ìš”ì•½ ë°ì´í„° ì €ì¥
                        if 'market_insights' not in st.session_state:
                            st.session_state['market_insights'] = {}
                        if category not in st.session_state['market_insights']:
                            st.session_state['market_insights'][category] = {}
                        if 'recent_summary' not in st.session_state['market_insights'][category]:
                            st.session_state['market_insights'][category]['recent_summary'] = {}
                        
                        # ìµœê·¼ ì˜ˆì¸¡ ìš”ì•½
                        if 'predicted_direction' in recent_data.columns:
                            # ë°ì´í„° ë¶„ì„ ê²°ê³¼ ìš”ì•½
                            summary_col1, summary_col2 = st.columns([2, 1])
                            
                            with summary_col1:
                                # ì „ì²´ ì¸ì‚¬ì´íŠ¸ ìš”ì•½ ì¹´ë“œ
                                st.markdown("""
                                <style>
                                .insight-card {
                                    border: 1px solid #ddd;
                                    border-radius: 5px;
                                    padding: 20px;
                                    background-color: #f9f9f9;
                                    margin-bottom: 20px;
                                    color: black;
                                }
                                .insight-title {
                                    font-size: 18px;
                                    font-weight: bold;
                                    margin-bottom: 10px;
                                    color: black;
                                }
                                </style>
                                """, unsafe_allow_html=True)
                                
                                latest_prediction = recent_data['predicted_direction'].iloc[-1] if not recent_data.empty else 'Unknown'
                                prediction_trend = recent_data['predicted_direction'].value_counts().idxmax() if not recent_data.empty else 'Unknown'
                                
                                # ì„¸ì…˜ì— ìµœê·¼ ì˜ˆì¸¡ ìš”ì•½ ì €ì¥
                                st.session_state['market_insights'][category]['recent_summary']['latest_prediction'] = latest_prediction
                                st.session_state['market_insights'][category]['recent_summary']['prediction_trend'] = prediction_trend
                                
                                trend_color = "green" if prediction_trend == "Up" else "red"
                                trend_icon = "ğŸ“ˆ" if prediction_trend == "Up" else "ğŸ“‰"
                                
                                st.markdown(f"""
                                <div class="insight-card">
                                    <div class="insight-title">{trend_icon} Market Trend Analysis</div>
                                    <p>The model indicates a <span style='color:{trend_color};font-weight:bold'>{prediction_trend.lower()}</span> trend for <b>{category}</b>. Recent predictions suggest {prediction_trend.lower()}ward price movements are more likely in the near term.</p>
                                    <p>Latest prediction: <span style='color:{trend_color};font-weight:bold'>{latest_prediction}</span></p>
                                </div>
                                """, unsafe_allow_html=True)
                                
                                # ì„±ëŠ¥ ì¸ì‚¬ì´íŠ¸
                                if 'prediction_metrics' in st.session_state['market_insights'][category]:
                                    metrics = st.session_state['market_insights'][category]['prediction_metrics']
                                    accuracy = metrics.get('accuracy', 0)
                                    reliability = "High" if accuracy > 0.7 else ("Moderate" if accuracy > 0.5 else "Low")
                                    rel_color = "green" if reliability == "High" else ("orange" if reliability == "Moderate" else "red")
                                    
                                    st.markdown(f"""
                                    <div class="insight-card">
                                        <div class="insight-title">ğŸ¯ Model Reliability Assessment</div>
                                        <p>Model reliability: <span style='color:{rel_color};font-weight:bold'>{reliability}</span> ({accuracy:.2%} accuracy)</p>
                                        <p>The model shows {metrics.get('up_precision', 0):.2%} precision for upward movements and {metrics.get('down_precision', 0):.2%} precision for downward movements.</p>
                                    </div>
                                    """, unsafe_allow_html=True)
                                
                                # ìƒê´€ê´€ê³„ ì¸ì‚¬ì´íŠ¸
                                if 'correlations' in st.session_state['market_insights'][category]:
                                    correlations = pd.Series(st.session_state['market_insights'][category]['correlations'])
                                    top_pos = correlations[correlations > 0].nlargest(3)
                                    top_neg = correlations[correlations < 0].nsmallest(3)
                                    
                                    corr_html = "<div class='insight-card'><div class='insight-title'>ğŸ”„ Key Correlations</div><p>"
                                    
                                    if not top_pos.empty:
                                        corr_html += f"<b>Top positive indicators</b>: "
                                        for i, (ind, val) in enumerate(top_pos.items()):
                                            corr_html += f"{ind} ({val:.2f})" + (", " if i < len(top_pos) - 1 else "")
                                    
                                    if not top_neg.empty:
                                        corr_html += f"</p><p><b>Top negative indicators</b>: "
                                        for i, (ind, val) in enumerate(top_neg.items()):
                                            corr_html += f"{ind} ({val:.2f})" + (", " if i < len(top_neg) - 1 else "")
                                    
                                    corr_html += "</p></div>"
                                    st.markdown(corr_html, unsafe_allow_html=True)
                            
                            with summary_col2:
                                # ìµœê·¼ 5ì¼ ì˜ˆì¸¡ ë™í–¥
                                st.markdown("### ğŸ“… Recent Predictions")
                                
                                last_5_days = recent_data.iloc[-5:]['predicted_direction'].tolist() if len(recent_data) >= 5 else recent_data['predicted_direction'].tolist()
                                
                                # ì„¸ì…˜ì— ìµœê·¼ 5ì¼ ì˜ˆì¸¡ ë™í–¥ ì €ì¥
                                st.session_state['market_insights'][category]['recent_summary']['last_5_days'] = last_5_days
                                
                                # ìµœê·¼ ì˜ˆì¸¡ ì‹œê°í™” - í–¥ìƒëœ ë””ìì¸
                                for i, pred in enumerate(reversed(last_5_days)):
                                    days_ago = len(last_5_days) - 1 - i
                                    day_text = "Today" if days_ago == 0 else f"{days_ago} day{'s' if days_ago > 1 else ''} ago"
                                    icon = "ğŸ“ˆ" if pred == "Up" else "ğŸ“‰"
                                    color = "green" if pred == "Up" else "red"
                                    bg_color = "#e6ffe6" if pred == "Up" else "#ffe6e6"
                                    
                                    st.markdown(f"""
                                    <div style="padding: 10px; margin-bottom: 5px; border-radius: 5px; background-color: {bg_color}; display: flex; justify-content: space-between; color: black;">
                                        <span style="font-weight: bold;">{day_text}</span>
                                        <span>{icon} <span style="color: {color}; font-weight: bold;">{pred}</span></span>
                                    </div>
                                    """, unsafe_allow_html=True)
                                
                                # ì¶”ê°€ í†µê³„ ì •ë³´
                                if 'stats' in st.session_state['market_insights'][category]:
                                    stats = st.session_state['market_insights'][category]['stats']
                                    
                                    st.markdown("### ğŸ“Š Price Statistics")
                                    st.markdown(f"""
                                    <div style="padding: 10px; margin-bottom: 5px; border-radius: 5px; background-color: #f0f0f0; color: black;">
                                        <span style="font-weight: bold; color: black;">Current Price: </span> {stats.get('current_price', 0):.2f}
                                    </div>
                                    <div style="padding: 10px; margin-bottom: 5px; border-radius: 5px; background-color: #f0f0f0; color: black;">
                                        <span style="font-weight: bold; color: black;">Period Change:</span> {stats.get('price_change', 0):.2%}
                                    </div>
                                    <div style="padding: 10px; margin-bottom: 5px; border-radius: 5px; background-color: #f0f0f0; color: black;">
                                        <span style="font-weight: bold; color: black;">Volatility:</span> {stats.get('volatility', 0):.2%}
                                    </div>
                                    """, unsafe_allow_html=True)
                            
                            # ì˜ˆì¸¡ ìº˜ë¦°ë” ì‹œê°í™”
                            st.subheader("ğŸ“† Prediction Calendar")
                            
                            # ìµœê·¼ 30ì¼ ì˜ˆì¸¡ì„ ìº˜ë¦°ë” í˜•ì‹ìœ¼ë¡œ í‘œì‹œ
                            if len(recent_data) > 0 and 'predicted_direction' in recent_data.columns:
                                calendar_data = recent_data.iloc[-30:] if len(recent_data) >= 30 else recent_data
                                calendar_fig = create_prediction_calendar(calendar_data, category)
                                st.plotly_chart(calendar_fig, use_container_width=True)
                            
                            # ìì‚° ì„±ê³¼ ë¹„êµ ì°¨íŠ¸
                            if 'price_data' in st.session_state['market_insights'][category] and len(daily_avg_prices.columns) > 1:
                                st.subheader("ğŸ’¹ Asset Performance Comparison")
                                
                                # ë¹„êµí•  ìì‚° ì„ íƒ (ìµœëŒ€ 3ê°œ)
                                other_assets = [cat for cat in daily_avg_prices.columns if cat != category]
                                if other_assets:
                                    compare_assets = st.multiselect(
                                        "Select assets to compare",
                                        other_assets,
                                        default=other_assets[:min(2, len(other_assets))],
                                        key=f"compare_assets_{category}"
                                    )
                                    
                                    if compare_assets:
                                        # ë¹„êµ ì°¨íŠ¸ ìƒì„±
                                        comparison_fig = create_asset_comparison_chart(
                                            daily_avg_prices, 
                                            [category] + compare_assets,
                                            start_date if 'market_dashboard' in st.session_state and category in st.session_state['market_dashboard'] and st.session_state['market_dashboard'][category]['date_range'] else None,
                                            end_date if 'market_dashboard' in st.session_state and category in st.session_state['market_dashboard'] and st.session_state['market_dashboard'][category]['date_range'] else None
                                        )
                                        st.plotly_chart(comparison_fig, use_container_width=True)
                        else:
                            st.info("No recent predictions available for trend analysis")
                    else:
                        st.info("No prediction data available. Run the analysis to generate predictions and insights.")
                except Exception as e:
                    st.error(f"Error generating market insights summary: {str(e)}")
                    st.exception(e)
    
    # ì„¸ì…˜ ìƒíƒœì— í˜„ì¬ ì¹´í…Œê³ ë¦¬ ì—…ë°ì´íŠ¸
    if categories:
        st.session_state['current_category'] = categories[0]



# í—¬í¼ í•¨ìˆ˜: ê°€ê²© í†µê³„ ê³„ì‚°
def compute_price_statistics(price_series):
    """ê°€ê²© ì‹œê³„ì—´ì— ëŒ€í•œ í†µê³„ ê³„ì‚°"""
    if len(price_series) < 2:
        return {}
    
    # ìˆ˜ìµë¥  ê³„ì‚°
    returns = price_series.pct_change().dropna()
    
    # ê¸°ë³¸ í†µê³„
    stats = {
        'current_price': float(price_series.iloc[-1]),
        'start_price': float(price_series.iloc[0]),
        'price_change': float((price_series.iloc[-1] - price_series.iloc[0]) / price_series.iloc[0]),
        'min_price': float(price_series.min()),
        'max_price': float(price_series.max()),
        'mean_price': float(price_series.mean()),
        'std_price': float(price_series.std()),
        
        # ìˆ˜ìµë¥  í†µê³„
        'mean_return': float(returns.mean()),
        'std_return': float(returns.std()),
        'skew_return': float(returns.skew()) if len(returns) > 2 else 0,
        'kurtosis_return': float(returns.kurtosis()) if len(returns) > 3 else 0,
        
        # ë³€ë™ì„± (ì—°ê°„í™”)
        'volatility': float(returns.std() * (252 ** 0.5)),
        
        # ì„±ê³¼ ì§€í‘œ
        'sharpe': float((returns.mean() / returns.std()) * (252 ** 0.5)) if returns.std() != 0 else 0,
    }
    
    return stats

# í—¬í¼ í•¨ìˆ˜: Plotly ê°€ê²© íŠ¸ë Œë“œ ì°¨íŠ¸ ìƒì„±
def create_price_trend_chart(price_series, category, ma_days=20):
    """Plotlyë¥¼ ì‚¬ìš©í•œ ê°€ê²© íŠ¸ë Œë“œ ì°¨íŠ¸ ìƒì„±"""
    import plotly.graph_objects as go
    from plotly.subplots import make_subplots
    
    # ì„œë¸Œí”Œë¡¯ ìƒì„± (ê°€ê²© ë° ê±°ë˜ëŸ‰ í‘œì‹œìš©)
    fig = make_subplots(rows=2, cols=1, shared_xaxes=True, 
                         vertical_spacing=0.02, row_heights=[0.7, 0.3],
                         subplot_titles=(f"{category} Price Trend", "Returns"))
    
    # ê°€ê²© íŠ¸ë Œë“œ ë¼ì¸
    fig.add_trace(
        go.Scatter(
            x=price_series.index, 
            y=price_series.values,
            mode='lines',
            name='Price',
            line=dict(color='royalblue', width=2)
        ),
        row=1, col=1
    )
    
    # ì´ë™ í‰ê· ì„  ì¶”ê°€
    if len(price_series) > ma_days:
        ma = price_series.rolling(window=ma_days).mean()
        fig.add_trace(
            go.Scatter(
                x=ma.index, 
                y=ma.values,
                mode='lines',
                name=f'{ma_days}-day MA',
                line=dict(color='firebrick', width=1.5, dash='dot')
            ),
            row=1, col=1
        )
    
    # ìˆ˜ìµë¥  ë°” ì°¨íŠ¸
    returns = price_series.pct_change().dropna()
    colors = ['green' if x >= 0 else 'red' for x in returns.values]
    
    fig.add_trace(
        go.Bar(
            x=returns.index,
            y=returns.values * 100,  # í¼ì„¼íŠ¸ë¡œ ë³€í™˜
            name='Daily Returns (%)',
            marker_color=colors
        ),
        row=2, col=1
    )
    
    # ì°¨íŠ¸ ë ˆì´ì•„ì›ƒ ì„¤ì •
    fig.update_layout(
        height=600,
        template='plotly_white',
        title=f"{category} Price Analysis",
        legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1),
        xaxis_rangeslider_visible=False,
        hovermode="x unified"
    )
    
    # Yì¶• ë ˆì´ë¸” ì„¤ì •
    fig.update_yaxes(title_text="Price", row=1, col=1)
    fig.update_yaxes(title_text="Returns (%)", row=2, col=1)
    
    return fig
# í—¬í¼ í•¨ìˆ˜: ê°€ê²© íˆìŠ¤í† ê·¸ë¨ ìƒì„±
def create_price_histogram(price_series, category):
    """Plotlyë¥¼ ì‚¬ìš©í•œ ê°€ê²© ë¶„í¬ íˆìŠ¤í† ê·¸ë¨ ìƒì„±"""
    import plotly.graph_objects as go
    
    fig = go.Figure()
    
    fig.add_trace(
        go.Histogram(
            x=price_series.values,
            nbinsx=30,
            name='Price Distribution',
            marker_color='royalblue',
            opacity=0.7
        )
    )
    
    # í˜„ì¬ ê°€ê²© í‘œì‹œ ë¼ì¸
    current_price = price_series.iloc[-1]
    
    fig.add_vline(
        x=current_price,
        line_dash="dash",
        line_color="firebrick",
        annotation_text=f"Current: {current_price:.2f}",
        annotation_position="top right"
    )
    
    # í‰ê·  ê°€ê²© í‘œì‹œ ë¼ì¸
    mean_price = price_series.mean()
    
    fig.add_vline(
        x=mean_price,
        line_dash="dot",
        line_color="green",
        annotation_text=f"Mean: {mean_price:.2f}",
        annotation_position="top left"
    )
    
    fig.update_layout(
        title=f"{category} Price Distribution",
        xaxis_title="Price",
        yaxis_title="Frequency",
        template='plotly_white',
        bargap=0.01
    )
    
    return fig

# í—¬í¼ í•¨ìˆ˜: ìˆ˜ìµë¥  íˆìŠ¤í† ê·¸ë¨ ìƒì„±
def create_returns_histogram(returns, category):
    """Plotlyë¥¼ ì‚¬ìš©í•œ ìˆ˜ìµë¥  ë¶„í¬ íˆìŠ¤í† ê·¸ë¨ ìƒì„±"""
    import plotly.graph_objects as go
    import numpy as np
    
    fig = go.Figure()
    
    # ìˆ˜ìµë¥ ì„ í¼ì„¼íŠ¸ë¡œ ë³€í™˜
    returns_pct = returns * 100
    
    fig.add_trace(
        go.Histogram(
            x=returns_pct.values,
            nbinsx=30,
            name='Returns Distribution',
            marker_color='royalblue',
            opacity=0.7
        )
    )
    
    # ì •ê·œ ë¶„í¬ ì˜¤ë²„ë ˆì´
    x = np.linspace(returns_pct.min(), returns_pct.max(), 100)
    mean = returns_pct.mean()
    std = returns_pct.std()
    
    # ì •ê·œ ë¶„í¬ PDF ê³„ì‚°
    from scipy.stats import norm
    pdf = norm.pdf(x, mean, std)
    pdf = pdf * (returns_pct.count() * (returns_pct.max() - returns_pct.min()) / 30)
    
    fig.add_trace(
        go.Scatter(
            x=x,
            y=pdf,
            mode='lines',
            name='Normal Distribution',
            line=dict(color='firebrick', dash='dash'),
        )
    )
    
    # 0 ë¼ì¸ ì¶”ê°€
    fig.add_vline(
        x=0,
        line_dash="dash",
        line_color="black",
        annotation_text="Zero Return",
        annotation_position="top right"
    )
    
    fig.update_layout(
        title=f"{category} Daily Returns Distribution (%)",
        xaxis_title="Returns (%)",
        yaxis_title="Frequency",
        template='plotly_white',
        bargap=0.01
    )
    
    return fig

# í—¬í¼ í•¨ìˆ˜: ìƒê´€ê´€ê³„ ì°¨íŠ¸ ìƒì„±
def create_correlation_chart(correlations, category):
    """Plotlyë¥¼ ì‚¬ìš©í•œ ìƒê´€ê´€ê³„ ë°” ì°¨íŠ¸ ìƒì„±"""
    import plotly.graph_objects as go
    
    # ìƒê´€ê´€ê³„ ê°’ì— ë”°ë¼ ìƒ‰ìƒ ì§€ì •
    colors = ['green' if x >= 0 else 'red' for x in correlations.values]
    
    # ë°ì´í„° ì •ë ¬
    sorted_indices = correlations.abs().argsort().values
    sorted_features = correlations.index[sorted_indices]
    sorted_correlations = correlations.values[sorted_indices]
    sorted_colors = [colors[i] for i in sorted_indices]
    
    fig = go.Figure()
    
    fig.add_trace(
        go.Bar(
            y=sorted_features,
            x=sorted_correlations,
            orientation='h',
            marker_color=sorted_colors,
            text=[f"{x:.3f}" for x in sorted_correlations],
            textposition='auto'
        )
    )
    
    fig.update_layout(
        title=f"Top Correlations with {category}",
        xaxis_title="Correlation Coefficient",
        yaxis_title="Indicator",
        template='plotly_white',
        height=500
    )
    
    return fig

# í—¬í¼ í•¨ìˆ˜: ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ ìƒì„±
def create_correlation_heatmap(corr_matrix, category):
    """Plotlyë¥¼ ì‚¬ìš©í•œ ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ ìƒì„±"""
    import plotly.graph_objects as go
    
    fig = go.Figure(data=go.Heatmap(
        z=corr_matrix.values,
        x=corr_matrix.columns,
        y=corr_matrix.index,
        colorscale='RdBu_r',  # ë¹¨ê°•-íŒŒë‘ ìƒ‰ìƒ ìŠ¤ì¼€ì¼ (ì—­ë°©í–¥)
        zmid=0,  # 0ì„ ì¤‘ê°„ê°’ìœ¼ë¡œ ì„¤ì •
        text=corr_matrix.round(2).values,
        texttemplate="%{text}",
        textfont={"size":10},
        hoverongaps=False
    ))
    
    fig.update_layout(
        height=600,
        template='plotly_white',
        xaxis={'side': 'top'},
        font=dict( size= 14)  # xì¶• ë ˆì´ë¸”ì„ ìœ„ìª½ì— í‘œì‹œ
    )
    
    return fig

# í—¬í¼ í•¨ìˆ˜: ì˜ˆì¸¡ ì°¨íŠ¸ ìƒì„±
def create_prediction_chart(recent_predictions, category):
    """Plotlyë¥¼ ì‚¬ìš©í•œ ì˜ˆì¸¡ ì°¨íŠ¸ ìƒì„±"""
    import plotly.graph_objects as go
    from plotly.subplots import make_subplots
    
    fig = make_subplots(specs=[[{"secondary_y": True}]])
    
    # ì‹¤ì œ ê°€ê²© ë³€í™”
    if category in recent_predictions.columns:
        price_changes = recent_predictions[category]
        
        # ìƒ‰ìƒ ì„¤ì • (ìƒìŠ¹/í•˜ë½ì— ë”°ë¼)
        colors = ['green' if x >= 0 else 'red' for x in price_changes]
        
        fig.add_trace(
            go.Bar(
                x=recent_predictions.index,
                y=price_changes,
                name='Actual Price Change',
                marker_color=colors,
                opacity=0.7
            ),
            secondary_y=False
        )
    
    # ì˜ˆì¸¡ ê²°ê³¼ (ì„ í˜• ì°¨íŠ¸)
    if 'predictions' in recent_predictions.columns:
        fig.add_trace(
            go.Scatter(
                x=recent_predictions.index,
                y=recent_predictions['predictions'],
                mode='lines+markers',
                name='Prediction (1=Up, 0=Down)',
                line=dict(color='black', width=2),
                marker=dict(
                    size=8,
                    color=['green' if p == 1 else 'red' for p in recent_predictions['predictions']],
                    line=dict(width=1, color='black')
                )
            ),
            secondary_y=True
        )
    
    # ë ˆì´ì•„ì›ƒ ì„¤ì •
    fig.update_layout(
        title=f"Recent Predictions for {category}",
        template='plotly_white',
        hovermode="x unified",
        legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="center", x=0.5)
    )
    
    # ì¶• ë ˆì´ë¸” ì„¤ì •
    fig.update_yaxes(title_text="Price Change", secondary_y=False)
    fig.update_yaxes(title_text="Prediction (1=Up, 0=Down)", secondary_y=True,
                    range=[-0.1, 1.1], tickvals=[0, 1], ticktext=['Down', 'Up'])
    fig.update_xaxes(title_text="Date")
    
    return fig
# í—¬í¼ í•¨ìˆ˜: ì˜ˆì¸¡ ìº˜ë¦°ë” ì‹œê°í™”
def create_prediction_calendar(calendar_data, category):
    """Plotlyë¥¼ ì‚¬ìš©í•œ ì˜ˆì¸¡ ìº˜ë¦°ë” ì‹œê°í™”"""
    import plotly.graph_objects as go
    import pandas as pd
    
    # ë°ì´í„° ì¤€ë¹„
    if not isinstance(calendar_data.index, pd.DatetimeIndex):
        if 'date' in calendar_data.columns:
            calendar_data = calendar_data.set_index('date')
        else:
            return go.Figure()  # ë‚ ì§œ ì •ë³´ê°€ ì—†ìœ¼ë©´ ë¹ˆ ê·¸ë¦¼ ë°˜í™˜
    
    # ë‚ ì§œë¥¼ ì£¼ ë‹¨ìœ„ë¡œ êµ¬ì„±
    calendar_data = calendar_data.copy()
    calendar_data['day_of_week'] = calendar_data.index.dayofweek
    calendar_data['week'] = calendar_data.index.isocalendar().week
    
    # ì£¼ë³„ë¡œ ê·¸ë£¹í™”
    weeks = calendar_data['week'].unique()
    
    fig = go.Figure()
    
    # ìš”ì¼ ë° ì£¼ ë ˆì´ë¸”
    day_labels = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']
    week_labels = [f"Week {w}" for w in weeks]
    
    # ê° ì…€ì˜ ìƒ‰ìƒ ë° í…ìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„
    text = []
    colors = []
    dates = []
    
    # ê° ì£¼ì— ëŒ€í•´
    for week in weeks:
        week_data = calendar_data[calendar_data['week'] == week]
        
        # ê° ìš”ì¼ì— ëŒ€í•´
        for day in range(7):
            day_data = week_data[week_data['day_of_week'] == day]
            
            if not day_data.empty:
                date_str = day_data.index[0].strftime('%Y-%m-%d')
                
                # ì˜ˆì¸¡ ë°©í–¥
                if 'predicted_direction' in day_data.columns:
                    pred_dir = day_data['predicted_direction'].iloc[0]
                    color = 'rgba(0, 128, 0, 0.7)' if pred_dir == 'Up' else 'rgba(255, 0, 0, 0.7)'
                    
                    # ì‹¤ì œ ë°©í–¥ì´ ìˆëŠ” ê²½ìš°
                    if 'actual_direction' in day_data.columns:
                        actual_dir = day_data['actual_direction'].iloc[0]
                        # ì˜ˆì¸¡ì´ ë§ì•˜ëŠ”ì§€ í‘œì‹œ
                        if pred_dir == actual_dir:
                            text_content = f"{date_str}\nPred: {pred_dir} âœ“"
                        else:
                            text_content = f"{date_str}\nPred: {pred_dir} âœ—"
                    else:
                        text_content = f"{date_str}\nPred: {pred_dir}"
                else:
                    text_content = date_str
                    color = 'rgba(200, 200, 200, 0.5)'
            else:
                text_content = ""
                color = 'rgba(255, 255, 255, 0)'
            
            text.append(text_content)
            colors.append(color)
            dates.append(day_data.index[0] if not day_data.empty else None)
    
    # Heatmap ìƒì„±
    fig.add_trace(go.Heatmap(
        z=[[1 for _ in range(7)] for _ in range(len(weeks))],  # ë”ë¯¸ ë°ì´í„°
        x=day_labels,
        y=week_labels,
        text=text,
        texttemplate="%{text}",
        colorscale=[[0, 'rgba(0,0,0,0)'], [1, 'rgba(0,0,0,0)']],  # íˆ¬ëª… ë°°ê²½
        showscale=False,
        hoverinfo='text'
    ))
    
    # ì…€ ìƒ‰ìƒ ì¶”ê°€ (ê°œë³„ ì‚¬ê°í˜•ìœ¼ë¡œ)
    for i, week in enumerate(weeks):
        for j in range(7):
            idx = i * 7 + j
            if idx < len(colors) and colors[idx] != 'rgba(255, 255, 255, 0)':
                fig.add_shape(
                    type="rect",
                    x0=j-0.45, y0=i-0.45,
                    x1=j+0.45, y1=i+0.45,
                    fillcolor=colors[idx],
                    line=dict(color="white", width=1),
                    layer="below"
                )
    
    fig.update_layout(
        title=f"Prediction Calendar - {category}",
        template='plotly_white',
        height=len(weeks) * 60 + 100,  # ì£¼ ìˆ˜ì— ë”°ë¼ ë†’ì´ ì¡°ì •
        xaxis=dict(
            tickvals=list(range(7)),
            ticktext=day_labels
        ),
        yaxis=dict(
            autorange="reversed",  # ìœ„ì—ì„œ ì•„ë˜ë¡œ ì£¼ í‘œì‹œ
            tickvals=list(range(len(weeks))),
            ticktext=week_labels
        )
    )
    
    return fig
# í—¬í¼ í•¨ìˆ˜: ë°©í–¥ ì¹´ìš´íŠ¸ ì°¨íŠ¸ ìƒì„±
def create_direction_counts_chart(labels, actual_counts, predicted_counts):
    """ì‹¤ì œ ë°©í–¥ vs ì˜ˆì¸¡ ë°©í–¥ ì¹´ìš´íŠ¸ ë¹„êµ ì°¨íŠ¸ ìƒì„±"""
    import plotly.graph_objects as go
    
    fig = go.Figure()
    
    fig.add_trace(
        go.Bar(
            x=labels,
            y=actual_counts,
            name='Actual',
            marker_color='royalblue',
            opacity=0.7
        )
    )
    
    fig.add_trace(
        go.Bar(
            x=labels,
            y=predicted_counts,
            name='Predicted',
            marker_color='firebrick',
            opacity=0.7
        )
    )
    
    fig.update_layout(
        title="Actual vs Predicted Direction Counts",
        xaxis_title="Direction",
        yaxis_title="Count",
        template='plotly_white',
        barmode='group',
        legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="center", x=0.5)
    )
    
    # ë°ì´í„° ë ˆì´ë¸” ì¶”ê°€
    for i, (actual, pred) in enumerate(zip(actual_counts, predicted_counts)):
        fig.add_annotation(
            x=labels[i],
            y=actual,
            text=str(actual),
            showarrow=False,
            yshift=10
        )
        fig.add_annotation(
            x=labels[i],
            y=pred,
            text=str(pred),
            showarrow=False,
            yshift=10,
            xshift=20
        )
    
    return fig
# í—¬í¼ í•¨ìˆ˜: í˜¼ë™ í–‰ë ¬ ì°¨íŠ¸ ìƒì„±
def create_confusion_matrix_chart(cm, category):
    """í˜¼ë™ í–‰ë ¬ íˆíŠ¸ë§µ ìƒì„±"""
    import plotly.graph_objects as go
    import numpy as np
    
    # ë°±ë¶„ìœ¨ë¡œ ë³€í™˜
    cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100
    
    # ë ˆì´ë¸” ë° ì£¼ì„ í…ìŠ¤íŠ¸ ì¤€ë¹„
    labels = ['Down', 'Up']
    annotations = []
    
    for i in range(len(labels)):
        for j in range(len(labels)):
            annotations.append(
                dict(
                    x=labels[j],
                    y=labels[i],
                    text=f"{cm[i, j]} ({cm_percent[i, j]:.1f}%)",
                    font=dict(color='white' if cm[i, j] > cm.max() / 2 else 'black'),
                    showarrow=False
                )
            )
    
    # ìƒ‰ìƒ ìŠ¤ì¼€ì¼ ì„¤ì •
    colorscale = [
        [0, 'rgb(247, 251, 255)'],
        [0.1, 'rgb(222, 235, 247)'],
        [0.2, 'rgb(198, 219, 239)'],
        [0.3, 'rgb(158, 202, 225)'],
        [0.4, 'rgb(107, 174, 214)'],
        [0.5, 'rgb(66, 146, 198)'],
        [0.6, 'rgb(33, 113, 181)'],
        [0.7, 'rgb(8, 81, 156)'],
        [0.8, 'rgb(8, 48, 107)'],
        [1, 'rgb(3, 19, 43)']
    ]
    
    fig = go.Figure(data=go.Heatmap(
        z=cm,
        x=labels,
        y=labels,
        colorscale=colorscale,
        text=[[f"{cm[i, j]} ({cm_percent[i, j]:.1f}%)" for j in range(len(labels))] for i in range(len(labels))],
        texttemplate="%{text}",
        hoverongaps=False
    ))
    
    fig.update_layout(
        title=f"Confusion Matrix - {category}",
        xaxis_title="Predicted",
        yaxis_title="Actual",
        annotations=annotations,
        template='plotly_white'
    )
    
    return fig

# í—¬í¼ í•¨ìˆ˜: ì •í™•ë„ ì¶”ì´ ì°¨íŠ¸ ìƒì„±
def create_accuracy_trend_chart(rolling_accuracy, window_size, category):
    """ì‹œê°„ì— ë”°ë¥¸ ì˜ˆì¸¡ ì •í™•ë„ ì¶”ì´ ì°¨íŠ¸ ìƒì„±"""
    import plotly.graph_objects as go
    
    fig = go.Figure()
    
    fig.add_trace(
        go.Scatter(
            x=rolling_accuracy.index,
            y=rolling_accuracy.values,
            mode='lines',
            name=f'{window_size}-period Rolling Accuracy',
            line=dict(color='royalblue', width=2)
        )
    )
    
    # 50% ë¼ì¸ ì¶”ê°€ (ëœë¤ ì˜ˆì¸¡)
    fig.add_hline(
        y=0.5,
        line_dash="dash",
        line_color="red",
        annotation_text="Random Guess (50%)",
        annotation_position="bottom right"
    )
    
    # ì „ì²´ í‰ê·  ì •í™•ë„ ë¼ì¸ ì¶”ê°€
    mean_accuracy = rolling_accuracy.mean()
    fig.add_hline(
        y=mean_accuracy,
        line_dash="dot",
        line_color="green",
        annotation_text=f"Avg Accuracy: {mean_accuracy:.2%}",
        annotation_position="top right"
    )
    
    fig.update_layout(
        title=f"Prediction Accuracy Trend - {category}",
        xaxis_title="Date",
        yaxis_title="Accuracy (Rolling Window)",
        template='plotly_white',
        yaxis=dict(tickformat='.0%', range=[0, 1])
    )
    
    return fig

# í—¬í¼ í•¨ìˆ˜: ì˜ˆì¸¡ ìº˜ë¦°ë” ì‹œê°í™”
def create_prediction_calendar(calendar_data, category):
    """Plotlyë¥¼ ì‚¬ìš©í•œ ì˜ˆì¸¡ ìº˜ë¦°ë” ì‹œê°í™”"""
    import plotly.graph_objects as go
    import pandas as pd
    
    # ë°ì´í„° ì¤€ë¹„
    if not isinstance(calendar_data.index, pd.DatetimeIndex):
        if 'date' in calendar_data.columns:
            calendar_data = calendar_data.set_index('date')
        else:
            return go.Figure()  # ë‚ ì§œ ì •ë³´ê°€ ì—†ìœ¼ë©´ ë¹ˆ ê·¸ë¦¼ ë°˜í™˜
    
    # ë‚ ì§œë¥¼ ì£¼ ë‹¨ìœ„ë¡œ êµ¬ì„±
    calendar_data = calendar_data.copy()
    calendar_data['day_of_week'] = calendar_data.index.dayofweek
    calendar_data['week'] = calendar_data.index.isocalendar().week
    
    # ì£¼ë³„ë¡œ ê·¸ë£¹í™”
    weeks = calendar_data['week'].unique()
    
    fig = go.Figure()
    
    # ìš”ì¼ ë° ì£¼ ë ˆì´ë¸”
    day_labels = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']
    week_labels = [f"Week {w}" for w in weeks]
    
    # ê° ì…€ì˜ ìƒ‰ìƒ ë° í…ìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„
    text = []
    colors = []
    dates = []
    
    # ê° ì£¼ì— ëŒ€í•´
    for week in weeks:
        week_data = calendar_data[calendar_data['week'] == week]
        
        # ê° ìš”ì¼ì— ëŒ€í•´
        for day in range(7):
            day_data = week_data[week_data['day_of_week'] == day]
            
            if not day_data.empty:
                date_str = day_data.index[0].strftime('%Y-%m-%d')
                
                # ì˜ˆì¸¡ ë°©í–¥
                if 'predicted_direction' in day_data.columns:
                    pred_dir = day_data['predicted_direction'].iloc[0]
                    color = 'rgba(0, 128, 0, 0.7)' if pred_dir == 'Up' else 'rgba(255, 0, 0, 0.7)'
                    
                    # ì‹¤ì œ ë°©í–¥ì´ ìˆëŠ” ê²½ìš°
                    if 'actual_direction' in day_data.columns:
                        actual_dir = day_data['actual_direction'].iloc[0]
                        # ì˜ˆì¸¡ì´ ë§ì•˜ëŠ”ì§€ í‘œì‹œ
                        if pred_dir == actual_dir:
                            text_content = f"{date_str}\nPred: {pred_dir} âœ“"
                        else:
                            text_content = f"{date_str}\nPred: {pred_dir} âœ—"
                    else:
                        text_content = f"{date_str}\nPred: {pred_dir}"
                else:
                    text_content = date_str
                    color = 'rgba(200, 200, 200, 0.5)'
            else:
                text_content = ""
                color = 'rgba(255, 255, 255, 0)'
            
            text.append(text_content)
            colors.append(color)
            dates.append(day_data.index[0] if not day_data.empty else None)
    
    # Heatmap ìƒì„±
    fig.add_trace(go.Heatmap(
        z=[[1 for _ in range(7)] for _ in range(len(weeks))],  # ë”ë¯¸ ë°ì´í„°
        x=day_labels,
        y=week_labels,
        text=text,
        texttemplate="%{text}",
        colorscale=[[0, 'rgba(0,0,0,0)'], [1, 'rgba(0,0,0,0)']],  # íˆ¬ëª… ë°°ê²½
        showscale=False,
        hoverinfo='text'
    ))
    
    # ì…€ ìƒ‰ìƒ ì¶”ê°€ (ê°œë³„ ì‚¬ê°í˜•ìœ¼ë¡œ)
    for i, week in enumerate(weeks):
        for j in range(7):
            idx = i * 7 + j
            if idx < len(colors) and colors[idx] != 'rgba(255, 255, 255, 0)':
                fig.add_shape(
                    type="rect",
                    x0=j-0.45, y0=i-0.45,
                    x1=j+0.45, y1=i+0.45,
                    fillcolor=colors[idx],
                    line=dict(color="white", width=1),
                    layer="below"
                )
    
    fig.update_layout(
    title=f"Prediction Calendar - {category}",
    template='plotly_white',
    height=len(weeks) * 60 + 100,  # ì£¼ ìˆ˜ì— ë”°ë¼ ë†’ì´ ì¡°ì •
    xaxis=dict(
        tickvals=list(range(7)),
        ticktext=day_labels
    ),
    yaxis=dict(
        autorange="reversed",  # ìœ„ì—ì„œ ì•„ë˜ë¡œ ì£¼ í‘œì‹œ
        tickvals=list(range(len(weeks))),
        ticktext=week_labels
    )
)

    return fig

def create_asset_comparison_chart(daily_prices, assets, start_date=None, end_date=None):
    """ì—¬ëŸ¬ ìì‚°ì˜ ì„±ê³¼ë¥¼ ë¹„êµí•˜ëŠ” ì°¨íŠ¸ ìƒì„±"""
    import plotly.graph_objects as go
    import pandas as pd
    
    fig = go.Figure()
    
    # ë‚ ì§œ í•„í„°ë§
    if start_date is not None and end_date is not None:
        mask = (daily_prices.index.date >= start_date) & (daily_prices.index.date <= end_date)
        filtered_prices = daily_prices.loc[mask]
    else:
        filtered_prices = daily_prices
    
    # ë¹„êµë¥¼ ìœ„í•´ ì²« ë‚ ì„ 100ìœ¼ë¡œ ì •ê·œí™”
    normalized_prices = pd.DataFrame()
    
    for asset in assets:
        if asset in filtered_prices.columns:
            asset_prices = filtered_prices[asset].dropna()
            if not asset_prices.empty:
                normalized_prices[asset] = asset_prices / asset_prices.iloc[0] * 100
    
    # ê° ìì‚°ì˜ ì„±ê³¼ ê·¸ë˜í”„ ì¶”ê°€
    for asset in assets:
        if asset in normalized_prices.columns:
            color = 'royalblue' if asset == assets[0] else None  # ì²« ë²ˆì§¸ ìì‚°(ì„ íƒëœ ìì‚°)ì€ íŒŒë€ìƒ‰ìœ¼ë¡œ ê°•ì¡°
            width = 3 if asset == assets[0] else 1.5  # ì²« ë²ˆì§¸ ìì‚°ì€ ë” ë‘ê»ê²Œ
            
            fig.add_trace(
                go.Scatter(
                    x=normalized_prices.index,
                    y=normalized_prices[asset],
                    mode='lines',
                    name=asset,
                    line=dict(color=color, width=width)
                )
            )
    
    # ê¸°ì¤€ì„  (100) ì¶”ê°€
    fig.add_hline(
        y=100,
        line_dash="dash",
        line_color="black",
        annotation_text="Baseline (100)",
        annotation_position="bottom right"
    )
    
    # ë ˆì´ì•„ì›ƒ ì„¤ì •
    fig.update_layout(
        title="Asset Performance Comparison (Normalized to 100)",
        xaxis_title="Date",
        yaxis_title="Normalized Price (First day = 100)",
        template='plotly_white',
        legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="center", x=0.5)
    )
    
    return fig
def create_performance_comparison_chart(performance_df):
    """ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ì°¨íŠ¸ ìƒì„±"""
    import plotly.graph_objects as go
    from plotly.subplots import make_subplots
    
    if performance_df.empty:
        # ë¹ˆ ì°¨íŠ¸ ë°˜í™˜
        return go.Figure()
    
    # ë°ì´í„° ì¤€ë¹„
    categories = performance_df['Category'].tolist()
    accuracy = performance_df['Accuracy'].tolist()
    optimal_accuracy = performance_df['Optimal Threshold Accuracy'].tolist()
    roc_auc = performance_df['ROC-AUC'].tolist()
    
    # ì°¨íŠ¸ ìƒì„±
    fig = make_subplots(rows=1, cols=1)
    
    # ì •í™•ë„ ë°”
    fig.add_trace(
        go.Bar(
            x=categories,
            y=accuracy,
            name='Accuracy',
            marker_color='royalblue',
            opacity=0.7
        )
    )
    
    # ìµœì  ì„ê³„ê°’ ì •í™•ë„ ë°”
    fig.add_trace(
        go.Bar(
            x=categories,
            y=optimal_accuracy,
            name='Optimal Accuracy',
            marker_color='firebrick',
            opacity=0.7
        )
    )
    
    # ROC-AUC ì„ 
    fig.add_trace(
        go.Scatter(
            x=categories,
            y=roc_auc,
            mode='lines+markers',
            name='ROC-AUC',
            line=dict(color='green', width=2),
            marker=dict(size=10)
        )
    )
    
    # ë ˆì´ì•„ì›ƒ ì„¤ì •
    fig.update_layout(
        title="Model Performance Metrics by Category",
        xaxis_title="Category",
        yaxis_title="Score",
        template='plotly_white',
        barmode='group',
        legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="center", x=0.5),
        yaxis=dict(tickformat='.0%', range=[0, 1])
    )
    
    return fig
import streamlit as st
import pandas as pd

# --- ì—¬ê¸°ì— í•„ìš”í•œ í•¨ìˆ˜ë“¤ import (ë˜ëŠ” ì •ì˜) í•´ì¤˜ì•¼ í•´ ---
# preprocess_data, create_correlation_features, get_indicator_columns,
# prepare_enhanced_model_data, build_improved_prediction_model,
# generate_focused_ml_insights, generate_market_insights_dashboard,
# extreme_events_dashboard, create_output_directory

def main():
    """Streamlit ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""

    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    if 'category_analyses' not in st.session_state:
        st.session_state['category_analyses'] = {}
    if 'current_category' not in st.session_state:
        st.session_state['current_category'] = None

    # í˜ì´ì§€ íƒ€ì´í‹€
    st.title("Financial Data Analysis & Prediction App")

    # ì‚¬ì´ë“œë°” ì„¤ì •
    st.sidebar.header("Setting")
    st.sidebar.subheader("Data Upload")
    uploaded_file = st.sidebar.file_uploader("Upload CSV File", type=["csv"])

    app_mode = st.sidebar.selectbox("Select Mode", ["ğŸ¤– ML: Stock Up/Down Prediction", "ğŸš¨ Extreme Events & Anomalies"])

    st.sidebar.subheader(" Model Settings")
    n_features = st.sidebar.slider("Number of Features to Select", 10, 70, 30)
    n_trials = st.sidebar.slider("Number of Optuna Trials", 10, 150, 50)

    run_analysis = st.sidebar.button("ğŸš€Run Analysis")

    # ë°ì´í„° ì¡´ì¬ ì—¬ë¶€ ì²´í¬
    if uploaded_file is not None or (
        'df' in st.session_state and
        'daily_avg_prices' in st.session_state and
        'daily_indicators' in st.session_state and
        'enhanced_model_data' in st.session_state
    ):

        # íŒŒì¼ ì—…ë¡œë“œ ì‹œ ë°ì´í„° ì²˜ë¦¬
        if uploaded_file is not None:
            with st.spinner("Loading data..."):
                try:
                    df = pd.read_csv(uploaded_file)
                    st.session_state['df'] = df
                    st.subheader(" Data Preview")
                    st.dataframe(df.head())

                    st.subheader("Basic Info")
                    st.write(f"- **Rows**: {df.shape[0]}")
                    st.write(f"- **Columns**: {df.shape[1]}")

                    required_cols = ['date', 'category', 'close']
                    missing_cols = [col for col in required_cols if col not in df.columns]
                    if missing_cols:
                        st.error(f" Missing required columns: {missing_cols}")
                        st.stop()

                    if 'date' in df.columns:
                        if not pd.api.types.is_datetime64_any_dtype(df['date']):
                            df['date'] = pd.to_datetime(df['date'], errors='coerce')

                    if 'category' in df.columns:
                        categories_in_data = df['category'].unique().tolist()
                        st.write(f"Categories: {categories_in_data}")
                        categories = st.sidebar.multiselect(
                            "Select Categories to Analyze",
                            categories_in_data,
                            default=categories_in_data[:min(4, len(categories_in_data))]
                        )
                        st.session_state['categories'] = categories
                    else:
                        st.error("'category' column is missing in the data.")
                        st.stop()

                    with st.spinner("Preprocessing data..."):
                        df, daily_avg_prices, daily_indicators = preprocess_data(df)

                except Exception as e:
                    st.error(f" Error during data processing: {str(e)}")
                    st.exception(e)
                    return

        # ë¶„ì„ ëª¨ë“œ ë¶„ê¸°
        if app_mode == "ğŸ¤– ML: Stock Up/Down Prediction":
            model_tab, insight_tab = st.tabs(["ğŸ“Š Model Result", "ğŸ§  Market Insight"])

            if run_analysis and uploaded_file is not None:
                output_dir = create_output_directory("ML_results")

                with st.spinner("Creating correlation features..."):
                    category_tickers = {cat: [cat] for cat in categories}
                    df_corr, daily_avg_prices, daily_indicators = create_correlation_features(df, category_tickers)

                _, _, _, all_norm_indicators = get_indicator_columns()
                important_indicators = [ind for ind in all_norm_indicators if ind in daily_indicators.columns]

                enhanced_model_data = prepare_enhanced_model_data(
                    daily_avg_prices, daily_indicators, categories, important_indicators
                )

                results = {}
                for category in categories:
                    if category in enhanced_model_data:
                        result = build_improved_prediction_model(
                            enhanced_model_data, category, output_dir,
                            n_features=n_features, n_trials=n_trials
                        )
                        results[category] = result

                # ì„¸ì…˜ì— ê²°ê³¼ ì €ì¥
                st.session_state['model_results'] = results
                st.session_state['enhanced_model_data'] = enhanced_model_data
                st.session_state['daily_avg_prices'] = daily_avg_prices
                st.session_state['daily_indicators'] = daily_indicators
                st.session_state['df'] = df

                st.success("The analysis is complete. Click each tab to view the results.")

                with model_tab:
                    generate_focused_ml_insights(
                        model_results=results,
                        model_data=enhanced_model_data,
                        daily_avg_prices=daily_avg_prices,
                        daily_indicators=daily_indicators
                    )
                with insight_tab:
                    generate_market_insights_dashboard(
                        enhanced_model_data=enhanced_model_data,
                        daily_avg_prices=daily_avg_prices,
                        daily_indicators=daily_indicators
                    )
            else:
                # ë¶„ì„ ì—†ì´ ì„¸ì…˜ ë°ì´í„°ë§Œìœ¼ë¡œ ëŒ€ì‹œë³´ë“œ í‘œì‹œ
                if 'enhanced_model_data' in st.session_state:
                    enhanced_model_data = st.session_state['enhanced_model_data']
                    results = st.session_state['model_results']
                    daily_avg_prices = st.session_state['daily_avg_prices']
                    daily_indicators = st.session_state['daily_indicators']

                    with model_tab:
                        generate_focused_ml_insights(
                            model_results=results,
                            model_data=enhanced_model_data,
                            daily_avg_prices=daily_avg_prices,
                            daily_indicators=daily_indicators
                        )
                    with insight_tab:
                        generate_market_insights_dashboard(
                            enhanced_model_data=enhanced_model_data,
                            daily_avg_prices=daily_avg_prices,
                            daily_indicators=daily_indicators
                        )
                else:
                    st.info(" Please run the analysis first.")

        elif app_mode == "ğŸš¨ Extreme Events & Anomalies":
            required_keys = ['enhanced_model_data', 'df', 'daily_avg_prices', 'daily_indicators']
            missing_keys = [key for key in required_keys if key not in st.session_state]

            if missing_keys:
                st.error(f" Missing data: {', '.join(missing_keys)}. Please run analysis first in App1.")
                st.stop()

            enhanced_model_data = st.session_state['enhanced_model_data']
            df = st.session_state['df']
            daily_avg_prices = st.session_state['daily_avg_prices']
            daily_indicators = st.session_state['daily_indicators']
            categories = st.session_state['categories']

            extreme_events_dashboard(df, daily_avg_prices, daily_indicators)

    else:
        st.info(" Please upload a CSV file from the sidebar to get started.")


if __name__ == "__main__":
    main()

# def main():
#     """Streamlit ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""

#     # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
#     if 'category_analyses' not in st.session_state:
#         st.session_state['category_analyses'] = {}
#     if 'current_category' not in st.session_state:
#         st.session_state['current_category'] = None

#     # í˜ì´ì§€ íƒ€ì´í‹€
#     st.title(" Financial Data Analysis & Prediction App")

#     # ì‚¬ì´ë“œë°” ì„¤ì •
#     st.sidebar.header(" Setting")
#     st.sidebar.subheader("Data Upload")
#     uploaded_file = st.sidebar.file_uploader("Upload CSV File", type=["csv"])

#     app_mode = st.sidebar.selectbox("Select Mode", ["ğŸ¤– ML: Stock Up/Down Prediction", "ğŸš¨ Extreme Events & Anomalies"])

#     st.sidebar.subheader(" Model Settings")
#     n_features = st.sidebar.slider(" Number of Features to Select", 10, 70, 30)
#     n_trials = st.sidebar.slider(" Number of Optuna Trials", 10, 150, 50)

#     run_analysis = st.sidebar.button(" Run Analysis")

#     if uploaded_file is not None or ('df' in st.session_state and 'daily_avg_prices' in st.session_state and 'daily_indicators' in st.session_state and 'enhanced_model_data' in st.session_state):
#         if uploaded_file is not None:
#             with st.spinner("Loading data..."):
#                 try:
#                     df = pd.read_csv(uploaded_file)
#                     st.session_state['df'] = df
#                     st.subheader("Data Preview")
#                     st.dataframe(df.head())

#                     st.subheader("Basic Info")
#                     st.write(f"- **Rows**: {df.shape[0]}")
#                     st.write(f"- **Columns**: {df.shape[1]}")

#                     required_cols = ['date', 'category', 'close']
#                     missing_cols = [col for col in required_cols if col not in df.columns]
#                     if missing_cols:
#                         st.error(f"Missing required columns: {missing_cols}")
#                         st.stop()

#                     if 'date' in df.columns:
#                         if not pd.api.types.is_datetime64_any_dtype(df['date']):
#                             df['date'] = pd.to_datetime(df['date'], errors='coerce')

#                     if 'category' in df.columns:
#                         categories_in_data = df['category'].unique().tolist()
#                         st.write(f"\ Categories: {categories_in_data}")
#                         categories = st.sidebar.multiselect(
#                             "Select Categories to Analyze",
#                             categories_in_data,
#                             default=categories_in_data[:min(4, len(categories_in_data))]
#                         )
#                     else:
#                         st.error("'category' column is missing in the data.")
#                         st.stop()

#                     with st.spinner("Preprocessing data..."):
#                         df, daily_avg_prices, daily_indicators = preprocess_data(df)
#                         st.session_state['daily_avg_prices'] = daily_avg_prices
#                         st.session_state['daily_indicators'] = daily_indicators

#                 except Exception as e:
#                     st.error(f" Error during data processing: {str(e)}")
#                     st.exception(e)
#                     return

#         # ë¶„ì„ ëª¨ë“œ ë¶„ê¸°
#         if app_mode == "ğŸ¤– ML: Stock Up/Down Prediction":
#             model_tab, insight_tab = st.tabs(["ğŸ“Š Model Result", "ğŸ§  Market Insight"])

#             if run_analysis and uploaded_file is not None:
#                 output_dir = create_output_directory("ML_results")

#                 with st.spinner("Creating correlation features..."):
#                     category_tickers = {cat: [cat] for cat in categories}
#                     df_corr, daily_avg_prices, daily_indicators = create_correlation_features(df, category_tickers)

#                 # âœ¨ important_indicators ì •ì˜ ì¶”ê°€
#                 _, _, _, all_norm_indicators = get_indicator_columns()
#                 important_indicators = [ind for ind in all_norm_indicators if ind in daily_indicators.columns]

#                 enhanced_model_data = prepare_enhanced_model_data(
#                     daily_avg_prices, daily_indicators, categories, important_indicators
#                 )

#                 results = {}
#                 for category in categories:
#                     if category in enhanced_model_data:
#                         result = build_improved_prediction_model(
#                             enhanced_model_data, category, output_dir,
#                             n_features=n_features, n_trials=n_trials
#                         )
#                         results[category] = result

#                 st.session_state['model_results'] = results
#                 st.session_state['enhanced_model_data'] = enhanced_model_data
#                 st.session_state['daily_avg_prices'] = daily_avg_prices
#                 st.session_state['daily_indicators'] = daily_indicators
#                 st.session_state['df'] = df
#                 st.success("âœ… The analysis is complete. Click each tab to view the results.")

#                 with model_tab:
#                     generate_focused_ml_insights(
#                         model_results=results,
#                         model_data=enhanced_model_data,
#                         daily_avg_prices=daily_avg_prices,
#                         daily_indicators=daily_indicators
#                     )
#                 with insight_tab:
#                     generate_market_insights_dashboard(
#                         enhanced_model_data=enhanced_model_data,
#                         daily_avg_prices=daily_avg_prices,
#                         daily_indicators=daily_indicators
#                     )

#             else:
#                 if 'enhanced_model_data' in st.session_state:
#                     enhanced_model_data = st.session_state['enhanced_model_data']
#                     results = st.session_state['model_results']
#                     daily_avg_prices = st.session_state['daily_avg_prices']
#                     daily_indicators = st.session_state['daily_indicators']
                    
#                     with model_tab:
#                         generate_focused_ml_insights(
#                             model_results=results,
#                             model_data=enhanced_model_data,
#                             daily_avg_prices=daily_avg_prices,
#                             daily_indicators=daily_indicators
#                         )
#                     with insight_tab:
#                         generate_market_insights_dashboard(
#                             enhanced_model_data=enhanced_model_data,
#                             daily_avg_prices=daily_avg_prices,
#                             daily_indicators=daily_indicators
#                         )
#                 else:
#                     st.info("Please run the analysis first.")

#         elif app_mode == "ğŸš¨ Extreme Events & Anomalies":
#             required_keys = ['enhanced_model_data', 'df', 'daily_avg_prices', 'daily_indicators']
#             missing_keys = [key for key in required_keys if key not in st.session_state]

#             if missing_keys:
#                 st.error(f"âŒ Missing data: {', '.join(missing_keys)}. Please run analysis first in App1.")
#                 st.stop()

#             # âœ… ì„¸ì…˜ì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
#             enhanced_model_data = st.session_state['enhanced_model_data']
#             df = st.session_state['df']
#             daily_avg_prices = st.session_state['daily_avg_prices']
#             daily_indicators = st.session_state['daily_indicators']

#             # âœ… ëŒ€ì‹œë³´ë“œ í˜¸ì¶œ
#             extreme_events_dashboard(df, daily_avg_prices, daily_indicators)

#     else:
#         st.info(" Please upload a CSV file from the sidebar to get started.")

# if __name__ == "__main__":
#     main()


